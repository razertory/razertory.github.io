{"meta":{"title":"Razertory 的技术博客","subtitle":"一曲肝肠断，天涯何处觅知音","description":"Maker, Coder, Puzzle Solver","author":"Razertory","url":"https://razertory.me","root":"/"},"pages":[{"title":"读书","date":"2019-10-16T13:31:33.000Z","updated":"2020-05-04T07:19:22.465Z","comments":true,"path":"books/index.html","permalink":"https://razertory.me/books/index.html","excerpt":"","text":"反应式设计模式 七周七语言 数据密集型应用系统设计"},{"title":"算法题解","date":"2019-10-16T13:31:33.000Z","updated":"2020-05-04T07:19:57.688Z","comments":true,"path":"algo-solution/index.html","permalink":"https://razertory.me/algo-solution/index.html","excerpt":"","text":"项目 java-code-lab Java 实现的数据结构/算法以及计算机科学相关的代码和完整的 Junit 测试用例。 https://github.com/razertory/jianzhi-offer 《剑指 offer》 排序快速排序 归并排序 堆排序 链表反转链表 复制复杂链表 链表的公共节点 链表环的入口 删除重复的有序链表节点 链表相加 链表两两交换 二叉树重建二叉树 判断是否是子树 判断数组是 BST 后序遍历 和为定值的所有路径 判断是否是二叉搜索树 栈和队列最小栈 栈实现队列 括号的合法序列 汉诺塔问题 位运算位运算实际问题-找毒药 位运算实现加法 二进制中 1 的个数 只出现了一次的数 个数多于一半的数 汉明距离 搜索各种层序遍历二叉树 完美的二分搜索 有序数组转化为 BST 无重复数组的全排列 N 皇后问题 动态规划股票买卖最大利润 最大子序列 抢劫房子 最长回文子串 二维数组中和最小路径 编辑距离 动态规划合集 高级数据结构LRU 缓存"},{"title":"关于本博客","date":"2019-10-16T13:01:48.000Z","updated":"2020-10-26T03:37:02.719Z","comments":true,"path":"links/index.html","permalink":"https://razertory.me/links/index.html","excerpt":"","text":"博客的主题是 indigo，Material Design 风格，功能比较多的。博客的评论系统用的是 gitalk，我会用 telegram 的机器人去收评论消息快速回复。当然，如果想持续关注，RSS 依旧是个不错的选择。 找到我 telegram (快速联系) spring.razer AT gmail.com https://github.com/razertory vx: chendalichun 友链如果需要添加友链，可以在 telegram 上直接联系我（这样很快），或者发我邮件 个人windanchaososhmkufacounter2015Atticus 团队NetflixDropboxFacebookCloudflarePingCAPSnowflakeLightbendDatabricksAirbnb阿里中间件小米信息部技术团队"},{"title":"","date":"2020-04-18T08:42:18.669Z","updated":"2020-04-18T08:42:18.669Z","comments":false,"path":"categories/index.html","permalink":"https://razertory.me/categories/index.html","excerpt":"","text":""},{"title":"Scala","date":"2020-03-28T13:02:10.020Z","updated":"2020-03-28T13:02:10.020Z","comments":true,"path":"scala-school/index.html","permalink":"https://razertory.me/scala-school/index.html","excerpt":"","text":"Scala 基础Scala 总览 集合列表，映射，功能组合 (map, foreach, filter, zip, folds) 模式匹配与函数组合更多函数！偏函数，更多模式匹配 类型和多态基础基本类型和类型多态性，类型推断，变性，边界，量化 高级类型高级类型，视界，更高级多态性类型，递归类型，结构类型 参考资料 Scala School (https://twitter.github.io/scala_school/) Scala 99 (https://github.com/shekhargulati/99-problems/tree/master/scala) Databricks Scala 编程风格指南 (https://github.com/databricks/scala-style-guide/blob/master/README-ZH.md)"},{"title":"计算机系统","date":"2019-10-16T13:31:33.000Z","updated":"2020-07-21T08:24:03.561Z","comments":true,"path":"cs/index.html","permalink":"https://razertory.me/cs/index.html","excerpt":"","text":"并发编程 按照顺序打印 操作系统 「Linux 内核」系统调用 「Linux 内核」中断 「Linux 内核」进程管理 「Linux 内核」」代码分析 select, poll, epoll 线程、进程和协程以及 IO 多路复用实现并发 Java 伪共享 Java 控制 CPU 占用 Go Uber Go Guide 改善你的 Go 代码 Scala Scala 总览 Scala 基础 Scala 集合 模式匹配与函数组合类型和多态基础 Scala 高级类型 Scala 函数式编程设计 Infra 开源项目之 Nginx MySQL Top Tips"},{"title":"","date":"2020-04-18T08:42:18.669Z","updated":"2020-04-18T08:42:18.669Z","comments":false,"path":"tags/index.html","permalink":"https://razertory.me/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"改善你的 Go 代码","slug":"go-coding-skills","date":"2020-07-21T06:29:48.000Z","updated":"2020-07-21T08:03:51.072Z","comments":true,"path":"2020/07/21/go-coding-skills/","link":"","permalink":"https://razertory.me/2020/07/21/go-coding-skills/","excerpt":"","text":"之前参与翻译了 Uber 公司分享的 Go 语言编码规范。一直以来，我对编码规范的态度是80%的正确性和20%的一致性。写代码，重要的是让程序正确和高效以及可维护性。近期整理了工作中遇到的改善 Go 编码的一些案例。 \b闭包（Closure）和 defer闭包的官方解释是：闭包是由函数和与其相关的引用环境组合而成的实体。defer 可以让一个闭包在函数 return 的时刻调用。比如有一个这样的代码 1234567for _, v := range values &#123; conn := d.redis.Get(ctx) err := conn.Send(\"SADD\") err = conn.Send(\"EXPIRE\") err = conn.Flush() conn.Close()&#125; 3 到 5 行的代码都需要处理 err，同时还需要 conn.Close()。所以可以改为 12345678910111213141516171819for _, v := range values &#123; conn := d.redis.Get(ctx) err := conn.Send(\"SADD\") if err != nil &#123; // 处理错误 conn.Close() &#125; err = conn.Send(\"EXPIRE\") if err != nil &#123; // 处理错误 conn.Close() &#125; err = conn.Flush() if err != nil &#123; // 处理错误 conn.Close() &#125; conn.Close()&#125; 这么写阅读起来有太多重复的逻辑，同时修改之后，也容易忘记 conn.Close()。这种情况，可以试着用闭包配合 defer 来改善 123456789101112131415161718for _, v := range values &#123; err := func() error &#123; conn := d.redis.Get(ctx) defer conn.Close() if err := conn.Send(\"SADD\"); err != nil &#123; return err &#125; if err := conn.Send(\"EXPIRE\"); err != nil &#123; return err &#125; if err := conn.Flush(); err != nil &#123; return err &#125; &#125; if err != nil &#123; // 处理错误 &#125;&#125; nil interface123456789101112131415161718192021222324type Audi struct &#123; price int&#125;type Tesla struct &#123; price int&#125;func main() &#123; car := getCar() if car == nil &#123; fmt.Printf(\"nil\") return &#125; fmt.Printf(\"%+v is not nil\", car)&#125;func getCar() interface&#123;&#125;&#123; var audi *Audi var tesla *Tesla if rand.Int63() / 2 == 0 &#123; return tesla &#125; return audi&#125; 这段代码的输出是 14 行的 &lt;nil&gt; is not nil。针对 interface{} 类型，判断是否 nil 需要判断类型的和值，本质上只要类型存在，比如这里可能是 *Audi 或者 *Tesla，那么就不会为 nil。如果需要设计返回 interface{} 的函数，可以加上一个 ok bool 表示是否存在。 123456789101112func main() &#123; car, ok := getCar() if !ok &#123; fmt.Printf(\"nil\") &#125; fmt.Printf(\"%+v is not nil\", car)&#125;func getCar() (interface&#123;&#125;, bool)&#123; var audi *Audi // 在确定类型的地方，判断是否空，这样再用 bool 类型给到外面 return audi, false&#125; 命名返回值named return value 在和 defer 相遇的时候，会有一些需要注意的细节。 1234567891011121314151617181920func main() &#123; getName() getName2()&#125;func getName() (name string)&#123; name = \"john\" defer func() &#123; fmt.Printf(name) &#125;() return \"jerry\"&#125;func getName2() (string)&#123; name := \"john\" defer func() &#123; fmt.Printf(name) &#125;() return \"jerry\"&#125; 上述代码输出的是 jerryjohn。如果闭包用到了 named retrun value，就意味着这个值及时没有被明显的复制，比如 return &quot;jerry&quot; 实际上是让 name 这个变量赋值了。这种代码难以 review 和调试。所以在返回值只有一个、两个的时候，推荐不用 named return value。只有返回值多个，或者返回值的类型一样的时候，再推荐 named return value。 错误处理编写生产环境代码的时候，为了让 err 产生的时候，尽量带有更多有利于排查问题的信息，推荐以下方法 标准库产生的 err，使用 `errors.WithStack() 除此之外的 err，用 github.com/pkg/errors 中的 Wrapf(), New(), Errorf() for 循环Go 语言的 for 循环，循环变量实际上总是指向同一块地址，在循环过程中，不断进行覆盖值的操作。例如 12345678func main() &#123; var arr []*int for i := 0; i &lt; 3; i++ &#123; arr = append(arr, &amp;i) &#125; fmt.Println(\"Values:\", *arr[0], *arr[1], *arr[2]) fmt.Println(\"Addresses:\", arr[0], arr[1], arr[2])&#125; 输出是:Values: 3 3 3Addresses: 0xc00001c078 0xc00001c078 0xc00001c078","categories":[],"tags":[]},{"title":"按照顺序打印","slug":"print-in-order","date":"2020-06-21T02:41:56.000Z","updated":"2020-06-21T04:15:30.886Z","comments":true,"path":"2020/06/21/print-in-order/","link":"","permalink":"https://razertory.me/2020/06/21/print-in-order/","excerpt":"","text":"没错，我又开了个坑。这次的问题是 按顺序打印。大意就是有 one two three 三个线程同时在跑，一定要让 two 在 one 后面、three 在 two 后面执行。 题目本来觉得用 Go 的 channel 应该是最好写的。 12var oneDone chan //one 结束就 oneDone &lt;-var twoDone chan //two 结束就 twoDone &lt;- 可惜并没有 Go。所以，要不 Java ? 阻塞队列那么要做到和 channel 类似的效果，首先就是确保 BlockingQueue 能在没有元素 pop 的时候能阻塞。查一下 api 有个 take () 方法 12345678/** * Retrieves and removes the head of this queue, waiting if necessary * until an element becomes available. * * @return the head of this queue * @throws InterruptedException if interrupted while waiting */E take () throws InterruptedException; 已通过代码 12345678910111213141516171819202122232425262728293031323334import java.util.concurrent.BlockingQueue;import java.util.concurrent.LinkedBlockingQueue;class Foo &#123; private BlockingQueue&lt;Integer&gt; one; private BlockingQueue&lt;Integer&gt; two; public Foo () &#123; one = new LinkedBlockingQueue&lt;&gt;(); two = new LinkedBlockingQueue&lt;&gt;(); &#125; public void first (Runnable printFirst) throws InterruptedException &#123; //printFirst.run () outputs \"first\". Do not change or remove this line. printFirst.run(); one.add(1); &#125; public void second (Runnable printSecond) throws InterruptedException &#123; //printSecond.run () outputs \"second\". Do not change or remove this line. one.take(); printSecond.run (); two.add(1); &#125; public void third (Runnable printThird) throws InterruptedException &#123; //printThird.run () outputs \"third\". Do not change or remove this line. two.take(); printThird.run(); &#125;&#125; 这道题的本质是要在 * 并发执行的过程中，保证部分单元的执行顺序 *。也就是某个执行单元在条件没有达到的时候，保持阻塞，让出 CPU。 互斥锁信号量 信号量（英语：semaphore）又称为信号标，是一个同步对象，用于保持在 0 至指定最大值之间的一个计数值。当线程完成一次对该 semaphore 对象的等待（wait）时，该计数值减一；当线程完成一次对 semaphore 对象的释放（release）时，计数值加一。当计数值为 0，则线程等待该 semaphore 对象不再能成功直至该 semaphore 对象变成 signaled 状态。semaphore 对象的计数值大于 0，为 signaled 状态；计数值等于 0，为 nonsignaled 状态.semaphore 对象适用于控制一个仅支持有限个用户的共享资源，是一种不需要使用忙碌等待（busy waiting）的方法。信号量的概念是由荷兰计算机科学家艾兹赫尔・戴克斯特拉（Edsger W. Dijkstra）发明的，广泛的应用于不同的操作系统中。在系统中，给予每一个进程一个信号量，代表每个进程目前的状态，未得到控制权的进程会在特定地方被强迫停下来，等待可以继续进行的信号到来。如果信号量是一个任意的整数，通常被称为计数信号量（Counting semaphore），或一般信号量（general semaphore）；如果信号量只有二进制的 0 或 1，称为二进制信号量（binary semaphore）。在 linux 系统中，二进制信号量（binary semaphore）又称互斥锁（Mutex）。 – 维基百科 Java 里面信号量有两个重要的行为，acquire 和 release。对此分别有以下描述 12345678910111213141516171819202122232425262728293031/** * Acquires a permit from this semaphore, blocking until one is * available, or the thread is &#123;@linkplain Thread#interrupt interrupted&#125;. * * &lt;p&gt;Acquires a permit, if one is available and returns immediately, * reducing the number of available permits by one. * * &lt;p&gt;If no permit is available then the current thread becomes * disabled for thread scheduling purposes and lies dormant until * one of two things happens: * &lt;ul&gt; * &lt;li&gt;Some other thread invokes the &#123;@link #release&#125; method for this * semaphore and the current thread is next to be assigned a permit; or * &lt;li&gt;Some other thread &#123;@linkplain Thread#interrupt interrupts&#125; * the current thread. * &lt;/ul&gt; * * &lt;p&gt;If the current thread: * &lt;ul&gt; * &lt;li&gt;has its interrupted status set on entry to this method; or * &lt;li&gt;is &#123;@linkplain Thread#interrupt interrupted&#125; while waiting * for a permit, * &lt;/ul&gt; * then &#123;@link InterruptedException&#125; is thrown and the current thread's * interrupted status is cleared. * * @throws InterruptedException if the current thread is interrupted */public void acquire() throws InterruptedException &#123; sync.acquireSharedInterruptibly(1);&#125; 12345678910111213141516/** * Releases a permit, returning it to the semaphore. * * &lt;p&gt;Releases a permit, increasing the number of available permits by * one. If any threads are trying to acquire a permit, then one is * selected and given the permit that was just released. That thread * is (re) enabled for thread scheduling purposes. * * &lt;p&gt;There is no requirement that a thread that releases a permit must * have acquired that permit by calling &#123;@link #acquire&#125;. * Correct usage of a semaphore is established by programming convention * in the application. */public void release() &#123; sync.releaseShared (1);&#125; 123456789101112131415161718192021222324252627import java.util.concurrent.Semaphore;class Foo &#123; private Semaphore s1; private Semaphore s2; public Foo() &#123; s1 = new Semaphore(0); s2 = new Semaphore(0); &#125; public void first (Runnable printFirst) throws InterruptedException &#123; printFirst.run(); s1.release(); &#125; public void second (Runnable printSecond) throws InterruptedException &#123; s1.acquire(); printSecond.run(); s2.release(); &#125; public void third (Runnable printThird) throws InterruptedException &#123; s2.acquire(); printThird.run(); &#125;&#125; Wait 和 Notify先说结论，用这种写法的是 Java 大佬。传送门 12345678910111213141516171819202122232425262728293031323334353637class Foo &#123; private boolean firstFinished; private boolean secondFinished; private Object lock = new Object(); public Foo() &#123; &#125; public void first (Runnable printFirst) throws InterruptedException &#123; synchronized(lock) &#123; printFirst.run(); firstFinished = true; lock.notifyAll(); &#125; &#125; public void second(Runnable printSecond) throws InterruptedException &#123; synchronized(lock) &#123; while (!firstFinished) &#123; lock.wait(); &#125; printSecond.run(); secondFinished = true; lock.notifyAll(); &#125; &#125; public void third(Runnable printThird) throws InterruptedException &#123; synchronized(lock) &#123; while (!secondFinished) &#123; lock.wait(); &#125; printThird.run(); &#125; &#125;&#125; 这里面牵涉到了几个 Java 并发体系的概念。对象锁和 syncronize、Object#Wait、Object#Notify。","categories":[],"tags":[]},{"title":"「Linux 内核」系统调用","slug":"linux-kernel-system-call","date":"2020-04-12T05:50:04.000Z","updated":"2020-04-18T08:54:30.936Z","comments":true,"path":"2020/04/12/linux-kernel-system-call/","link":"","permalink":"https://razertory.me/2020/04/12/linux-kernel-system-call/","excerpt":"","text":"内核内存空间在操作系统中是受保护的。通常情况下，一个普通程序执行在用户空间。当需要与内核通信的时候，比如写文件到磁盘，就需要进行系统调用。 有趣的是，内核原生提供的系统调用很少(参考List of Linux/i386 system calls)。当我们需要系统调用的时候，实际上是在调用调用的上一层 API。这一层 API 封装了各种系统调用（通常就是操作系统内置的 C 库）。现阶段，这一层 API 的标准来自于 IEEE ，名为 POSIX（Portable Operating System Interface）。这么做的原因是 unix 倡导的 提供机制，而不是提供策略。意思是提供有限的，但可以很好组合的工具、方法，在此基础上实现各种功能。 例如， get_pid() 在内核中的被定义为 sys_getpid()bar() 在内核中被定义为 sys_bar() 一个用户空间的程序，要做系统调用，具体的过程为 陷入内核 传递系统调用编号和参数 执行系统调用 将系统调用的返回值带给用户空间","categories":[],"tags":[{"name":"操作系统","slug":"操作系统","permalink":"https://razertory.me/tags/操作系统/"}]},{"title":"「Linux 内核」中断","slug":"linux-kernel-interrupt","date":"2020-04-12T05:50:04.000Z","updated":"2020-04-19T14:28:18.526Z","comments":true,"path":"2020/04/12/linux-kernel-interrupt/","link":"","permalink":"https://razertory.me/2020/04/12/linux-kernel-interrupt/","excerpt":"","text":"中断信号中断机制，简单说就是键盘、鼠标、磁盘之类的硬件，在需要的时候向内核发信号的一种机制。比如我此刻正在用键盘打字，其实就是在发出电信号给一个名叫「中断控制器」的物理芯片中，再通过一个和处理器直连的管线给处理器。处理器接收到之后，监测到了是中断信号，就会中断当前的工作处理该信号进而告诉操作系统，让操作系统处理这个信号。不同的中断信号，有的着唯一的 IRQ 编号。比如时钟 IRQ 0，键盘 IRQ 1，有的是动态分配的，比如在 PCI 总线上的设备。 中断处理操作系统处理中断的过程一般分成两个部分，命名为「上半部（top half）」和「下半部（bottom half）」。一般上半部(中断处理程序)有严格时限的操作，比方说快速应答。而有的可以延后执行的操作就交给了下半部。举个例子，操作系统处理网卡数据包的时候，会在上半部把网络数据拷贝到内存，下半部做数据处理的操作。 image.png 下半部时间敏感，硬件相关或者保证不能中断的任务，通常一定是在上半部，否则都在下半部。下半部的实现机制，在 linux 内核发展中经历了几个版本。 tasklet 基于软中断实现 软中断是编译期间分配的，由 softirq_action 表示。定义在 linux/interrupt.h。 kernel/softirq.c 里面定义了一个包含 32 个结构体的数组。每个结构体表示一个软中断，因此软中断最多有 32 个。不过目前这用到了 9 个。当软中断开始工作的时候，会执行一个名叫 void softirq_handler(struct softirq_action *) 的函数去标记注已经册的软中断。 等到合适的时候，该软中断就会执行，比如 从硬件中断代码返回的时候 在 ksoftirqd 内核线程中 在一些显式执行、显式检查软中断的程序中，比如网络系统中 tasklet 源自软中断，提供了动态分配的特性，有着更广泛的应用场景。","categories":[],"tags":[{"name":"操作系统","slug":"操作系统","permalink":"https://razertory.me/tags/操作系统/"}]},{"title":"「Linux 内核」进程管理","slug":"linux-kernel-process","date":"2020-04-11T10:15:42.000Z","updated":"2020-04-11T11:54:05.665Z","comments":true,"path":"2020/04/11/linux-kernel-process/","link":"","permalink":"https://razertory.me/2020/04/11/linux-kernel-process/","excerpt":"","text":"概览进程是处于执行期的程序以及相关资源的总称。比如打开的文件、挂起的信号、内核内部数据等等。在 linux 源码中，一个进程的相关信息维护在 task_struct task_struct的结构体中。这里面包含了进程的状态(pid，运行状态)、相关资源、以及相关进程（子进程、父进程）信息。内核会维护一个双向链表，每个链表的节点指向对应的 task_struct。 状态 TASK_RUNNING: 可执行的；等待执行的 TASK_INTERRUPTIBLE: 可中断的; 比如被阻塞、或者在 sleep TASK_UNINTERRUPTIBLE: 不可中断的; 对外界的信号不做出响应 EXIT_ZOMBIE: 主动退出；还没有完全释放资源 进程状态图 生命周期在进程被 fork 出来之后 task_struct 会有自己的 pid 和父进程的 pid。但一些必要的系统资源并不会拷贝过来，而是当需要写入的时候再做(copy-on-write)。进程调用 exit() 结束, 部分资源会释放，同时调用 exit_notify() 向父进程发信号。若父进程及时响应，此时释放所有的资源；否则认为此时的这个进程是僵尸进程。同理，如果父进程先于子进程退出，子进程就会成为孤儿进程。内核会将这类进程归给 pid 为 1 的进程。 线程线程是一种特殊的进程（强调只是 linux）同一个进程的 N 个线程只是 N 个共享同一块资源的task_struct。比如进程创建的时候会依赖 clone 方法 1clone(SIGCHLD, 0) 而线程的创建就是传递来一些参数来指明被共享的资源，这个设计现在看起来也是非常优雅的。 1clone(CLONE_VM | CLONE_FS | CLONE_FILES | CLONE_SIGHAND, 0)","categories":[],"tags":[{"name":"操作系统","slug":"操作系统","permalink":"https://razertory.me/tags/操作系统/"}]},{"title":"Scala 函数式编程设计","slug":"functional-program-design-in-scala","date":"2020-02-20T15:53:05.000Z","updated":"2020-03-28T13:02:10.018Z","comments":true,"path":"2020/02/20/functional-program-design-in-scala/","link":"","permalink":"https://razertory.me/2020/02/20/functional-program-design-in-scala/","excerpt":"","text":"在这个 2020 年初的特殊时期。 Scala 是一门现代化，多范式的 JVM 语言。 传送门（可能需要🚀) Functional Program Design in Scala on Coursera 没什么用的 certificate W1case class 和模式匹配用 Scala 来表示 JSON 对象可以这样 1234567abstract class JSONcase class JSeq (elems: List[JSON]) extends JSON // JSON 的数组case class JObj (bindings: Map[String, JSON]) extends JSON // JSON 对象case class JNum (num: Double) extends JSON // 数字类型case class JStr (str: String) extends JSONcase class JBool(b: Boolean) extends JSONcase object JNull extends JSON // 空 所以， 12345678910111213&#123; \"firstName\" : \"John\", \"lastName\" : \"Smith\", \"address\" : &#123; \"streetAddress\" : \"21 2 nd Street\", \"state\" : \"NY\", \"postalCode\": 10021 &#125;, \"phoneNumbers\": [ &#123;\"type\" : \"home\",\"number \": \"212 555 -1234\"&#125; , &#123;\"type\" : \"fax\",\"number \": \"646 555 -4567\"&#125; ]&#125; 可以表示为 123456789101112131415val data = JObj(Map( \"firstName\" -&gt; JStr(\"John\"), \"lastName\" -&gt; JStr(\"Smith\"), \"address\" -&gt; JObj(Map( \"streetAddress\" -&gt; JStr(\"21 2nd Street\"), \"state\" -&gt; JStr(\"NY\"), \"postalCode\" -&gt; JNum(10021))),\"phoneNumbers\" -&gt; JSeq(List( JObj(Map( \"type\" -&gt; JStr(\"home\"), \"number\" -&gt; JStr(\"212 555-1234\"))),JObj(Map( \"type\" -&gt; JStr(\"fax\"), \"number\" -&gt; JStr(\"646 555-4567\"))) )) )) 比如说我要实现一个打印 JSON 对象的方法，就可以 12345678910111213def show(json: JSON): String = json match &#123; case JSeq(elems) =&gt; \"[\" + (elems map show mkString \", \") + \"]\" case JObj(bindings) =&gt; val assocs = bindings map &#123; case (key, value) =&gt; \"\\\"\" + key + \"\\\": \" + show(value) &#125; \"&#123;\" + (assocs mkString \", \") + \"&#125;\" case JNum(num) =&gt; num.toString case JStr(str) =&gt; '\\\"' + str + '\\\"' case JBool(b) =&gt; b.toString case JNull =&gt; \"null\"&#125; 上述的 show(data) 就输出 1res1: String = &#123;\"firstName\": \"John\", \"lastName\": \"Smith\", \"address\": &#123;\"streetAddress\": \"21 2nd Street\", \"state\": \"NY\", \"postalCode\": 10021.0&#125;, \"phoneNumbers\": [&#123;\"type\": \"home\", \"number\": \"212 555-1234\"&#125;, &#123;\"type\": \"fax\", \"number\": \"646 555-4567\"&#125;]&#125; 第 6 行代码里面 map 传入的函数是，我们知道，在这里 map 函数的签名是 def map[B](f: (A) =&gt; B): List[B]那么第六行的 f 就是 case (key, value) =&gt; &quot;\\&quot;&quot; + key + &quot;\\&quot;: &quot; + show(value)。 这个函数如果单独拎出来，其实就是 1val f: (String, JSON) =&gt; String = &#123; case (key, value) =&gt; key + ”: ” + value &#125; // 注意看 JObj 定义 所以用 case class 最方便的地方在于模式匹配中的 decompose 函数即对象任何情况下，函数 A =&gt; B 其实就是 1scala.Function1[A, B] 的简写。 普通函数根据 Function1 的定义 123trait Function1[-A, +R] &#123; def apply(x: A): R&#125; 例如， 12val f1: Int =&gt; Int = (x: Int) =&gt; x + 1val f2 = new Function1[Int, Int] &#123;def apply(x: Int) = x + 1&#125; f1 和 f2 等价。 偏函数对于偏函数 PartialFunction 定义为 1234trait PartialFunction[-A, +R] extends Function1[-A, +R] &#123; def apply(x: A): R def isDefinedAt(x: A): Boolean&#125; 例如， 12345678910val f1: PartialFunction[String, String] = &#123; case \"ping\" =&gt; \"pong\" &#125;val f2 = new PartialFunction[String, String] &#123; def apply(x: String) = x match &#123; case \"ping\" =&gt; \"pong\" &#125; def isDefinedAt(x: String) = x match &#123; case \"ping\" =&gt; true case _ =&gt; false &#125;&#125; f1 和 f2 等价。 ps: 以上，「等价」的意思是两个函数只要输入相同，输出一定相同。 W2LazyList下面代码 1234567def f(x: Int): Boolean = &#123; println(x + \" \") x % 11 == 0&#125;val v1 = (1 to 100000).filter(f)(1)val v2 = (1 to 100000).to(LazyList).filter(f)(1) v1 和 v2 等价。计算 v1 的时候会打印 1 2 3...9999 100000, 而计算 v2 的时候打印 1 2 3 ... 21 22。这是因为对于 v2，(1 to 100000).to(LazyList) 产生的是 scala.collection.immutable.LazyList。其中官方文档对其有个简单的描述 This class implements an immutable linked list that evaluates elements in order and only when needed 也就是当 LazyList 里面的元素被需要的时候，才会进行有序计算，并且计算会终止知道所需的元素计算结束。这也就是为什么会有上述的输出了。 惰性求值 Lazy Evaluation在 Scala 里面，Lazy 意味着两点 尽量延后求值计算 只计算一次 例如 1234567def expr = &#123; val x = &#123; print(”x”); 1 &#125; lazy val y = &#123; print(”y”); 2 &#125; def z = &#123; print(”z”); 3 &#125; z + y + x + z + y + x // last line&#125;expr 这样的输出为 xzyz Infinite Sequences利用 LazyList 的特性，可以构造出无限序列。比如 1234567891011scala&gt; def from(n: Int): LazyList[Int] = n #:: from(n + 1) // 无限序列from: (n: Int)LazyList[Int]scala&gt; val natures = from(0) // 自然数natures: LazyList[Int] = LazyList(&lt;not computed&gt;)scala&gt; val hundreds = natures.filter(_ % 100 == 0)hundreds: scala.collection.immutable.LazyList[Int] = LazyList(&lt;not computed&gt;)scala&gt; val fourHundred = hundreds(4) // 计算fourHundred: Int = 400 埃拉托斯特尼筛法 埃拉托斯特尼筛法是列出所有小素数最有效的方法之一，其名字来自于古希腊数学家埃拉托斯特尼，并且被描述在另一位古希腊数学家尼科马库斯所著的《算术入门》中。-维基百科 用 LazyList 实现会非常简单， 12345678910111213def from(n: Int): LazyList[Int] = &#123; n #:: from(n + 1)&#125;def sieve(s: LazyList[Int]): LazyList[Int] = &#123; s.head #:: sieve(s.tail.filter(_ % s.head != 0))&#125;def primeNumber(n: Int): Int = &#123; sieve(from(2))(n)&#125;val n = sieve(from(2))(100) // 输出 547 题目：倒水问题给几个知道容量，但是没有刻度的杯子，有个可以取之不尽的水源用来给杯子装满水和将杯子的水全部倒出。我们需要做的就是盛出给定容量的水。我们能做的操作有三个 empty 清空 fill 放满水 pour(a, b) 把 a 的水全部往 b 倒直至 b 杯满了或者 a 杯没水了 现在需要找到取得容量为 N 的水时，需要的操作。 附：1. Scala 完整代码 2. python 解法 W3 下面会用 Scala 进行加法器模拟。对于计算机中，两个数如何相加，可以提前阅读 维基百科-加法器，最好能够做一下 位运算实现加法。 通过以上内容，我们需要知道的最重要的点在于：两个一位二进制数相加，将会产生两个输出，其中一个是当前位相加的值 S，另一个是相加后的进位信息 C。其中 S 可以用异或门，进位可以用与门。两个半加器用或门组合为一个全加器。 异或的操作通过三种门的组合实现 两个用与门组合的半加器构成一个全加器 根据上图，我们至少需要实现的是 电线：用来传导信号，信号可以用 boolean 类型表示 0 和 1 123456789class Wire &#123; private var sigVal = false def getSignal: Boolean = sigVal def setSignal(s: Boolean): Unit = if (s != sigVal) &#123; sigVal = s actions foreach(_()) &#125;&#125; 与门：输入两个电信号，输出 &amp; 对应的信号 1def andGate(in1: Wire, in2: Wire, output: Wire): Unit = ??? 或门：输入两个电信号，输出 | 对应的信号 1def orGate(in1: Wire, in2: Wire, output: Wire): Unit = ??? 逆变器：输入电信号，输出 ! 对应的信号 1def inverter(in: Wire, output: Wire): Unit = ??? 在此基础上，实现出半加器和全加器 12345678def halfAdder(a: Wire, b: Wire, s: Wire, c: Wire): Unit = &#123; val d = new Wire val e = new Wire orGate(a, b, d) andGate(a, b, c) inverter(c, e) andGate(d, e, s)&#125; 12345678def fullAdder(a: Wire, b: Wire, cin: Wire, sum: Wire, cout: Wire): Unit = &#123; val s = new Wire val c1 = new Wire val c2 = new Wire halfAdder(b, cin, s, c1) halfAdder(a, s, sum, c2) orGate(c1, c2, cout)&#125; 实现了全加器后。加入模拟信号的输入输出即可，完整代码 模拟加法器 W4Functional Reactive Programming (FRP)在 MVC 模型中，FRP 可以让 model 在产生变化的同时 view 自动变化。这样的模型一般也就是 pub-sub 模型或者叫观察者模型 (observers)。 12345Subscriber1 subscribe ___________Subscriber2 ------------&gt; | |Subscriber3 |Publisher |Subscriber4 &lt;------------ | |Subscriber5 publish |__________| 通用的说比如 a = f(b) 里面。当 b 在发生变化的时候 a 也会跟着变。实现这一点，可以把「变化」归化为「event」，而自动检查这些「event」的东西叫做 Signal。在 Scala 里面通常把这样的 Signal 实现为一个类型，通过 apply() 给其赋值。比如 12val s = Signal(1)val v = s() // 1 同时，为了给 Signal 再次赋值，可以扩展新增一个 update 方法。比如 s.update(5)，顺带可以写成 s() = 5。这样的话，实现一个扩展了 Signal 名为 Var 的类型，让其拥有 update 方法。 在这样的条件下， 12345678// (1)val num = Var(1)val twice = Signal(num() * 2)num() = 2// (2)var num = Var(1)val twice = Signal(num() * 2)num = Var(2) (1) 里面的 twice 变为 4，（2）里面的 twice 还是 2。上述体现的 FRP 里面，num 是 publisher，twice 是 subscriber。num 产生了 event，twice 接受这样的 event。并且所有的变量本身都是 immutable。 实现 FRPVar 和 Signal我们知道 Signal 和 Var 应该是这样 12345678910111213141516// signalclass Signal[T](expr: =&gt; T) &#123; def apply(): T = ???&#125;object Signal &#123; def apply[T](expr: =&gt; T) = new Signal(expr)&#125;// varclass Var[T](expr: =&gt; T) extends Signal[T](expr) &#123; def update(expr: =&gt; T): Unit = ???&#125;object Var &#123; def apply[T](expr: =&gt; T) = new Var(expr)&#125; 维护订阅关系有了这些方法，就可以做到赋值和再次赋值。还需要的是，当一个 publisher 产生了 event 的时候，如何自动通知到对应的 subscriber，并且让其重新计算（re-evaluate）。 例如，s 是 Var 类型，并且进入到了表达式 expr 里，形如 expr s，那么当表达式 expr s，作为参数传递给了某个 Signal 形如 val t = Signal(expr s) 的时候。就需要给 s 维护一个 subscriber。同理，当有多个 subscribers 的时候就给 s 维护一个 subscriber 的集合。比如 12345678910// val set = Set()var num = Var(1)// 触发的条件就是 class Var 的 apply 方法。// 触发：set += Signal(num() * 2)val double = Signal(num() * 2)// 触发：set += Signal(num() * 3)val tiple = Signal(num() * 3) 触发的条件达到了，可是如何把 Signal(num() * 3) 这样的对象传递给 num 的 set ？ 这个时候就需要在 Object Signal 里面新增一块用于维护方法调用的数据结构，没做就是 stack 1234object Signal &#123; val caller = new StackableVariable[Signal[_]](NoSignal) def apply[T](expr: =&gt; T) = new Signal(expr)&#125; StackableVariable 在当前的上下文中多个 Signal/Object 会共享。当调用 class Signal 的 apply() 方法的时候就去这里面找到当前的调用方。 维护调用栈调用栈里面存放有序的 Signal。例如，有下面测试代码 12345678910111213test(\"multi subscribers\") &#123; val publisher = Var(1) val subscriber1 = Signal(publisher() * 20) // 注意 scala 的 call-by-name 和 call-by-value。这里传递的是表达式 val subscriber2 = Signal(publisher() * 30) val subscriber3 = Signal(publisher() * 40) assert(subscriber1() == 20) assert(subscriber2() == 30) assert(subscriber3() == 40) publisher() = 2 assert(subscriber1() == 40) assert(subscriber2() == 60) assert(subscriber3() == 80)&#125; 第三行开始，包含表达式的 Signal 进入调用栈，并不断被带入 subscriber 的集合。 12345678910class StackableVariable[T](init: T) &#123; private var values: List[T] = List(init) def value: T = values.head def withValue[R](newValue: T)(op: =&gt; R): R = &#123; values = newValue :: values try op finally values = values.tail &#125;&#125; 完整代码","categories":[],"tags":[]},{"title":"LRU 缓存","slug":"lru-cache","date":"2020-02-16T09:39:04.000Z","updated":"2020-03-28T13:02:10.018Z","comments":true,"path":"2020/02/16/lru-cache/","link":"","permalink":"https://razertory.me/2020/02/16/lru-cache/","excerpt":"","text":"LRU 的全称是 Least recently used，是缓存替换策略的一种。电脑存储器空间的大小固定，无法容纳服务器上所有的文件，所以当有新的文件要被置换入缓存时，必须根据一定的原则来取代掉适当的文件。此原则即所谓缓存替换策略。传送门 实现 LRU 缓存需要用到 kv 类型和线型的数据结构，用来做到 O(1) 的查找效率和实现淘汰机制。我们可以用 HashMap 和一个双向链表。其中用双向链表的好处是：当要删除的节点为 node 的时候，通过 node.pre 和 node.next 可以快速找到前后节点，从而在链表中去掉这个节点。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889public class LRUCache &#123; class LinkNode &#123; int key; int value; LinkNode prev; LinkNode next; &#125; // 新增的 node 只作为 head private void addNode(LinkNode node) &#123; node.prev = head; node.next = head.next; head.next.prev = node; head.next = node; &#125; // 通过 prev 和 next 快速删除节点 private void removeNode(LinkNode node)&#123; LinkNode prev = node.prev; LinkNode next = node.next; prev.next = next; next.prev = prev; &#125; // 先删除，后新增从而指向头部 private void moveToHead(LinkNode node)&#123; removeNode(node); addNode(node); &#125; // 淘汰末尾 private LinkNode popTail() &#123; LinkNode res = tail.prev; removeNode(res); return res; &#125; private HashMap&lt;Integer, LinkNode&gt; cache = new HashMap&lt;Integer, LinkNode&gt;(); private int size; private int capacity; private LinkNode head, tail; public LRUCache(int capacity) &#123; this.size = 0; this.capacity = capacity; head = new LinkNode(); tail = new LinkNode(); head.next = tail; tail.prev = head; &#125; public int get(int key) &#123; LinkNode node = cache.get(key); if (node == null) return -1; moveToHead(node); return node.value; &#125; public void put(int key, int value) &#123; LinkNode node = cache.get(key); if(node == null) &#123; LinkNode newNode = new LinkNode(); newNode.key = key; newNode.value = value; cache.put(key, newNode); addNode(newNode); ++size; if(size &gt; capacity) &#123; LinkNode tail = popTail(); cache.remove(tail.key); --size; &#125; &#125; else &#123; node.value = value; moveToHead(node); &#125; &#125;&#125; 一种更为优雅的实现是让这个双向链表成环状，初始化为 value 都为 -1，大小为 capacity。每当有 get 和 put 操作的时候就调整环。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960class LRUCache &#123; private class Node &#123; int key, val; Node next, pre; Node(int key, int val, Node pre, Node next) &#123; this.key = key; this.val = val; this.pre = pre; this.next = next; &#125; &#125; private Node head = new Node(-1, -1, null, null); private HashMap&lt;Integer, Node&gt; map = new HashMap&lt;&gt;(); private void move2Head(Node cur)&#123; if (cur == head) &#123; head = head.pre; return; &#125; cur.pre.next = cur.next; cur.next.pre = cur.pre; cur.next = head.next; cur.next.pre = cur; head.next = cur; cur.pre = head; &#125; LRUCache(int capacity)&#123; Node cur = head; for (int i = 0; i &lt; capacity - 1; i++) &#123; cur.next = new Node(-1, -1, cur, null); cur = cur.next; &#125; cur.next = head; head.pre = cur; &#125; public int get(int key) &#123; if (!map.containsKey(key)) return -1; Node node = map.get(key); move2Head(node); return node.val; &#125; public void put(int key, int val) &#123; if (map.containsKey(key)) &#123; Node node = map.get(key); node.val = val; move2Head(node); &#125; else &#123; if (head.val != -1) map.remove(head.key); head.key = key; head.val = val; map.put(key, head); head = head.pre; &#125; &#125;&#125; 此外，Java 中的 LinkedHashMap 可以直接实现 LRU 缓存。:)","categories":[],"tags":[]},{"title":"编辑距离","slug":"edit-distance","date":"2020-01-11T03:35:32.000Z","updated":"2020-03-28T13:02:10.016Z","comments":true,"path":"2020/01/11/edit-distance/","link":"","permalink":"https://razertory.me/2020/01/11/edit-distance/","excerpt":"","text":"给出两个单词 word1 和 word2，找出把 word1 编辑成 word2 的最短编辑距离。给出的可用编辑操作有： 插入一个字符 insert 删除一个字符 delete 更新一个字符 update 编辑距离是针对二个字符串（例如英文字）的差异程度的量化量测，量测方式是看至少需要多少次的处理才能将一个字符串变成另一个字符串。编辑距离可以用在自然语言处理中，例如拼写检查可以根据一个拼错的字和其他正确的字的编辑距离，判断哪一个（或哪几个）是比较可能的字。DNA也可以视为用A、C、G和T组成的字符串，因此编辑距离也用在生物信息学中，判断二个DNA的类似程度。Unix 下的 diff 及 patch 即是利用编辑距离来进行文本编辑对比的例子。 传送门对于 “a”, “”, 执行步骤 1对于 “”, “a”, 执行步骤 2对于 “a”, “b”, 执行步骤 3 假设字符串串 word1 和 word2 的最小编辑距离是 d[i][j] (i 和 j 代表 word1 和 word2 的长度 - 1)，当我们给 word1 和 word2 都 append 一个相同的字符的时候，这个编辑距离不变。因此可以认为，此时的 d[i+1][j+1] == d[i][j]。当 append 的字符不同的的时候。就是找到之前的最小编辑距离 + 1，这个之前的最小编辑距离可能是 d[i][j]、d[i][j+1]、或者 d[i+1][j]。 Edit-Distance.png 1234567891011121314151617181920212223242526272829class Solution &#123; public int minDistance(String word1, String word2) &#123; if (word1 == null || word2 == null) return 0; int m = word1.length() + 1, n = word2.length() + 1; //多的一行留给空字符串 int[][] d = new int[m][n]; for (int i = 0; i &lt; m; i++) d[i][0] = i; for (int j = 0; j &lt; n; j++) d[0][j] = j; for(int i = 1; i &lt; m; ++i) &#123; for (int j = 1; j &lt; n; ++j) &#123; if (word1.charAt(i-1) == word2.charAt(j-1)) d[i][j] = d[i-1][j-1]; else d[i][j] = min(d[i-1][j-1], d[i][j-1], d[i-1][j]) + 1; &#125; &#125; return d[m-1][n-1]; &#125; private int min(int a, int b, int c) &#123; return Math.min(a, Math.min(b, c)); &#125;&#125;","categories":[],"tags":[]},{"title":"二维数组中和最小路径","slug":"minimum-path-sum","date":"2020-01-11T02:38:44.000Z","updated":"2020-03-28T13:02:10.017Z","comments":true,"path":"2020/01/11/minimum-path-sum/","link":"","permalink":"https://razertory.me/2020/01/11/minimum-path-sum/","excerpt":"","text":"在一个 m * n 大的非负组整数组成的二维数组中，找到一条从 left-top 到 right-bottom 的和最小的路径。输出这个路径的值。传送门 12345[ [1,3,1], [1,5,1], [4,2,1]] 在这样一个矩阵中，从 m[0][0] 到 m[2][2] 的最短路径，可以看作是 m[0][0] 到 m[1][2] 的最短路径 (1-&gt;3-&gt;1-&gt;1) 以及 m[2][1] 的最短路径中取到的最小值 (1-&gt;1-&gt;4-&gt;2) 加上 m[2][2] 取到的最小值 1-&gt;3-&gt;1-&gt;1-&gt;1。因此可以得到关系式 dp[m][n] = min(dp[m-1][n], dp[m][n-1]) + g[m][n] 利用这个关系式，处理好 m，n 为 0 的边界情况即可得出最短路径 123456789101112131415161718192021class Solution &#123; // dp[m][n] = min(dp[m-1][n], dp[m][n-1]) + g[m][n] public int minPathSum(int[][] g) &#123; if (g == null || g[0] == null) return 0; int h = g.length, w = g[0].length; int[][] dp = new int[h][w]; dp[0][0] = g[0][0]; for (int i = 0; i &lt; h; i++) for (int j = 0; j &lt; w; j++) &#123; if (i == 0 &amp;&amp; j == 0) continue; if (i == 0) dp[i][j] = dp[i][j-1] + g[i][j]; else if (j == 0) dp[i][j] = dp[i-1][j] + g[i][j]; else dp[i][j] = Math.min(dp[i-1][j], dp[i][j-1]) + g[i][j]; &#125; return dp[h-1][w-1]; &#125;&#125;","categories":[],"tags":[]},{"title":"最长回文子串","slug":"longest-palindromic-substring","date":"2020-01-11T02:00:36.000Z","updated":"2020-03-28T13:02:10.017Z","comments":true,"path":"2020/01/11/longest-palindromic-substring/","link":"","permalink":"https://razertory.me/2020/01/11/longest-palindromic-substring/","excerpt":"","text":"给一个字符串 s，找到其中最长的回文串。传送门 回文串有两种模式，一种是 aabb 类型，另一种是 aacbb 类型。当我们对一个字符串做是否是回文的时候可以实现一个 expand 函数用作判断int expand(String s, int lelf, int right) 当 s[left] == s[right] 且 left 和 right 没有到达字符串边界的时候计算长度即 right - left + 1。 实现了 expand 之后，对于任意字符串可以确认： 字符串长度为 1 的时候，本身就是回文串 字符串长度为 2 的时候，判断两个字符是否相等，相等说明本身是，否则和 1 一致 字符串长度为 n 的时候，在对字符串进行从左到右扫描找到最大值 max (expand(s, i, i), expand(s, i, i + 1)) 1234567891011121314151617181920212223class Solution &#123; public String longestPalindrome(String s) &#123; int start = 0, max = 0; for (int i = 0; i &lt; s.length(); i++) &#123; int len1 = expand(s, i, i); int len2 = expand(s, i, i + 1); int len = Math.max(len1, len2); if (len &gt; max) &#123; start = i - (len-1)/2; max = len; &#125; &#125; return s.substring(start, start + max); &#125; int expand(String s, int left, int right) &#123; while(left &gt;= 0 &amp;&amp; right &lt; s.length() &amp;&amp; s.charAt(left) == s.charAt(right))&#123; left--; right++; &#125; return right - left - 1; &#125;&#125;","categories":[],"tags":[]},{"title":"抢劫房子","slug":"house-robber","date":"2020-01-11T01:33:19.000Z","updated":"2020-03-28T13:02:10.017Z","comments":true,"path":"2020/01/11/house-robber/","link":"","permalink":"https://razertory.me/2020/01/11/house-robber/","excerpt":"","text":"你是一个专业的小偷，计划偷窃沿街的房屋。每间房内都藏有一定的现金，影响你偷窃的唯一制约因素就是相邻的房屋装有相互连通的防盗系统，如果两间相邻的房屋在同一晚上被小偷闯入，系统会自动报警。给定一个代表每个房屋存放金额的非负整数数组，计算你在不触动警报装置的情况下，能够偷窃到的最高金额。 传送门 假设房子定义为 house[i] 房屋数量是 1 的时候，小偷只能偷这一家 house[0]。 房屋数量是 2 的时候，小偷从现有的选一个多的，也就是 max(house[0], house[1]) 房屋数量是 3 的时候，小偷的选择为 max(house[0] + house[2], house[1]) 如果我们知道房屋数量为 i - 1 的时候，小偷的最佳选择是 d[i-1]，那么当再增加一个房屋，即房屋数量为 i 的时候。依据题意可以推导出 1d[i] = max(house[i] + d[i-2], d[i-1]) 翻译成代码就是 123456789101112class Solution &#123; public int rob(int[] nums) &#123; if (nums == null || nums.length == 0) return 0; int[] d = new int[nums.length]; for (int i = 0; i &lt; nums.length; i++) &#123; if (i == 0) d[i] = nums[i]; else if (i == 1) d[i] = Math.max(nums[i], nums[i-1]); else d[i] = Math.max(d[i-2] + nums[i], d[i-1]); &#125; return d[nums.length - 1]; &#125;&#125;","categories":[],"tags":[]},{"title":"买卖股票的最大利润","slug":"max-stock-profit","date":"2020-01-09T05:02:03.000Z","updated":"2020-03-28T13:02:10.016Z","comments":true,"path":"2020/01/09/max-stock-profit/","link":"","permalink":"https://razertory.me/2020/01/09/max-stock-profit/","excerpt":"","text":"假设把某股票的价格按照时间先后顺序存储在数组中，请问买卖 一次 该股票可能获得的利润是多少？ 例如一只股票在某些时间节点的价格为 [9, 11, 8, 5, 7, 12, 16, 14]。 如果我们能在价格为 5 的时候买入并在价格为 16 时卖出，则能收获最大的利润 11。传送门 股票最大利润，其实就是个从头到尾一直更新最大利润。最大利润 = Math.max(前面的最大利润，当前股票价格 - 前面最低价格) 123456789public int maxProfit(int[] nums) &#123; if (nums == null || nums.length == 0) return 0; int min = nums[0], maxProfit = 0; for (int i = 1; i &lt; nums.length; i++) &#123; min = Math.min(min, nums[i]); maxProfit = Math.max(maxProfit, nums[i] - min); &#125; return maxProfit;&#125;","categories":[],"tags":[]},{"title":"2020 新年快乐","slug":"happy-new-year-2020","date":"2020-01-01T04:25:41.000Z","updated":"2020-03-28T13:02:10.016Z","comments":true,"path":"2020/01/01/happy-new-year-2020/","link":"","permalink":"https://razertory.me/2020/01/01/happy-new-year-2020/","excerpt":"","text":"现在是 2020 年的一月一日，和普通放假的日常一样：1. KFC，2.电脑， 3.看书/编程。和之前不一样的是，咖啡点的是加冰的美式。回想起过去的一年，不在办公室的时候，我通常也会去找一个地方学习，常做的事情有 刷课。MIT 6.824、SICP 那种 刷 OJ。通常是 Leetcode 或者 codewars，其中在 leetcode 上打了一段时间的周赛 看书。通常都是计算机相关的书籍，最近看的是 FP 相关 写博客。和现在一样 这些事情都是我换个人喜欢做的，但是对外却有优先级。比如说如果我最近要参加面试，我可能会花更多的时间在刷 OJ；如果我最近要用一门新语言/框架，我可能会选择看书；如果我最近时间比较充裕且没有外部的需求，那么我会像现在一样写博客或者用一段连续且足够长的时间来刷课程。 这么做既满足了自己的喜好，也能够根据外部的状态及调整。 随着时间的推移，我越来越觉得自己即将成为那种很标准的「职场码农」。快速完成外部的需求，给团队带来价值，快速学习一门新技术，快速补偿技术债。但是我自己也不得不承认，因为这种策略，我到目前没有那种能做到 top 的事情。首先解释一下做到 top：比如说刷 OJ，我可以通过周赛或者自己的积分排名来判断，或者我可以去给某个知名开源的框架做贡献，通过 contributors 排名来看自己做的程度。做到 top 有时候不仅仅是因为自己多多少少有「追名逐利」的情结，更因为我坚持认为，如果喜欢做一件事情，肯定有把事情做好的意愿，那么只要不断坚持做自然就会做到 top。 前段时间的一个周末，我去了一家摊饼店，拍下了一行字 如果说，一个人可以一辈子专注于一件事情上，并且不会被别的事情所干扰或者诱惑应该是非常值得羡慕的。可是如果用资本家的眼光去看待这个，肯定会让人觉得这个人只知道打工(这里不把打工做贬义)。我记得在「泰囧」电影里面有个对白 资本家热爱「挣钱」本身这件事情，所以会用经营、扩张的眼光去看待做葱油饼。葱油饼小哥说秘方就是要自己亲自做，新鲜出炉，这是热爱做葱油饼人的眼光。不过这个世界就是需要这两种人。 如果把「做葱油饼」换成「写代码」，这个对白产生的效果应该是类似的。 我想说不论是为了更高的薪水不断学习，还是因为热爱编程本身不断学习，这两者应该是都被认可的。在现实世界中，我认为这两种动机应该是都存在的，或者说人本身就是这样的，既希望做的事情是自己喜欢的，也希望做的事情能给自己带来更多收入。 2020年，我们是野性和文明共存的。 新年快乐！","categories":[],"tags":[{"name":"思考","slug":"思考","permalink":"https://razertory.me/tags/思考/"}]},{"title":"有序数组转化为平衡二叉搜索树（BST）","slug":"sorted-arr-to-bst","date":"2019-12-12T13:41:57.000Z","updated":"2020-03-28T13:02:10.016Z","comments":true,"path":"2019/12/12/sorted-arr-to-bst/","link":"","permalink":"https://razertory.me/2019/12/12/sorted-arr-to-bst/","excerpt":"","text":"给定一个有序数组，数组元素升序排列，试将该数组转换为一棵平衡二叉搜索树（Balanced Binary Search Tree）。 1TreeNode convertToBST(ArrayList&lt;Integer&gt; arr) 根据 BST 的定义 二叉排序树的查找过程和次优二叉树类似，通常采取二叉链表作为二叉排序树的存储结构。中序遍历二叉排序树可得到一个关键字的有序序列，一个无序序列可以通过构造一棵二叉排序树变成一个有序序列，构造树的过程即为对无序序列进行排序的过程。每次插入的新的结点都是二叉排序树上新的叶子结点，在进行插入操作时，不必移动其它结点，只需改动某个结点的指针，由空变为非空即可。搜索,插入,删除的复杂度等于树高，O(log(n)). 可以对其进行反中序的遍历。也就是 中-左-右 的一个逆向过程 12345678910111213TreeNode convertToBST(ArrayList&lt;Integer&gt; arr) &#123; if (arr == null || arr.length == 0) return null; return convert(arr, 0, arr.length - 1);&#125;TreeNode convert(ArrayList&lt;Integer&gt; arr, int start, int end) &#123; if (start &gt; end) return null; int mid = start + (end - start) / 2; TreeNode node = new TreeNode(arr[mid]); node.left = convert(arr, start, mid - 1); node.right = convert(arr, mid + 1, end); return node;&#125;","categories":[],"tags":[]},{"title":"高级类型","slug":"scala-school-advanced-types","date":"2019-12-01T03:20:54.000Z","updated":"2020-05-04T07:02:15.759Z","comments":true,"path":"2019/12/01/scala-school-advanced-types/","link":"","permalink":"https://razertory.me/2019/12/01/scala-school-advanced-types/","excerpt":"","text":"视界（View Bound “类型类”）有时候，你并不需要指定一个类型是等/子/超于另一个类，你可以通过转换这个类来伪装这种关联关系。一个视界指定一个类型可以被“看作是”另一个类型。这对对象的只读操作是很有用的。 隐式 (implicit) 函数允许类型自动转换。更确切地说，如果隐式函数有助于满足类型推断时，隐式函数可以按需地应用。例如： 1234567891011scala&gt; implicit def strToInt(x: String) = x.toIntstrToInt: (x: String)Intscala&gt; \"123\"res0: java.lang.String = 123scala&gt; val y: Int = \"123\"y: Int = 123scala&gt; math.max(\"123\", 111)res1: Int = 123 视界，就像类型边界，要求存在一个能够将某类型转换为指定类型的函数。你可以使用 &lt;% 指定类型限制，例如： 1234567891011121314scala&gt; class Container[A &lt;% Int] &#123; def addIt(x: A) = 123 + x &#125;defined class Container这是说 A 必须“可被视作” Int 。让我们试试。scala&gt; (new Container[String]).addIt(\"123\")res11: Int = 246scala&gt; (new Container[Int]).addIt(123) res12: Int = 246scala&gt; (new Container[Float]).addIt(123.2F)&lt;console&gt;:8: error: could not find implicit value for evidence parameter of type (Float) =&gt; Int (new Container[Float]).addIt(123.2) ^ 注意，新版本的 Scala 可能会提示： view bounds are deprecated; use an implicit parameter instead. 其他类型限制方法可以通过隐式参数执行更复杂的类型限制。例如，List 支持对数字内容执行 sum，但对其他内容却不行。可是 Scala 的数字类型并不都共享一个超类，所以我们不能使用 T &lt;: Number。相反，要使之能工作，Scala 的 math 库对适当的类型 T 定义了一个隐含的 Numeric[T]。 然后在 List 定义中使用它： 1sum[B &gt;: A](implicit num: Numeric[B]): B 如果你调用 List(1,2).sum()，你并不需要传入一个 num 参数；它是隐式设置的。但如果你调用 List(“whoop”).sum()，它会抱怨无法设置 num。 在没有设定陌生的对象为 Numeric 的时候，方法可能会要求某种特定类型的“证据”。这时可以使用以下类型-关系运算符： A =:= B A 必须和 B 相等 A &lt;:&lt; B A 必须是 B 的子类 A &lt;%&lt; B A 必须可以被看做是 B （如果你在尝试使用 &lt;:&lt; 或者 &lt;%&lt; 的时候出错了，那请注意这些符号在 Scala 2.10 中被移除了。Scala School 里的例子仅能在 Scala 2.9.x 下正常工作。你可以使用新版本的 Scala，但可能会遇到错误。） 12345678scala&gt; class Container[A](value: A) &#123; def addIt(implicit evidence: A =:= Int) = 123 + value &#125;defined class Containerscala&gt; (new Container(123)).addItres11: Int = 246scala&gt; (new Container(\"123\")).addIt&lt;console&gt;:10: error: could not find implicit value for parameter evidence: =:=[java.lang.String,Int] 类似地，根据之前的隐式转换，我们可以将约束放松为可视性： 12345scala&gt; class Container[A](value: A) &#123; def addIt(implicit evidence: A &lt;%&lt; Int) = 123 + value &#125;defined class Containerscala&gt; (new Container(\"123\")).addItres15: Int = 246 使用视图 (View) 进行泛型编程在 Scala 标准库中，视图主要用于实现集合的通用函数。例如 min 函数（在 Seq[] 上）就使用了这种技术： 123456def min[B &gt;: A](implicit cmp: Ordering[B]): A = &#123; if (isEmpty) throw new UnsupportedOperationException(\"empty.min\") reduceLeft((x, y) =&gt; if (cmp.lteq(x, y)) x else y)&#125; 其主要优点是： 集合中的元素不必实现 Ordered trait，但 Ordered 的使用仍然可以执行静态类型检查。无需任何额外的库支持，你也可以定义自己的排序： 12345scala&gt; List(1,2,3,4).minres0: Int = 1scala&gt; List(1,2,3,4).min(new Ordering[Int] &#123; def compare(a: Int, b: Int) = b compare a &#125;)res3: Int = 4 作为旁注，标准库中有视图来将 Ordered 转换为 Ordering（反之亦然）。 12345trait LowPriorityOrderingImplicits &#123; implicit def ordered[A &lt;: Ordered[A]]: Ordering[A] = new Ordering[A] &#123; def compare(x: A, y: A) = x.compare(y) &#125;&#125; 上下文边界和 implicitly[]Scala 2.8 引入了一种串联和访问隐式参数的简单记法。 12345scala&gt; def foo[A](implicit x: Ordered[A]) &#123;&#125;foo: [A](implicit x: Ordered[A])Unitscala&gt; def foo[A : Ordered] &#123;&#125; foo: [A](implicit evidence$1: Ordered[A])Unit 隐式值可以通过 implicitly 被访问 12scala&gt; implicitly[Ordering[Int]]res37: Ordering[Int] = scala.math.Ordering$Int$@3a9291cf 相结合后往往会使用更少的代码，尤其是串联视图的时候。 高阶多态性类型和特设多态性Scala 可以对“高阶”的类型进行抽象。例如，假设你需要用几种类型的容器处理几种类型的数据。你可能定义了一个 Container 的接口，它可以被实现为几种类型的容器：Option、List 等。你要定义可以使用这些容器里的值的接口，但不想确定值的类型。 这类似于函数柯里化。例如，尽管“一元类型”有类似 List[A] 的构造器，这意味着我们必须满足一个“级别”的类型变量来产生一个具体的类型（就像一个没有柯里化的函数需要只提供一个参数列表来被调用），更高阶的类型需要更多。 12345678910scala&gt; trait Container[M[_]] &#123; def put[A](x: A): M[A]; def get[A](m: M[A]): A &#125;scala&gt; val container = new Container[List] &#123; def put[A](x: A) = List(x); def get[A](m: List[A]) = m.head &#125;container: java.lang.Object with Container[List] = $anon$1@7c8e3f75scala&gt; container.put(\"hey\")res24: List[java.lang.String] = List(hey)scala&gt; container.put(123)res25: List[Int] = List(123) 注意：Container 是参数化类型的多态（“容器类型”）。 如果我们结合隐式转换 implicits 使用容器，我们会得到“特设的”多态性：即对容器写泛型函数的能力。 1234567891011121314151617scala&gt; trait Container[M[_]] &#123; def put[A](x: A): M[A]; def get[A](m: M[A]): A &#125;scala&gt; implicit val listContainer = new Container[List] &#123; def put[A](x: A) = List(x); def get[A](m: List[A]) = m.head &#125;scala&gt; implicit val optionContainer = new Container[Some] &#123; def put[A](x: A) = Some(x); def get[A](m: Some[A]) = m.get &#125;scala&gt; def tupleize[M[_]: Container, A, B](fst: M[A], snd: M[B]) = &#123; | val c = implicitly[Container[M]] | c.put(c.get(fst), c.get(snd)) | &#125;tupleize: [M[_],A,B](fst: M[A],snd: M[B])(implicit evidence$1: Container[M])M[(A, B)]scala&gt; tupleize(Some(1), Some(2))res33: Some[(Int, Int)] = Some((1,2))scala&gt; tupleize(List(1), List(2))res34: List[(Int, Int)] = List((1,2)) F-界多态性通常有必要来访问一个泛型 trait 的具体子类。例如，想象你有一些泛型 trait，但需要可以与它的某一子类进行比较。 1trait Container extends Ordered[Container] 现在子类必须实现 compare 方法 1def compare(that: Container): Int 但我们不能访问具体子类型，例如： 123class MyContainer extends Container &#123; def compare(that: MyContainer): Int&#125; 编译失败，因为我们对 Container 指定了 Ordered 特质，而不是对特定子类型指定的。 一个可选的解决方案是将 Container 参数化，以便我们能在子类中访问其子类型。 1trait Container[A] extends Ordered[A] 现在子类可以这样做： 123class MyContainer extends Container[MyContainer] &#123; def compare(that: MyContainer): Int&#125; 但问题在于 A 类型没有被任何东西约束，这导致你可能会做类似这样的事情： 123class MyContainer extends Container[String] &#123; def compare(that: String): Int&#125; 为了调和这一点，我们改用 F-界的多态性。 1trait Container[A &lt;: Container[A]] extends Ordered[A] 奇怪的类型！但注意现在如何用 A 作为 Ordered 的类型参数，而 A 本身就是 Container[A] 所以，现在 123class MyContainer extends Container[MyContainer] &#123; def compare(that: MyContainer) = 0 &#125; 他们是有序的了： 12345scala&gt; List(new MyContainer, new MyContainer, new MyContainer)res3: List[MyContainer] = List(MyContainer@30f02a6d, MyContainer@67717334, MyContainer@49428ffa)scala&gt; List(new MyContainer, new MyContainer, new MyContainer).minres4: MyContainer = MyContainer@33dfeb30 鉴于他们都是 Container[_] 的子类型，我们可以定义另一个子类并创建 Container[_] 的一个混合列表： 123456scala&gt; class YourContainer extends Container[YourContainer] &#123; def compare(that: YourContainer) = 0 &#125;defined class YourContainerscala&gt; List(new MyContainer, new MyContainer, new MyContainer, new YourContainer) res2: List[Container[_ &gt;: YourContainer with MyContainer &lt;: Container[_ &gt;: YourContainer with MyContainer &lt;: ScalaObject]]] = List(MyContainer@3be5d207, MyContainer@6d3fe849, MyContainer@7eab48a7, YourContainer@1f2f0ce9) 注意现在结果类型是由 YourContainer with MyContainer 类型确定的下界。这是类型推断器的工作。有趣的是，这种类型甚至不需要是有意义的，它仅仅对列表的统一类型提供了一个逻辑上最优的下界。如果现在我们尝试使用 Ordered 会发生什么？ 123(new MyContainer, new MyContainer, new MyContainer, new YourContainer).min&lt;console&gt;:9: error: could not find implicit value for parameter cmp: Ordering[Container[_ &gt;: YourContainer with MyContainer &lt;: Container[_ &gt;: YourContainer with MyContainer &lt;: ScalaObject]]] 对统一的类型 Ordered[] 不存在了。太糟糕了。 结构类型Scala 支持结构类型（structural types） — 类型需求由接口结构表示，而不是由具体的类型表示。 12345scala&gt; def foo(x: &#123; def get: Int &#125;) = 123 + x.getfoo: (x: AnyRef&#123;def get: Int&#125;)Intscala&gt; foo(new &#123; def get = 10 &#125;) res0: Int = 133 这可能在很多场景都是相当不错的，但这个实现中使用了反射，所以要注意性能！ 抽象类型成员在特质中，你可以让类型成员保持抽象。 12345678scala&gt; trait Foo &#123; type A; val x: A; def getX: A = x &#125;defined trait Fooscala&gt; (new Foo &#123; type A = Int; val x = 123 &#125;).getX res3: Int = 123scala&gt; (new Foo &#123; type A = String; val x = \"hey\" &#125;).getXres4: java.lang.String = hey 在做依赖注入等情况下，这往往是一个有用的技巧。 你可以使用 hash 操作符来引用一个抽象类型的变量： 12345scala&gt; trait Foo[M[_]] &#123; type t[A] = M[A] &#125;defined trait Fooscala&gt; val x: Foo[List]#t[Int] = List(1)x: List[Int] = List(1) 类型擦除和清单正如我们所知道的，类型信息在编译的时候会因为擦除而丢失。 Scala 提供了清单（Manifests）功能，使我们能够选择性地恢复类型信息。清单作为一个隐式的值被提供，它由编译器根据需要生成。 1234scala&gt; class MakeFoo[A](implicit manifest: Manifest[A]) &#123; def make: A = manifest.erasure.newInstance.asInstanceOf[A] &#125;scala&gt; (new MakeFoo[String]).makeres10: String = \"\" 案例分析：Finagle参见：https://github.com/twitter/finagle 123456789101112131415161718192021222324trait Service[-Req, +Rep] extends (Req =&gt; Future[Rep])trait Filter[-ReqIn, +RepOut, +ReqOut, -RepIn] extends ((ReqIn, Service[ReqOut, RepIn]) =&gt; Future[RepOut])&#123; def andThen[Req2, Rep2](next: Filter[ReqOut, RepIn, Req2, Rep2]) = new Filter[ReqIn, RepOut, Req2, Rep2] &#123; def apply(request: ReqIn, service: Service[Req2, Rep2]) = &#123; Filter.this.apply(request, new Service[ReqOut, RepIn] &#123; def apply(request: ReqOut): Future[RepIn] = next(request, service) override def release() = service.release() override def isAvailable = service.isAvailable &#125;) &#125; &#125; def andThen(service: Service[ReqOut, RepIn]) = new Service[ReqIn, RepOut] &#123; private[this] val refcounted = new RefcountedService(service) def apply(request: ReqIn) = Filter.this.apply(request, refcounted) override def release() = refcounted.release() override def isAvailable = refcounted.isAvailable &#125; &#125; 一个服务可以通过过滤器对请求进行身份验证。 12345678910111213141516trait RequestWithCredentials extends Request &#123; def credentials: Credentials&#125;class CredentialsFilter(credentialsParser: CredentialsParser) extends Filter[Request, Response, RequestWithCredentials, Response]&#123; def apply(request: Request, service: Service[RequestWithCredentials, Response]): Future[Response] = &#123; val requestWithCredentials = new RequestWrapper with RequestWithCredentials &#123; val underlying = request val credentials = credentialsParser(request) getOrElse NullCredentials &#125; service(requestWithCredentials) &#125;&#125; 注意底层服务是如何需要对请求进行身份验证的，而且还是静态验证。因此，过滤器可以被看作是服务转换器。 许多过滤器可以被组合在一起： 1234567val upFilter = logTransaction andThen handleExceptions andThen extractCredentials andThen homeUser andThen authenticate andThen route 享用安全的类型吧！","categories":[],"tags":[]},{"title":"类型和多态基础","slug":"scala-school-type-basics","date":"2019-12-01T02:47:32.000Z","updated":"2020-05-04T07:03:04.626Z","comments":true,"path":"2019/12/01/scala-school-type-basics/","link":"","permalink":"https://razertory.me/2019/12/01/scala-school-type-basics/","excerpt":"","text":"什么是静态类型？它们为什么有用？ 按 Pierce 的话讲：“类型系统是一个语法方法，它们根据程序计算的值的种类对程序短语进行分类，通过分类结果错误行为进行自动检查。” 另外，笔者推荐阅读 编程语言的类型系统为何如此重要？ 类型允许你表示函数的定义域和值域。例如，从数学角度看这个定义： f: R -&gt; N // 它告诉我们函数 f 是从实数集到自然数集的映射。 抽象地说，这就是具体类型的准确定义。类型系统给我们提供了一些更强大的方式来表达这些集合。鉴于这些注释，编译器可以静态地（在编译时）验证程序是合理的。也就是说，如果值（在运行时）不符合程序规定的约束，编译将失败。一般说来，类型检查只能保证不合理的程序不能编译通过。它不能保证每一个合理的程序都可以编译通过。 随着类型系统表达能力的提高，我们可以生产更可靠的代码，因为它能够在我们运行程序之前验证程序的不变性（当然是发现类型本身的模型 bug！）。学术界一直很努力地提高类型系统的表现力，包括值依赖（value-dependent）类型！ 需要注意的是，所有的类型信息会在编译时被删去，因为它已不再需要。这就是所谓的类型擦除。 Scala 中的类型Scala 强大的类型系统拥有非常丰富的表现力。其主要特性有： 参数化多态性 (parametric polymorphism )，粗略地说，就是泛型编程 局部类型推断，粗略地说，就是为什么你不需要这样写代码 val i: Int = 12: Int *存在量化 (existential quantification) *粗略地说，为一些没有名称的类型进行定义 *视图 (views) *粗略地说，就是将一种类型的值“强制转换”为另一种类型 参数化多态性多态性是在不影响静态类型丰富性的前提下，用来（给不同类型的值）编写通用代码的。 例如，如果没有参数化多态性，一个通用的列表数据结构总是看起来像这样（事实上，它看起来很像使用泛型前的 Java）： 12scala&gt; 2 :: 1 :: \"bar\" :: \"foo\" :: Nilres5: List[Any] = List(2, 1, bar, foo) 现在我们无法恢复其中成员的任何类型信息。 12scala&gt; res5.headres6: Any = 2 所以我们的应用程序将会退化为一系列类型转换（“asInstanceOf[]”），并且会缺乏类型安全的保障（因为这些都是动态的）。 多态性是通过指定类型变量实现的。 12345scala&gt; def drop1[A](l: List[A]) = l.taildrop1: [A](l: List[A])List[A]scala&gt; drop1(List(1,2,3))res1: List[Int] = List(2, 3) Scala 有 rank-1 多态性 (rank-1 polymorphism)粗略地说，这意味着在 Scala 中，有一些你想表达的类型概念“过于泛化”（泛化的层级大于 1) 以至于编译器无法理解。假设你有一个函数 1def toList[A](a: A) = List(a) 你希望继续泛型地使用它： 1def foo[A, B](f: A =&gt; List[A], b: B) = f(b) 这段代码不能编译，因为所有的类型变量只有在调用上下文中才被固定。即使你“钉住”了类型 B： 1def foo[A](f: A =&gt; List[A], i: Int) = f(i) 你也会得到一个类型不匹配的错误。 类型推断静态类型的一个传统反对意见是，它有大量的语法开销。Scala 通过类型推断来缓解这个问题。 在函数式编程语言中，类型推断的经典方法是 Hindley Milner 算法，它最早是实现在 ML 中的。 Scala 类型推断系统的实现稍有不同，但本质类似：推断约束，并试图统一类型。 例如，在 Scala 中你无法这样做： 123scala&gt; &#123; x =&gt; x &#125;&lt;console&gt;:7: error: missing parameter type &#123; x =&gt; x &#125; 而在 OCaml 中你可以： 12# fun x -&gt; x;;- : 'a -&gt; 'a = &lt;fun&gt; 在 Scala 中所有类型推断是局部的。Scala 一次分析一个表达式。例如： 1234567891011scala&gt; def id[T](x: T) = xid: [T](x: T)Tscala&gt; val x = id(322)x: Int = 322scala&gt; val x = id(\"hey\")x: java.lang.String = heyscala&gt; val x = id(Array(1,2,3,4))x: Array[Int] = Array(1, 2, 3, 4) 类型信息都保存完好，Scala 编译器为我们进行了类型推断。请注意我们并不需要明确指定返回类型。 变性 (Variance)Scala 的类型系统必须同时解释类层次和多态性。类层次结构可以表达子类关系。在混合 OO 和多态性时，一个核心问题是：如果 T’是 T 一个子类，Container[T’] 应该被看做是 Container[T] 的子类吗？变性（Variance）注解允许你表达类层次结构和多态类型之间的关系。 含义 Scala 标记 协变 (covariant) C[T’] 是 C[T] 的子类 [+T] 逆变 (contravariant) C[T] 是 C[T’] 的子类 [-T] 不变 (invariant) C[T] 和 C[T’] 无关 [T] 逆变协变更多信息参考维基百科 子类型关系的真正含义：对一个给定的类型 T，如果 T’是其子类型，你能替换它吗？ 协变123456789101112scala&gt; class Covariant[+A]defined class Covariantscala&gt; val cv: Covariant[AnyRef] = new Covariant[String]cv: Covariant[AnyRef] = Covariant@4035acf6scala&gt; val cv: Covariant[String] = new Covariant[AnyRef]&lt;console&gt;:6: error: type mismatch; found : Covariant[AnyRef] required: Covariant[String] val cv: Covariant[String] = new Covariant[AnyRef] ^ 逆变123456789101112scala&gt; class Contravariant[-A]defined class Contravariantscala&gt; val cv: Contravariant[String] = new Contravariant[AnyRef]cv: Contravariant[AnyRef] = Contravariant@49fa7bascala&gt; val fail: Contravariant[AnyRef] = new Contravariant[String]&lt;console&gt;:6: error: type mismatch; found : Contravariant[String] required: Contravariant[AnyRef] val fail: Contravariant[AnyRef] = new Contravariant[String] ^ 逆变似乎很奇怪。什么时候才会用到它呢？令人惊讶的是，函数特质的定义就使用了它！ 1trait Function1 [-T1, +R] extends AnyRef 如果你仔细从替换的角度思考一下，会发现它是非常合理的。让我们先定义一个简单的类层次结构： 12345678scala&gt; class Animal &#123; val sound = \"rustle\" &#125;defined class Animalscala&gt; class Bird extends Animal &#123; override val sound = \"call\" &#125;defined class Birdscala&gt; class Chicken extends Bird &#123; override val sound = \"cluck\" &#125;defined class Chicken 假设你需要一个以 Bird 为参数的函数： 1scala&gt; val getTweet: (Bird =&gt; String) = // TODO 标准动物库有一个函数满足了你的需求，但它的参数是 Animal。在大多数情况下，如果你说“我需要一个，我有一个的子类”是可以的。但是，在函数参数这里是逆变的。如果你需要一个接受参数类型 Bird 的函数变量，但却将这个变量指向了接受参数类型为 Chicken 的函数，那么给它传入一个 Duck 时就会出错。然而，如果将该变量指向一个接受参数类型为 Animal 的函数就不会有这种问题： 12scala&gt; val getTweet: (Bird =&gt; String) = ((a: Animal) =&gt; a.sound )getTweet: Bird =&gt; String = &lt;function1&gt; 函数的返回值类型是协变的。如果你需要一个返回 Bird 的函数，但指向的函数返回类型是 Chicken，这当然是可以的。 12scala&gt; val hatch: (() =&gt; Bird) = (() =&gt; new Chicken )hatch: () =&gt; Bird = &lt;function0&gt; 所以对于 Scala 中的函数而言，在入参逆变，返回值协变的基础上一切都会变得灵活而严谨。 边界Scala 允许你通过边界来限制多态变量。这些边界表达了子类型关系。 12345678910scala&gt; def cacophony[T](things: Seq[T]) = things map (_.sound)&lt;console&gt;:7: error: value sound is not a member of type parameter T def cacophony[T](things: Seq[T]) = things map (_.sound) ^scala&gt; def biophony[T &lt;: Animal](things: Seq[T]) = things map (_.sound)biophony: [T &lt;: Animal](things: Seq[T])Seq[java.lang.String]scala&gt; biophony(Seq(new Chicken, new Bird))res5: Seq[java.lang.String] = List(cluck, call) 类型下界也是支持的，这让逆变和巧妙协变的引入得心应手。List[+T] 是协变的；一个 Bird 的列表也是 Animal 的列表。List 定义一个操作 :: (elem T) 返回一个加入了 elem 的新的 List。新的 List 和原来的列表具有相同的类型： 12345scala&gt; val flock = List(new Bird, new Bird)flock: List[Bird] = List(Bird@7e1ec70e, Bird@169ea8d2)scala&gt; new Chicken :: flockres53: List[Bird] = List(Chicken@56fbda05, Bird@7e1ec70e, Bird@169ea8d2) List 同样 定义了 :: [B &gt;: T](x: B) 来返回一个 List[B]。请注意 B &gt;: T，这指明了类型 B 为类型 T 的超类。这个方法让我们能够做正确地处理在一个 List[Bird] 前面加一个 Animal 的操作： 12scala&gt; new Animal :: flockres59: List[Animal] = List(Animal@11f8d3a8, Bird@7e1ec70e, Bird@169ea8d2) 注意返回类型是 Animal。 量化有时候，你并不关心是否能够命名一个类型变量，例如： 12scala&gt; def count[A](l: List[A]) = l.sizecount: [A](List[A])Int 这时你可以使用通配符 _ 取而代之： 12scala&gt; def count(l: List[_]) = l.sizecount: (List[_])Int 这相当于是下面代码的简写： 12scala&gt; def count(l: List[T forSome &#123; type T &#125;]) = l.sizecount: (List[T forSome &#123; type T &#125;])Int 注意量化会的结果会变得非常难以理解： 12scala&gt; def drop1(l: List[_]) = l.taildrop1: (List[_])List[Any] 突然，我们失去了类型信息！让我们细化代码看看发生了什么： 12scala&gt; def drop1(l: List[T forSome &#123; type T &#125;]) = l.taildrop1: (List[T forSome &#123; type T &#125;])List[T forSome &#123; type T &#125;] 我们不能使用 T 因为类型不允许这样做。 你也可以为通配符类型变量应用边界： 1234567891011121314scala&gt; def hashcodes(l: Seq[_ &lt;: AnyRef]) = l map (_.hashCode)hashcodes: (Seq[_ &lt;: AnyRef])Seq[Int]scala&gt; hashcodes(Seq(1,2,3))&lt;console&gt;:7: error: type mismatch; found : Int(1) required: AnyRefNote: primitive types are not implicitly converted to AnyRef.You can safely force boxing by casting x.asInstanceOf[AnyRef]. hashcodes(Seq(1,2,3)) ^scala&gt; hashcodes(Seq(\"one\", \"two\", \"three\"))res1: Seq[Int] = List(110182, 115276, 110339486) 参考 D. R. MacIver 写的 Scala 中的存在类型","categories":[],"tags":[]},{"title":"模式匹配与函数组合","slug":"scala-school-pattern-matching-and-functional-composition","date":"2019-12-01T02:28:36.000Z","updated":"2020-05-04T07:03:11.841Z","comments":true,"path":"2019/12/01/scala-school-pattern-matching-and-functional-composition/","link":"","permalink":"https://razertory.me/2019/12/01/scala-school-pattern-matching-and-functional-composition/","excerpt":"","text":"函数组合让我们创建两个函数： 12scala&gt; def f(s: String) = \"f(\" + s + \")\"f: (String)java.lang.String 12scala&gt; def g(s: String) = \"g(\" + s + \")\"g: (String)java.lang.String composecompose 组合其他函数形成一个新的函数 f(g(x)) 12scala&gt; val fComposeG = f _ compose g _fComposeG: (String) =&gt; java.lang.String = &lt;function&gt; 12scala&gt; fComposeG(\"yay\")res0: java.lang.String = f(g(yay)) andThenandThen 和 compose 很像，但是调用顺序是先调用第一个函数，然后调用第二个，即 g(f(x)) 1234scala&gt; val fAndThenG = f _ andThen g _fAndThenG: (String) =&gt; java.lang.String = &lt;function&gt;scala&gt; fAndThenG(\"yay\")res1: java.lang.String = g(f(yay)) 柯里化(Curried) vs 部分应用(Partial Application)case 语句 那么究竟什么是 case 语句？ 这是一个名为 PartialFunction 的函数的子类。 多个 case 语句的集合是什么？ 他们是共同组合在一起的多个 PartialFunction。 ps: PartialFunction 详见 https://www.scala-lang.org/api/2.12.9/scala/PartialFunction.html 函数 vs 偏函数 (PartialFunction)函数: 对给定的输入参数类型，函数可接受该类型的任何值。换句话说，一个 (Int) =&gt; String 的函数可以接收任意 Int 值，并返回一个字符串。 偏函数: 对给定的输入参数类型，偏函数只能接受该类型的某些特定的值。一个定义为 (Int) =&gt; String 的偏函数可能不能接受所有 Int 值为输入。isDefinedAt 是偏函数的一个方法，用来确定是否能接受一个给定的参数。注意偏函数和我们前面提到的部分应用函数是无关的。 参考 Effective Scala 对 PartialFunction 的意见。 123456789scala&gt; val one: PartialFunction[Int, String] = &#123; case 1 =&gt; \"one\" &#125;one: PartialFunction[Int,String] = &lt;function1&gt;// `isDefinedAt` 是`偏函数`的一个方法，用来确定是否能接受一个给定的参数scala&gt; one.isDefinedAt(1)res0: Boolean = truescala&gt; one.isDefinedAt(2)res1: Boolean = false 您可以调用一个偏函数。 12scala&gt; one(1)res2: String = one 偏函数可以使用 orElse 组成新的函数，得到的偏函数反映了是否对给定参数进行了定义。 1234567891011121314151617181920212223242526scala&gt; val two: PartialFunction[Int, String] = &#123; case 2 =&gt; \"two\" &#125;two: PartialFunction[Int,String] = &lt;function1&gt;scala&gt; val three: PartialFunction[Int, String] = &#123; case 3 =&gt; \"three\" &#125;three: PartialFunction[Int,String] = &lt;function1&gt;scala&gt; val wildcard: PartialFunction[Int, String] = &#123; case _ =&gt; \"something else\" &#125;wildcard: PartialFunction[Int,String] = &lt;function1&gt;scala&gt; val partial = one orElse two orElse three orElse wildcardpartial: PartialFunction[Int,String] = &lt;function1&gt;scala&gt; partial(5)res24: String = something elsescala&gt; partial(3)res25: String = threescala&gt; partial(2)res26: String = twoscala&gt; partial(1)res27: String = onescala&gt; partial(0)res28: String = something else case 之谜上一篇 我们看到一些新奇的东西。我们在通常应该使用函数的地方看到了一个 case 语句。 12345678scala&gt; case class PhoneExt(name: String, ext: Int)defined class PhoneExtscala&gt; val extensions = List(PhoneExt(\"steve\", 100), PhoneExt(\"robey\", 200))extensions: List[PhoneExt] = List(PhoneExt(steve,100), PhoneExt(robey,200))scala&gt; extensions.filter &#123; case PhoneExt(name, extension) =&gt; extension &lt; 200 &#125;res0: List[PhoneExt] = List(PhoneExt(steve,100)) 为什么这段代码可以工作？ filter 使用一个函数。在这个例子中是一个谓词函数(PhoneExt) =&gt; Boolean。(返回一个布尔值的函数通常被称为谓词函数) 偏函数 PartialFunction 是 Function 的子类型，所以 filter 也可以使用 PartialFunction。","categories":[],"tags":[]},{"title":"Scala 课堂 - 集合","slug":"scala-school-collections","date":"2019-12-01T01:10:36.000Z","updated":"2020-05-04T07:02:42.386Z","comments":true,"path":"2019/12/01/scala-school-collections/","link":"","permalink":"https://razertory.me/2019/12/01/scala-school-collections/","excerpt":"","text":"基本数据结构Scala 提供了一些不错的集合。 参考 Effective Scala 对怎样使用集合的观点。 数组 (Array)数组是有序的，可以包含重复项，并且可变。 123456scala&gt; val numbers = Array(1, 2, 3, 4, 5, 1, 2, 3, 4, 5)numbers: Array[Int] = Array(1, 2, 3, 4, 5, 1, 2, 3, 4, 5)scala&gt; numbers(3) = 10scala&gt; numbersnumbers: Array[Int] = Array(1, 2, 3, 10, 5, 1, 2, 3, 4, 5) 列表 (List)列表是有序的，可以包含重复项，不可变。 123456scala&gt; val numbers = List(1, 2, 3, 4, 5, 1, 2, 3, 4, 5)numbers: List[Int] = List(1, 2, 3, 4, 5, 1, 2, 3, 4, 5)scala&gt; numbers(3) = 10&lt;console&gt;:9: error: value update is not a member of List[Int] numbers(3) = 10 集合 (Set)集合无序且不可包含重复项。 12scala&gt; val numbers = Set(1, 2, 3, 4, 5, 1, 2, 3, 4, 5)numbers: scala.collection.immutable.Set[Int] = Set(5, 1, 2, 3, 4) 元组 (Tuple)元组在不使用类的情况下，将元素组合起来形成简单的逻辑集合。 12scala&gt; val hostPort = (\"localhost\", 80)hostPort: (String, Int) = (localhost, 80) 与样本类不同，元组不能通过名称获取字段，而是使用位置下标来读取对象；而且这个下标基于 1，而不是基于 0。 12345scala&gt; hostPort._1res0: String = localhostscala&gt; hostPort._2res1: Int = 80 元组可以很好得与模式匹配相结合。 1234hostPort match &#123; case (\"localhost\", port) =&gt; ... case (host, port) =&gt; ...&#125; 在创建两个元素的元组时，可以使用特殊语法：-&gt; 12scala&gt; 1 -&gt; 2res0: (Int, Int) = (1,2) 参考 Effective Scala 对 解构绑定（“拆解”一个元组）的观点。 映射 (Map)它可以持有基本数据类型。 12Map(1 -&gt; 2)Map(\"foo\" -&gt; \"bar\") 这看起来像是特殊的语法，不过不要忘了上文讨论的 -&gt; 可以用来创建二元组。 Map() 方法也使用了从第一节课学到的变参列表：Map(1 -&gt; “one”, 2 -&gt; “two”) 将变为 Map((1, “one”), (2, “two”))，其中第一个元素是映射的键，第二个元素是映射的值。 映射的值可以是映射甚至是函数。 12Map(1 -&gt; Map(\"foo\" -&gt; \"bar\"))Map(\"timesTwo\" -&gt; &#123; timesTwo(_) &#125;) 选项 (Option)Option 是一个表示有可能包含值的容器。 Option 基本的接口是这样的： 12345trait Option[T] &#123; def isDefined: Boolean def get: T def getOrElse(t: T): T&#125; Option 本身是泛型的，并且有两个子类： Some[T] 或 None 我们看一个使用 Option 的例子： Map.get 使用 Option 作为其返回值，表示这个方法也许不会返回你请求的值。 12345678scala&gt; val numbers = Map(\"one\" -&gt; 1, \"two\" -&gt; 2)numbers: scala.collection.immutable.Map[java.lang.String,Int] = Map(one -&gt; 1, two -&gt; 2)scala&gt; numbers.get(\"two\")res0: Option[Int] = Some(2)scala&gt; numbers.get(\"three\")res1: Option[Int] = None 现在我们的数据似乎陷在 Option 中了，我们怎样获取这个数据呢？ 直觉上想到的可能是基于 isDefined 方法进行条件判断。 123456// We want to multiply the number by two, otherwise return 0.val result = if (res1.isDefined) &#123; res1.get * 2&#125; else &#123; 0&#125; 我们建议使用 getOrElse 或模式匹配处理这个结果。 getOrElse 让你轻松地定义一个默认值。 1234567val result = res1.getOrElse(0) * 2模式匹配能自然地配合 Option 使用。val result = res1 match &#123; case Some(n) =&gt; n * 2 case None =&gt; 0&#125; 参考 Effective Scala 对使用 Options 的意见。 函数组合子（Functional Combinators）List(1, 2, 3) map squared 对列表中的每一个元素都应用了 squared 平方函数，并返回一个新的列表 List(1, 4, 9)。我们把类似于 map 的操作称作组合子。 （如果想要更好的定义，你可以看看 Stackoverflow 上对 组合子 的说明。）他们常被用在标准的数据结构上。 mapmap 对列表中的每个元素应用一个函数，返回应用后的元素所组成的列表。 12345scala&gt; val numbers = List(1, 2, 3, 4)numbers: List[Int] = List(1, 2, 3, 4)scala&gt; numbers.map((i: Int) =&gt; i * 2)res0: List[Int] = List(2, 4, 6, 8) 或传入一个函数 (Scala 编译器自动把我们的方法转换为函数） 12345scala&gt; def timesTwo(i: Int): Int = i * 2timesTwo: (i: Int)Intscala&gt; numbers.map(timesTwo)res0: List[Int] = List(2, 4, 6, 8) foreachforeach 很像 map，但没有返回值。foreach 仅用于有副作用 side-effects 的函数。 12scala&gt; numbers.foreach((i: Int) =&gt; i * 2)// 什么也没有返回。 你可以尝试存储返回值，但它会是 Unit 类型（即 void） 12scala&gt; val doubled = numbers.foreach((i: Int) =&gt; i * 2)doubled: Unit = () filterfilter 移除任何对传入函数计算结果为 false 的元素。返回一个布尔值的函数通常被称为谓词函数 （或判定函数）。 1234567scala&gt; numbers.filter((i: Int) =&gt; i % 2 == 0)res0: List[Int] = List(2, 4)scala&gt; def isEven(i: Int): Boolean = i % 2 == 0isEven: (i: Int)Booleanscala&gt; numbers.filter(isEven)res2: List[Int] = List(2, 4) zipzip 将两个列表的内容聚合到一个对偶列表中。 12scala&gt; List(1, 2, 3).zip(List(\"a\", \"b\", \"c\"))res0: List[(Int, String)] = List((1,a), (2,b), (3,c)) partitionpartition 将使用给定的谓词函数分割列表。 123scala&gt; val numbers = List(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)scala&gt; numbers.partition(_ % 2 == 0)res0: (List[Int], List[Int]) = (List(2, 4, 6, 8, 10),List(1, 3, 5, 7, 9)) findfind 返回集合中第一个匹配谓词函数的元素。 12scala&gt; numbers.find((i: Int) =&gt; i &gt; 5)res0: Option[Int] = Some(6) drop &amp; dropWhiledrop 将删除前 i 个元素 12scala&gt; numbers.drop(5)res0: List[Int] = List(6, 7, 8, 9, 10) dropWhile 将删除匹配谓词函数的第一个元素。例如，如果我们在 numbers 列表上使用 dropWhile 函数来去除奇数，1 将被丢弃（但 3 不会被丢弃，因为他被 2 保护了）。 12scala&gt; numbers.dropWhile(_ % 2 != 0)res0: List[Int] = List(2, 3, 4, 5, 6, 7, 8, 9, 10) foldLeft12scala&gt; numbers.foldLeft(0)((m: Int, n: Int) =&gt; m + n)res0: Int = 55 0 为初始值（记住 numbers 是 List[Int] 类型），m 作为一个累加器。在标准库的实现为： 12345def foldLeft[B](z: B)(op: (B, A) =&gt; B): B = &#123; var result = z this.seq foreach (x =&gt; result = op(result, x)) result&#125; 可视化观察运行过程： 123456789101112scala&gt; numbers.foldLeft(0) &#123; (m: Int, n: Int) =&gt; println(\"m: \" + m + \" n: \" + n); m + n &#125;m: 0 n: 1m: 1 n: 2m: 3 n: 3m: 6 n: 4m: 10 n: 5m: 15 n: 6m: 21 n: 7m: 28 n: 8m: 36 n: 9m: 45 n: 10res0: Int = 55 foldRight和 foldLeft 一样，只是运行过程相反。在标准库的实现为： 12def foldRight[B](z: B)(op: (A, B) =&gt; B): B = reversed.foldLeft(z)((x, y) =&gt; op(y, x)) 123456789101112scala&gt; numbers.foldRight(0) &#123; (m: Int, n: Int) =&gt; println(\"m: \" + m + \" n: \" + n); m + n &#125;m: 10 n: 0m: 9 n: 10m: 8 n: 19m: 7 n: 27m: 6 n: 34m: 5 n: 40m: 4 n: 45m: 3 n: 49m: 2 n: 52m: 1 n: 54res0: Int = 55 flattenflatten 将嵌套结构扁平化一个层级。 12scala&gt; List(List(1, 2), List(3, 4)).flattenres0: List[Int] = List(1, 2, 3, 4) flatMapflatMap 是一种常用的组合子，结合映射 mapping 和扁平化 flattening。 flatMap 需要一个处理嵌套列表的函数，然后将结果串连起来。 12345scala&gt; val nestedNumbers = List(List(1, 2), List(3, 4))nestedNumbers: List[List[Int]] = List(List(1, 2), List(3, 4))scala&gt; nestedNumbers.flatMap(x =&gt; x.map(_ * 2))res0: List[Int] = List(2, 4, 6, 8) 可以把它看做是“先映射后扁平化”的快捷操作： 12scala&gt; nestedNumbers.map((x: List[Int]) =&gt; x.map(_ * 2)).flattenres1: List[Int] = List(2, 4, 6, 8) 这个例子先调用 map，然后调用 flatten，这就是 组合子 的特征，也是这些函数的本质。 参考 Effective Scala 对 flatMap 的意见。 扩展函数组合子现在我们已经学过集合上的一些函数。 我们将尝试写自己的函数组合子。 有趣的是，上面所展示的每一个函数组合子都可以用 fold 方法实现。让我们看一些例子。 12345678def ourMap(numbers: List[Int], fn: Int =&gt; Int): List[Int] = &#123; numbers.foldRight(List[Int]()) &#123; (x: Int, xs: List[Int]) =&gt; fn(x) :: xs // 两个冒号表示普通元素与 List 的连接操作 &#125;&#125;scala&gt; ourMap(numbers, timesTwo(_))res0: List[Int] = List(2, 4, 6, 8, 10, 12, 14, 16, 18, 20) 为什么是 List[Int]()？Scala 没有聪明到理解你的目的是将结果积聚在一个空的 Int 类型的列表中。 Map?所有展示的函数组合子都可以在 Map 上使用。Map 可以被看作是一个二元组的列表，所以你写的函数要处理一个键和值的二元组。 12scala&gt; val extensions = Map(\"steve\" -&gt; 100, \"bob\" -&gt; 101, \"joe\" -&gt; 201)extensions: scala.collection.immutable.Map[String,Int] = Map((steve,100), (bob,101), (joe,201)) 现在筛选出电话分机号码低于 200 的条目。 12scala&gt; extensions.filter((namePhone: (String, Int)) =&gt; namePhone._2 &lt; 200)res0: scala.collection.immutable.Map[String,Int] = Map((steve,100), (bob,101)) 因为参数是元组，所以你必须使用位置获取器来读取它们的键和值。呃！ 幸运的是，我们其实可以使用模式匹配更优雅地提取键和值。 12scala&gt; extensions.filter(&#123;case (name, extension) =&gt; extension &lt; 200&#125;)res0: scala.collection.immutable.Map[String,Int] = Map((steve,100), (bob,101)) 为什么这个代码可以工作？为什么你可以传递一个部分模式匹配？下一章将揭晓。","categories":[],"tags":[]},{"title":"Scala 课堂 - 基础（续）","slug":"scala-school-bascis2","date":"2019-11-30T14:20:10.000Z","updated":"2020-05-04T07:02:32.099Z","comments":true,"path":"2019/11/30/scala-school-bascis2/","link":"","permalink":"https://razertory.me/2019/11/30/scala-school-bascis2/","excerpt":"","text":"apply 方法当类或对象有一个主要用途的时候，apply 方法为你提供了一个很好的语法糖。 12345678910scala&gt; class Foo &#123;&#125;defined class Fooscala&gt; object FooMaker &#123; | def apply() = new Foo | &#125;defined module FooMakerscala&gt; val newFoo = FooMaker()newFoo: Foo = Foo@5b83f762 或 12345678910scala&gt; class Bar &#123; | def apply() = 0 | &#125;defined class Barscala&gt; val bar = new Barbar: Bar = Bar@47711479scala&gt; bar()res8: Int = 0 在这里，我们实例化对象看起来像是在调用一个方法。以后会有更多介绍！ 单例对象 (object)单例对象用于持有一个类的唯一实例。通常用于 工厂模式。 12345678object Timer &#123; var count = 0 def currentCount(): Long = &#123; count += 1 count &#125;&#125; 可以这样使用： 12scala&gt; Timer.currentCount()res0: Long = 1 单例对象可以和类具有相同的名称，此时该对象也被称为伴生对象。我们通常将伴生对象作为工厂使用。 下面是一个简单的例子，可以不需要使用 new 来创建一个实例了。 12345class Bar(foo: String)object Bar &#123; def apply(foo: String) = new Bar(foo)&#125; 函数即对象 Scala 中，我们经常谈论对象的函数式编程。这是什么意思？到底什么是函数呢？ 函数是一些特质的集合。具体来说，具有一个参数的函数是 Function1 trait 的一个实例。这个 trait 定义了 apply 语法糖，让你调用一个对象时就像你在调用一个函数。（关于 Function1 trait 在 Scala API doc 中有详细介绍 https://www.scala-lang.org/api/2.7.5/scala/Function1.html ) 1234567scala&gt; object addOne extends Function1[Int, Int] &#123; | def apply(m: Int): Int = m + 1 | &#125;defined module addOnescala&gt; addOne(1)res2: Int = 2 这个 Function trait 集合下标从 0 开始一直到 22。为什么是 22？这是一个主观的魔幻数字 (magic number)。我从来没有使用过多于 22 个参数的函数，所以这个数字似乎是合理的。（在 Scala ) apply 语法糖有助于统一对象和函数式编程的二重性。你可以传递类，并把它们当做函数使用，而函数本质上是类的实例。 这是否意味着，当你在类中定义一个方法时，得到的实际上是一个 Function *的实例？不是的，在类中定义的方法是方法而不是函数。在 repl 中独立定义的方法是 Function *的实例。 类也可以扩展 Function，这些类的实例可以使用 () 调用。 12345678910scala&gt; class AddOne extends Function1[Int, Int] &#123; | def apply(m: Int): Int = m + 1 | &#125;defined class AddOnescala&gt; val plusOne = new AddOne()plusOne: AddOne = &lt;function1&gt;scala&gt; plusOne(1)res0: Int = 2 可以使用更直观快捷的 extends (Int =&gt; Int) 代替 extends Function1[Int, Int] 123class AddOne extends (Int =&gt; Int) &#123; def apply(m: Int): Int = m + 1&#125; 包 (package)你可以将代码组织在包里。 1package com.twitter.example 在文件头部定义包，会将文件中所有的代码声明在那个包中。 值和函数不能在类或单例对象之外定义。单例对象是组织静态函数 static function 的有效工具。 123456package com.twitter.exampleobject colorHolder &#123; val BLUE = \"Blue\" val RED = \"Red\"&#125; 现在你可以直接访问这些成员 1println(\"the color is: \" + com.twitter.example.colorHolder.BLUE) 注意在你定义这个对象时 Scala 解释器的返回： 12345scala&gt; object colorHolder &#123; | val Blue = \"Blue\" | val Red = \"Red\" | &#125;defined module colorHolder 这暗示了 Scala 的设计者是把对象作为 Scala 的模块系统的一部分进行设计的。 模式匹配这是 Scala 中最有用的部分之一。 匹配值 1234567val times = 1times match &#123; case 1 =&gt; \"one\" case 2 =&gt; \"two\" case _ =&gt; \"some other number\"&#125; 使用守卫进行匹配 12345times match &#123; case i if i == 1 =&gt; \"one\" case i if i == 2 =&gt; \"two\" case _ =&gt; \"some other number\"&#125; 注意我们是怎样获取变量 i 的值的。 在最后一行指令中的_是一个通配符；它保证了我们可以处理所有的情况。 (注意第一章提到的 _ 在不同上下文中有不同含义)否则当传进一个不能被匹配的数字的时候，你将获得一个运行时错误。我们以后会继续讨论这个话题的。 参考 Effective Scala 对什么时候使用模式匹配和模式匹配格式化的建议。A Tour of Scala 也描述了模式匹配 匹配类型你可以使用 match 来分别处理不同类型的值。 123456789def bigger(o: Any): Any = &#123; o match &#123; case i: Int if i &lt; 0 =&gt; i - 1 case i: Int =&gt; i + 1 case d: Double if d &lt; 0.0 =&gt; d - 0.1 case d: Double =&gt; d + 0.1 case text: String =&gt; text + \"s\" &#125;&#125; 匹配类成员还记得我们之前的计算器吗。 让我们通过类型对它们进行分类。 一开始会很痛苦。 123456def calcType(calc: Calculator) = calc match &#123; case _ if calc.brand == \"HP\" &amp;&amp; calc.model == \"20B\" =&gt; \"financial\" case _ if calc.brand == \"HP\" &amp;&amp; calc.model == \"48G\" =&gt; \"scientific\" case _ if calc.brand == \"HP\" &amp;&amp; calc.model == \"30B\" =&gt; \"business\" case _ =&gt; \"unknown\"&#125; (⊙o⊙) 哦，太痛苦了。幸好 Scala 提供了一些应对这种情况的有效工具。 样本类 (Case Classes)使用样本类可以方便提前准备好需要进行匹配类内容，并且不用 new 关键字就可以创建它们。 12345scala&gt; case class Calculator(brand: String, model: String)defined class Calculatorscala&gt; val hp20b = Calculator(\"HP\", \"20b\")hp20b: Calculator = Calculator(hp,20b) 样本类基于构造函数的参数，自动地实现了相等性和易读的 toString 方法。 12345678scala&gt; val hp20b = Calculator(\"HP\", \"20b\")hp20b: Calculator = Calculator(hp,20b)scala&gt; val hp20B = Calculator(\"HP\", \"20b\")hp20B: Calculator = Calculator(hp,20b)scala&gt; hp20b == hp20Bres6: Boolean = true 样本类也可以像普通类那样拥有方法。 使用样本类进行模式匹配样本类就是被设计用在模式匹配中的。让我们简化之前的计算器分类器的例子。 123456789val hp20b = Calculator(\"HP\", \"20B\")val hp30b = Calculator(\"HP\", \"30B\")def calcType(calc: Calculator) = calc match &#123; case Calculator(\"HP\", \"20B\") =&gt; \"financial\" case Calculator(\"HP\", \"48G\") =&gt; \"scientific\" case Calculator(\"HP\", \"30B\") =&gt; \"business\" case Calculator(ourBrand, ourModel) =&gt; \"Calculator: %s %s is of unknown type\".format(ourBrand, ourModel)&#125; 最后一句也可以这样写 1case Calculator(_, _) =&gt; \"Calculator of unknown type\" 或者我们完全可以不将匹配对象指定为 Calculator 类型 1case _ =&gt; \"Calculator of unknown type\" 或者我们也可以将匹配的值重新命名。 1case c@Calculator(_, _) =&gt; \"Calculator: %s of unknown type\".format(c) 异常Scala 中的异常可以在 try-catch-finally 语法中通过模式匹配使用。 12345678910111213141516171819try &#123; remoteCalculatorService.add(1, 2)&#125; catch &#123; case e: ServerIsDownException =&gt; log.error(e, \"the remote calculator service is unavailable. should have kept your trusty HP.\")&#125; finally &#123; remoteCalculatorService.close()&#125;// try 也是面向表达式的val result: Int = try &#123; remoteCalculatorService.add(1, 2)&#125; catch &#123; case e: ServerIsDownException =&gt; &#123; log.error(e, \"the remote calculator service is unavailable. should have kept your trusty HP.\") 0 &#125;&#125; finally &#123; remoteCalculatorService.close()&#125; 这并不是一个完美编程风格的展示，而只是一个例子，用来说明 try-catch-finally 和 Scala 中其他大部分事物一样是表达式。 当一个异常被捕获处理了，finally 块将被调用；它不是表达式的一部分。","categories":[],"tags":[]},{"title":"Scala 总览","slug":"scala-overview","date":"2019-11-30T13:26:36.000Z","updated":"2020-05-04T07:02:57.428Z","comments":true,"path":"2019/11/30/scala-overview/","link":"","permalink":"https://razertory.me/2019/11/30/scala-overview/","excerpt":"","text":"Why Scala JVM 生态 简洁 类型推断 函数式 多范式 Actor 下面的所有操作都在 Scala REPL 中进行 表达式12scala&gt; 1 + 1res0: Int = 2 res0 是解释器自动创建的变量名称，用来指代表达式的计算结果。它是 Int 类型，值为 2。意味着你可以继续使用这个变量。 12scala&gt; res0 + 1res1: Int = 3 Scala 中（几乎）一切都是表达式。 值 (val) 和变量 (var)你可以给一个表达式的结果起个名字赋成一个不变量（val）。 12345scala&gt; val two = 1 + 1two: Int = 2scala&gt; two = 3 ^ error: reassignment to val 你不能改变这个不变量的值，但如果是 var 声明的就可以改变。 函数你可以使用 def 创建函数。 12scala&gt; def addOne(m: Int): Int = m + 1addOne: (m: Int)Int 你可以创建匿名函数。 12scala&gt; (x: Int) =&gt; x + 1res2: (Int) =&gt; Int = &lt;function1&gt; 你可以把匿名函数赋值给 val。 12345scala&gt; val addOne = (x: Int) =&gt; x + 1addOne: (Int) =&gt; Int = &lt;function1&gt;scala&gt; addOne(1)res4: Int = 2 你可以部分应用一个函数 123456789scala&gt; def adder(m: Int, n: Int) = m + nadder: (m: Int,n: Int)Int// 你可以「部分应用」参数列表中的任意参数，而不仅仅是最后一个。scala&gt; val add2 = adder(2, _:Int) add2: (Int) =&gt; Int = &lt;function1&gt;scala&gt; add2(3)res50: Int = 5 你可以柯里化函数 (Currying Function) 1234567891011121314151617scala&gt; def multiply(m: Int)(n: Int): Int = m * nmultiply: (m: Int)(n: Int)Int//你可以直接传入两个参数。scala&gt; multiply(2)(3)res0: Int = 6//你可以填上第一个参数并且部分应用第二个参数。scala&gt; val timesTwo = multiply(2) _timesTwo: (Int) =&gt; Int = &lt;function1&gt;scala&gt; timesTwo(3)res1: Int = 6// 你可以对任何多参数函数执行柯里化 (`.curried`)scala&gt; (adder _).curriedres1: (Int) =&gt; (Int) =&gt; Int = &lt;function1&gt; 你可以参数可变参数函数 12345678def capitalizeAll(args: String*) = &#123; args.map &#123; arg =&gt; arg.capitalize &#125;&#125;scala&gt; capitalizeAll(\"rarity\", \"applejack\")res2: Seq[String] = ArrayBuffer(Rarity, Applejack) OOP类、继承和 Java 比较类似。额外的 Scala 还引入了 trait。实现类似于 Ruby、Python 这样的动态语言的 mix-in object 关键字 和 apply() 方法。方便实现 single instance case class。为方便存放值和模式匹配引入了 Trait 1234567891011121314151617// trait 是一些字段和行为的集合，可以扩展或混入（`mixin`）你的类中。trait Car &#123; val brand: String&#125;trait Shiny &#123; val shineRefraction: Int&#125;class BMW extends Car &#123; val brand = \"BMW\"&#125;// 通过 with 关键字，一个类可以扩展多个 trait：class BMW extends Car with Shiny &#123; val brand = \"BMW\" val shineRefraction = 12&#125; object 123456789101112131415161718// object 实现 single instanceobject Timer &#123; var count = 0 def currentCount(): Long = &#123; count += 1 count &#125;&#125;// 不必再用 new 关键字scala&gt; Timer.currentCount()res0: Long = 1// Companion Objec 伴生对象，用来给 object 初始化值class Bar(foo: String)object Bar &#123; def apply(foo: String) = new Bar(foo)&#125; case class 12345678case class Calculator(brand: String, model: String)def calcType(calc: Calculator) = calc match &#123; case Calculator(\"HP\", \"20B\") =&gt; \"financial\" case Calculator(\"HP\", \"48G\") =&gt; \"scientific\" case Calculator(\"HP\", \"30B\") =&gt; \"business\" case Calculator(_, _) =&gt; \"Calculator: %s %s is of unknown type\".format(ourBrand, ourModel) case _ =&gt; \"Not a calculator\"&#125;","categories":[],"tags":[]},{"title":"完美的二分搜索","slug":"binary-search","date":"2019-11-28T14:45:06.000Z","updated":"2020-03-28T13:02:10.014Z","comments":true,"path":"2019/11/28/binary-search/","link":"","permalink":"https://razertory.me/2019/11/28/binary-search/","excerpt":"","text":"假设给一个有序的 int 类型的数组 arr，和一个目标值 target，找到这个目标值在数组中的下标。如果数组中没有target 返回 -1。 12345678910111213int search(int[] arr, int target) &#123; int start = 0, end = arr.length - 1; while (start &lt;= end) &#123; int mid = start + (end - start) / 2; // 需要注意用 (start + end) / 2 在数学上没有问题。但是考虑到 start + end 可能会溢出。 if (target == arr[mid]) return mid; else if (target &lt; arr[mid]) end = mid - 1; else start + mid + 1; &#125; return -1;&#125;","categories":[],"tags":[]},{"title":"汉明距离","slug":"hamming-distance","date":"2019-11-17T02:46:03.000Z","updated":"2020-03-28T13:02:10.013Z","comments":true,"path":"2019/11/17/hamming-distance/","link":"","permalink":"https://razertory.me/2019/11/17/hamming-distance/","excerpt":"","text":"两个整数之间的汉明距离指的是这两个数字对应二进制位不同的位置的数目。给出两个整数 x 和 y，计算它们之间的汉明距离。 1int hammingDistance(int x, int y) 对于 x, 和 y 先进行异或，然后再计算结果中的 1 的个数即可。（其中 count 算法来自 二进制中 1 的个数） 1234int hammingDistance(int x, int y) &#123; int n = x ^ y; return count(n); &#125;","categories":[],"tags":[]},{"title":"只出现一次的数","slug":"single-number","date":"2019-11-17T02:31:09.000Z","updated":"2020-03-28T13:02:10.013Z","comments":true,"path":"2019/11/17/single-number/","link":"","permalink":"https://razertory.me/2019/11/17/single-number/","excerpt":"","text":"给定一个非空整数数组，除了某个元素只出现一次以外，其余每个元素均出现两次。找出那个只出现了一次的元素。 1int singleNumber(int[] nums) 计算机科学中，异或满足四大定律 12341.恒定律：A ^ 0 = A2.归零率：A ^ A = 03.交换律：A ^ B = B ^ A4.结合律：(A ^ B) ^ C = A ^ (B ^ C) 假如给定的数组是 [1, 3, 1, 3, 2, 4, 4]。根据异或的定律，对数组从头到尾进行异或可以等价成分组之后的异或，这里可以理解成 [1, 1, 2, 3, 3, 4, 4]。相同的数都会变为 0 ，最后剩下的那个数也就和 0 进行异或等到自己本身。 1234567int singleNumber(int[] nums) &#123; int result = 0; for (int num : nums) &#123; result ^= num; &#125; return result;&#125;","categories":[],"tags":[]},{"title":"个数多于一半的数","slug":"major-element","date":"2019-11-16T12:08:11.000Z","updated":"2020-03-28T13:02:10.013Z","comments":true,"path":"2019/11/16/major-element/","link":"","permalink":"https://razertory.me/2019/11/16/major-element/","excerpt":"","text":"给定一个大小为 n 的数组，找出其中在数组中出现次数大于 ⌊ n/2 ⌋ 的元素（众数）。 1int majorityElement(int[] nums) 理解这个问题，首先可以把数组分为两个部分：所有的众数和非众数。把众数和非众数进行两两抵消，那么最后一定还剩下众数。实际上这个也就是摩尔投票法。在遍历数组的时候不断确认当前的众数，如果没有，则认为下一个就是。维护一个计数器，在遇到相同的时候给众数 +1，不同的时候 -1。当计数器为 0 的时候认为没有众数。 12345678910111213int majorityElement(int[] nums) &#123; int count = 0; Integer candidate = null; for (int num : nums) &#123; if (count == 0) &#123; candidate = num; &#125; count += (num == candidate) ? 1 : -1; &#125; return candidate;&#125;","categories":[],"tags":[]},{"title":"二进制中 1 的个数","slug":"count-of-one","date":"2019-11-14T01:42:44.000Z","updated":"2020-03-28T13:02:10.013Z","comments":true,"path":"2019/11/14/count-of-one/","link":"","permalink":"https://razertory.me/2019/11/14/count-of-one/","excerpt":"","text":"对于任意整数，求转换成二进制数之后，1 的个数。比如 5 转换成二进制是 101，其中 1 的个数是 2。 1int count (int num) 要判断一个二进制数的最低位是否是 1，只需要和 1 进行 &amp; 运算即可。那么具体的做法就一边移位一边统计。 12345678int count (int num) &#123; int count = 0; while(num != 0) &#123; if (num &amp; 1 == 1) count++; num &gt;&gt; 1; &#125; return count;&#125;","categories":[],"tags":[]},{"title":"位运算实现加法","slug":"add-by-bit","date":"2019-11-11T14:56:31.000Z","updated":"2020-03-28T13:02:10.013Z","comments":true,"path":"2019/11/11/add-by-bit/","link":"","permalink":"https://razertory.me/2019/11/11/add-by-bit/","excerpt":"","text":"计算机科学中，基础运算运算都来自于二进制中的位运算。其中不免提到加法器，维基百科是这么定义的 在电子学中，加法器（英语：adder）是一种用于执行加法运算的数字电路部件，是构成电子计算机核心微处理器中算术逻辑单元的基础。在这些电子系统中，加法器主要负责计算地址、索引等数据。除此之外，加法器也是其他一些硬件，例如二进制数的乘法器的重要组成部分。尽管可以为不同计数系统设计专门的加法器，但是由于数字电路通常以二进制为基础，因此二进制加法器在实际应用中最为普遍。在数字电路中，二进制数的减法可以通过加一个负数来间接完成。为了使负数的计算能够直接用加法器来完成，计算中的负数可以使用二补数（补码）来表示，具体的细节可以参考数字电路相关的书籍 对两个二进制数做相加，有以下规律 二进制加法 x y 和 进位 0 0 0 0 0 1 1 0 1 0 1 0 1 1 0 1 假设 x + y 相加用代码表示，那么就需要一个 sum 和 carry 分别表示和与进位。不过这个还不足够表示两个数相加。 仔细观察可以发现 x + y 的 sum 和 carry 其实分别是异或^和与&amp;运算。如下： 异或 x y ^ 0 0 0 0 1 1 1 0 1 1 1 0 与 x y &amp; 0 0 0 0 1 0 1 0 0 1 1 1 123456789101112// 二进制实现加法int add(int a, int b) &#123; int sum = a; int carry = b; while(carry != 0 ) &#123; int tmps = sum; sum = tmps ^ carry; carry = (tmps &amp; carry) &lt;&lt; 1; &#125; return sum;&#125;","categories":[],"tags":[]},{"title":"括号的合法序列","slug":"valid-brackets","date":"2019-11-10T01:55:10.000Z","updated":"2020-03-28T13:02:10.012Z","comments":true,"path":"2019/11/10/valid-brackets/","link":"","permalink":"https://razertory.me/2019/11/10/valid-brackets/","excerpt":"","text":"给你一个括号序列，里面包含小括号，中括号和大括号。你要判断这个括号序列是否有效。有效的括号序列要求，每个左括号都必须有一个同类的右括号与它正确配对。另外，空字符串认为是有效的括号序列。 1234567891011比如说，给你的序列是：()[]&#123;&#125;小括号/中括号/大括号的左右括号都能正确配对，因此这是一个有效的括号序列。再比如说给你的序列是：([)]这里面虽然正好有一对小括号和一对中括号，但它们的顺序不对，括号间无法正确配对，因此这不是一个有效的括号序列 1public boolean isValidBrackets(String s) &#123; 维护一个 stack，存放 char。在遍历 s 的过程中，不断对遇到左括号进行 push，右括号进行 pop。当最后 stack 为空的时候说明是合法的括号序列. 1234567891011121314151617public boolean isValidBrackets(String s) &#123; Stack&lt;Character&gt; stack = new Stack&lt;&gt;(); for (int i = 0; i &lt; s.length(); ++i) &#123; if (s.charAt(i) == '(' || s.charAt(i) == '[' || s.charAt(i) == '&#123;') &#123; stack.push(s.charAt(i)); &#125; else if (stack.isEmpty()) &#123; return false; &#125; else &#123; if (s.charAt(i) == ')' &amp;&amp; stack.peek() != '(') return false; if (s.charAt(i) == ']' &amp;&amp; stack.peek() != '[') return false; if (s.charAt(i) == '&#125;' &amp;&amp; stack.peek() != '&#123;') return false; stack.pop(); &#125; &#125; return stack.isEmpty(); &#125;","categories":[],"tags":[]},{"title":"栈实现队列","slug":"queue-by-stack","date":"2019-11-10T01:45:31.000Z","updated":"2020-03-28T13:02:10.012Z","comments":true,"path":"2019/11/10/queue-by-stack/","link":"","permalink":"https://razertory.me/2019/11/10/queue-by-stack/","excerpt":"","text":"维护一个 in，out 栈表示进出顺序，实现一个从 in 到 out 的转移方法。 1234567891011121314151617181920212223242526272829303132333435class MyQueue &#123; private Stack&lt;Integer&gt; in, out; MyQueue() &#123; in = new Stack&lt;&gt;(); out = new Stack&lt;&gt;(); &#125; void push(int val) &#123; in.push(val); &#125; int pop() &#123; transferIfEmpty(); return out.pop(); &#125; int peek() &#123; transferIfEmpty(); return out.peek(); &#125; boolean isEmpty() &#123; return in.isEmpty() &amp;&amp; out.empty(); &#125; private void transferIfEmpty() &#123; if(out.empty()) &#123; while(!in.empty()) &#123; out.push(in.pop()); &#125; &#125; &#125;&#125;","categories":[],"tags":[]},{"title":"uber-go-guide","slug":"uber-go-guide-cn","date":"2019-11-08T00:04:22.000Z","updated":"2020-03-28T13:02:10.012Z","comments":true,"path":"2019/11/08/uber-go-guide-cn/","link":"","permalink":"https://razertory.me/2019/11/08/uber-go-guide-cn/","excerpt":"","text":"本文长期维护地址: https://github.com/xxjwxc/uber_go_guide_cn Uber Go 语言编码规范 Uber 是一家美国硅谷的科技公司，也是 Go 语言的早期 adopter。其开源了很多 golang 项目，诸如被 Gopher 圈熟知的 zap、jaeger 等。2018 年年末 Uber 将内部的 Go 风格规范 开源到 GitHub，经过一年的积累和更新，该规范已经初具规模，并受到广大 Gopher 的关注。本文是该规范的中文版本。本版本会根据原版实时更新。 版本 当前更新版本：2019-11-01 版本地址：commit:#68 如果您发现任何更新、问题或改进，请随时 fork 和 PR Please feel free to fork and PR if you find any updates, issues or improvement. 目录 介绍 指导原则 指向 interface 的指针 接收器 (receiver) 与接口 零值 Mutex 是有效的 在边界处拷贝 Slices 和 Maps 使用 defer 释放资源 Channel 的 size 要么是 1，要么是无缓冲的 枚举从 1 开始 错误类型 错误包装 (Error Wrapping) 处理类型断言失败 不要 panic 使用 go.uber.org/atomic 性能 优先使用 strconv 而不是 fmt 避免字符串到字节的转换 尽量初始化时指定 Map 容量 规范 一致性 相似的声明放在一组 import 分组 包名 函数名 导入别名 函数分组与顺序 减少嵌套 不必要的 else 顶层变量声明 对于未导出的顶层常量和变量，使用_作为前缀 结构体中的嵌入 使用字段名初始化结构体 本地变量声明 nil 是一个有效的 slice 小变量作用域 避免参数语义不明确（Avoid Naked Parameters） 使用原始字符串字面值，避免转义 初始化 Struct 引用 初始化 Maps 字符串 string format 命名 Printf 样式的函数 编程模式 表驱动测试 功能选项 介绍样式 (style) 是支配我们代码的惯例。术语样式有点用词不当，因为这些约定涵盖的范围不限于由 gofmt 替我们处理的源文件格式。 本指南的目的是通过详细描述在 Uber 编写 Go 代码的注意事项来管理这种复杂性。这些规则的存在是为了使代码库易于管理，同时仍然允许工程师更有效地使用 Go 语言功能。 该指南最初由 Prashant Varanasi 和 Simon Newton 编写，目的是使一些同事能快速使用 Go。多年来，该指南已根据其他人的反馈进行了修改。 本文档记录了我们在 Uber 遵循的 Go 代码中的惯用约定。其中许多是 Go 的通用准则，而其他扩展准则依赖于下面外部的指南： Effective Go The Go common mistakes guide 所有代码都应该通过golint和go vet的检查并无错误。我们建议您将编辑器设置为： 保存时运行 goimports 运行 golint 和 go vet 检查错误 您可以在以下 Go 编辑器工具支持页面中找到更为详细的信息：https://github.com/golang/go/wiki/IDEsAndTextEditorPlugins 指导原则指向 interface 的指针您几乎不需要指向接口类型的指针。您应该将接口作为值进行传递，在这样的传递过程中，实质上传递的底层数据仍然可以是指针。 接口实质上在底层用两个字段表示： 一个指向某些特定类型信息的指针。您可以将其视为”type”。 数据指针。如果存储的数据是指针，则直接存储。如果存储的数据是一个值，则存储指向该值的指针。 如果希望接口方法修改基础数据，则必须使用指针传递。 接收器 (receiver) 与接口使用值接收器的方法既可以通过值调用，也可以通过指针调用。 例如， 12345678910111213141516171819202122232425type S struct &#123; data string&#125;func (s S) Read() string &#123; return s.data&#125;func (s *S) Write(str string) &#123; s.data = str&#125;sVals := map[int]S&#123;1: &#123;\"A\"&#125;&#125;// 你只能通过值调用 ReadsVals[1].Read()// 这不能编译通过：// sVals[1].Write(\"test\")sPtrs := map[int]*S&#123;1: &#123;\"A\"&#125;&#125;// 通过指针既可以调用 Read，也可以调用 Write 方法sPtrs[1].Read()sPtrs[1].Write(\"test\") 同样，即使该方法具有值接收器，也可以通过指针来满足接口。 123456789101112131415161718192021222324type F interface &#123; f()&#125;type S1 struct&#123;&#125;func (s S1) f() &#123;&#125;type S2 struct&#123;&#125;func (s *S2) f() &#123;&#125;s1Val := S1&#123;&#125;s1Ptr := &amp;S1&#123;&#125;s2Val := S2&#123;&#125;s2Ptr := &amp;S2&#123;&#125;var i Fi = s1Vali = s1Ptri = s2Ptr// 下面代码无法通过编译。因为 s2Val 是一个值，而 S2 的 f 方法中没有使用值接收器// i = s2Val Effective Go 中有一段关于 pointers vs. values 的精彩讲解。 零值 Mutex 是有效的零值 sync.Mutex 和 sync.RWMutex 是有效的。所以指向 mutex 的指针基本是不必要的。 BadGood 12mu := new(sync.Mutex)mu.Lock() 12var mu sync.Mutexmu.Lock() 如果你使用结构体指针，mutex 可以非指针形式作为结构体的组成字段，或者更好的方式是直接嵌入到结构体中。如果是私有结构体类型或是要实现 Mutex 接口的类型，我们可以使用嵌入 mutex 的方法： 123456789101112131415161718type smap struct &#123; sync.Mutex // only for unexported types（仅适用于非导出类型） data map[string]string&#125;func newSMap() *smap &#123; return &amp;smap&#123; data: make(map[string]string), &#125;&#125;func (m *smap) Get(k string) string &#123; m.Lock() defer m.Unlock() return m.data[k]&#125; 123456789101112131415161718type SMap struct &#123; mu sync.Mutex // 对于导出类型，请使用私有锁 data map[string]string&#125;func NewSMap() *SMap &#123; return &amp;SMap&#123; data: make(map[string]string), &#125;&#125;func (m *SMap) Get(k string) string &#123; m.mu.Lock() defer m.mu.Unlock() return m.data[k]&#125; 为私有类型或需要实现互斥接口的类型嵌入。 对于导出的类型，请使用专用字段。 在边界处拷贝 Slices 和 Mapsslices 和 maps 包含了指向底层数据的指针，因此在需要复制它们时要特别注意。 接收 Slices 和 Maps请记住，当 map 或 slice 作为函数参数传入时，如果您存储了对它们的引用，则用户可以对其进行修改。 Bad Good 123456789func (d *Driver) SetTrips(trips []Trip) &#123; d.trips = trips&#125;trips := ...d1.SetTrips(trips)// 你是要修改 d1.trips 吗？trips[0] = ... 12345678910func (d *Driver) SetTrips(trips []Trip) &#123; d.trips = make([]Trip, len(trips)) copy(d.trips, trips)&#125;trips := ...d1.SetTrips(trips)// 这里我们修改 trips[0]，但不会影响到 d1.tripstrips[0] = ... 返回 slices 或 maps同样，请注意用户对暴露内部状态的 map 或 slice 的修改。 BadGood 123456789101112131415161718type Stats struct &#123; mu sync.Mutex counters map[string]int&#125;// Snapshot 返回当前状态。func (s *Stats) Snapshot() map[string]int &#123; s.mu.Lock() defer s.mu.Unlock() return s.counters&#125;// snapshot 不再受互斥锁保护// 因此对 snapshot 的任何访问都将受到数据竞争的影响// 影响 stats.counterssnapshot := stats.Snapshot() 12345678910111213141516171819type Stats struct &#123; mu sync.Mutex counters map[string]int&#125;func (s *Stats) Snapshot() map[string]int &#123; s.mu.Lock() defer s.mu.Unlock() result := make(map[string]int, len(s.counters)) for k, v := range s.counters &#123; result[k] = v &#125; return result&#125;// snapshot 现在是一个拷贝snapshot := stats.Snapshot() 使用 defer 释放资源使用 defer 释放资源，诸如文件和锁。 BadGood 12345678910111213p.Lock()if p.count &lt; 10 &#123; p.Unlock() return p.count&#125;p.count++newCount := p.countp.Unlock()return newCount// 当有多个 return 分支时，很容易遗忘 unlock 1234567891011p.Lock()defer p.Unlock()if p.count &lt; 10 &#123; return p.count&#125;p.count++return p.count// 更可读 Defer 的开销非常小，只有在您可以证明函数执行时间处于纳秒级的程度时，才应避免这样做。使用 defer 提升可读性是值得的，因为使用它们的成本微不足道。尤其适用于那些不仅仅是简单内存访问的较大的方法，在这些方法中其他计算的资源消耗远超过 defer。 Channel 的 size 要么是 1，要么是无缓冲的channel 通常 size 应为 1 或是无缓冲的。默认情况下，channel 是无缓冲的，其 size 为零。任何其他尺寸都必须经过严格的审查。考虑如何确定大小，是什么阻止了 channel 在负载下被填满并阻止写入，以及发生这种情况时发生了什么。 BadGood 12// 应该足以满足任何情况！c := make(chan int, 64) 1234// 大小：1c := make(chan int, 1) // 或者// 无缓冲 channel，大小为 0c := make(chan int) 枚举从 1 开始在 Go 中引入枚举的标准方法是声明一个自定义类型和一个使用了 iota 的 const 组。由于变量的默认值为 0，因此通常应以非零值开头枚举。 BadGood 123456789type Operation intconst ( Add Operation = iota Subtract Multiply)// Add=0, Subtract=1, Multiply=2 123456789type Operation intconst ( Add Operation = iota + 1 Subtract Multiply)// Add=1, Subtract=2, Multiply=3 在某些情况下，使用零值是有意义的（枚举从零开始），例如，当零值是理想的默认行为时。 123456789type LogOutput intconst ( LogToStdout LogOutput = iota LogToFile LogToRemote)// LogToStdout=0, LogToFile=1, LogToRemote=2 错误类型Go 中有多种声明错误（Error) 的选项： errors.New 对于简单静态字符串的错误 fmt.Errorf 用于格式化的错误字符串 实现 Error() 方法的自定义类型 用 &quot;pkg/errors&quot;.Wrap 的 Wrapped errors 返回错误时，请考虑以下因素以确定最佳选择： 这是一个不需要额外信息的简单错误吗？如果是这样，errors.New 足够了。 客户需要检测并处理此错误吗？如果是这样，则应使用自定义类型并实现该 Error() 方法。 您是否正在传播下游函数返回的错误？如果是这样，请查看本文后面有关错误包装 section on error wrapping 部分的内容。 否则 fmt.Errorf 就可以了。 如果客户端需要检测错误，并且您已使用创建了一个简单的错误 errors.New，请使用一个错误变量。 BadGood 1234567891011121314151617// package foofunc Open() error &#123; return errors.New(\"could not open\")&#125;// package barfunc use() &#123; if err := foo.Open(); err != nil &#123; if err.Error() == \"could not open\" &#123; // handle &#125; else &#123; panic(\"unknown error\") &#125; &#125;&#125; 1234567891011121314151617// package foovar ErrCouldNotOpen = errors.New(\"could not open\")func Open() error &#123; return ErrCouldNotOpen&#125;// package barif err := foo.Open(); err != nil &#123; if err == foo.ErrCouldNotOpen &#123; // handle &#125; else &#123; panic(\"unknown error\") &#125;&#125; 如果您有可能需要客户端检测的错误，并且想向其中添加更多信息（例如，它不是静态字符串），则应使用自定义类型。 BadGood 12345678910111213func open(file string) error &#123; return fmt.Errorf(\"file %q not found\", file)&#125;func use() &#123; if err := open(); err != nil &#123; if strings.Contains(err.Error(), \"not found\") &#123; // handle &#125; else &#123; panic(\"unknown error\") &#125; &#125;&#125; 123456789101112131415161718192021type errNotFound struct &#123; file string&#125;func (e errNotFound) Error() string &#123; return fmt.Sprintf(\"file %q not found\", e.file)&#125;func open(file string) error &#123; return errNotFound&#123;file: file&#125;&#125;func use() &#123; if err := open(); err != nil &#123; if _, ok := err.(errNotFound); ok &#123; // handle &#125; else &#123; panic(\"unknown error\") &#125; &#125;&#125; 直接导出自定义错误类型时要小心，因为它们已成为程序包公共 API 的一部分。最好公开匹配器功能以检查错误。 12345678910111213141516171819202122232425262728// package footype errNotFound struct &#123; file string&#125;func (e errNotFound) Error() string &#123; return fmt.Sprintf(\"file %q not found\", e.file)&#125;func IsNotFoundError(err error) bool &#123; _, ok := err.(errNotFound) return ok&#125;func Open(file string) error &#123; return errNotFound&#123;file: file&#125;&#125;// package barif err := foo.Open(\"foo\"); err != nil &#123; if foo.IsNotFoundError(err) &#123; // handle &#125; else &#123; panic(\"unknown error\") &#125;&#125; 错误包装 (Error Wrapping)一个（函数/方法）调用失败时，有三种主要的错误传播方式： 如果没有要添加的其他上下文，并且您想要维护原始错误类型，则返回原始错误。 添加上下文，使用 &quot;pkg/errors&quot;.Wrap 以便错误消息提供更多上下文 ,&quot;pkg/errors&quot;.Cause 可用于提取原始错误。Use fmt.Errorf if the callers do not need to detect or handle that specific error case. 如果调用者不需要检测或处理的特定错误情况，使用 fmt.Errorf。 建议在可能的地方添加上下文，以使您获得诸如“调用服务 foo：连接被拒绝”之类的更有用的错误，而不是诸如“连接被拒绝”之类的模糊错误。 在将上下文添加到返回的错误时，请避免使用“failed to”之类的短语来保持上下文简洁，这些短语会陈述明显的内容，并随着错误在堆栈中的渗透而逐渐堆积： BadGood 12345s, err := store.New()if err != nil &#123; return fmt.Errorf( \"failed to create new store: %s\", err)&#125; 12345s, err := store.New()if err != nil &#123; return fmt.Errorf( \"new store: %s\", err)&#125; 1failed to x: failed to y: failed to create new store: the error 1x: y: new store: the error 但是，一旦将错误发送到另一个系统，就应该明确消息是错误消息（例如使用err标记，或在日志中以”Failed”为前缀）。 另请参见 Don’t just check errors, handle them gracefully. 不要只是检查错误，要优雅地处理错误 处理类型断言失败type assertion 的单个返回值形式针对不正确的类型将产生 panic。因此，请始终使用“comma ok”的惯用法。 BadGood 1t := i.(string) 1234t, ok := i.(string)if !ok &#123; // 优雅地处理错误&#125; 不要 panic在生产环境中运行的代码必须避免出现 panic。panic 是 cascading failures 级联失败的主要根源 。如果发生错误，该函数必须返回错误，并允许调用方决定如何处理它。 BadGood 1234567891011121314func foo(bar string) &#123; if len(bar) == 0 &#123; panic(\"bar must not be empty\") &#125; // ...&#125;func main() &#123; if len(os.Args) != 2 &#123; fmt.Println(\"USAGE: foo &lt;bar&gt;\") os.Exit(1) &#125; foo(os.Args[1])&#125; 1234567891011121314151617func foo(bar string) error &#123; if len(bar) == 0 &#123; return errors.New(\"bar must not be empty\") &#125; // ... return nil&#125;func main() &#123; if len(os.Args) != 2 &#123; fmt.Println(\"USAGE: foo &lt;bar&gt;\") os.Exit(1) &#125; if err := foo(os.Args[1]); err != nil &#123; panic(err) &#125;&#125; panic/recover 不是错误处理策略。仅当发生不可恢复的事情（例如：nil 引用）时，程序才必须 panic。程序初始化是一个例外：程序启动时应使程序中止的不良情况可能会引起 panic。 1var _statusTemplate = template.Must(template.New(\"name\").Parse(\"_statusHTML\")) 即使在测试代码中，也优先使用t.Fatal或者t.FailNow而不是 panic 来确保失败被标记。 BadGood 123456// func TestFoo(t *testing.T)f, err := ioutil.TempFile(\"\", \"test\")if err != nil &#123; panic(\"failed to set up test\")&#125; 123456// func TestFoo(t *testing.T)f, err := ioutil.TempFile(\"\", \"test\")if err != nil &#123; t.Fatal(\"failed to set up test\")&#125; 使用 go.uber.org/atomic使用 sync/atomic 包的原子操作对原始类型 (int32, int64等）进行操作，因为很容易忘记使用原子操作来读取或修改变量。 go.uber.org/atomic 通过隐藏基础类型为这些操作增加了类型安全性。此外，它包括一个方便的atomic.Bool类型。 BadGood 123456789101112131415type foo struct &#123; running int32 // atomic&#125;func (f* foo) start() &#123; if atomic.SwapInt32(&amp;f.running, 1) == 1 &#123; // already running… return &#125; // start the Foo&#125;func (f *foo) isRunning() bool &#123; return f.running == 1 // race!&#125; 123456789101112131415type foo struct &#123; running atomic.Bool&#125;func (f *foo) start() &#123; if f.running.Swap(true) &#123; // already running… return &#125; // start the Foo&#125;func (f *foo) isRunning() bool &#123; return f.running.Load()&#125; 性能性能方面的特定准则只适用于高频场景。 优先使用 strconv 而不是 fmt将原语转换为字符串或从字符串转换时，strconv速度比fmt快。 BadGood 123for i := 0; i &lt; b.N; i++ &#123; s := fmt.Sprint(rand.Int())&#125; 123for i := 0; i &lt; b.N; i++ &#123; s := strconv.Itoa(rand.Int())&#125; 1BenchmarkFmtSprint-4 143 ns/op 2 allocs/op 1BenchmarkStrconv-4 64.2 ns/op 1 allocs/op 避免字符串到字节的转换不要反复从固定字符串创建字节 slice。相反，请执行一次转换并捕获结果。 BadGood 123for i := 0; i &lt; b.N; i++ &#123; w.Write([]byte(\"Hello world\"))&#125; 1234data := []byte(\"Hello world\")for i := 0; i &lt; b.N; i++ &#123; w.Write(data)&#125; 1BenchmarkBad-4 50000000 22.2 ns/op 1BenchmarkGood-4 500000000 3.25 ns/op 尽量初始化时指定 Map 容量在尽可能的情况下，在使用 make() 初始化的时候提供容量信息 1make(map[T1]T2, hint) 为 make() 提供容量信息（hint）尝试在初始化时调整 map 大小，这减少了在将元素添加到 map 时增长和分配的开销。注意，map 不能保证分配 hint 个容量。因此，即使提供了容量，添加元素仍然可以进行分配。 BadGood 123456m := make(map[string]os.FileInfo)files, _ := ioutil.ReadDir(\"./files\")for _, f := range files &#123; m[f.Name()] = f&#125; 1234567files, _ := ioutil.ReadDir(\"./files\")m := make(map[string]os.FileInfo, len(files))for _, f := range files &#123; m[f.Name()] = f&#125; m 是在没有大小提示的情况下创建的； 在运行时可能会有更多分配。 m 是有大小提示创建的；在运行时可能会有更少的分配。 规范一致性本文中概述的一些标准都是客观性的评估，是根据场景、上下文、或者主观性的判断； 但是最重要的是，保持一致. 一致性的代码更容易维护、是更合理的、需要更少的学习成本、并且随着新的约定出现或者出现错误后更容易迁移、更新、修复 bug 相反，一个单一的代码库会导致维护成本开销、不确定性和认知偏差。所有这些都会直接导致速度降低、代码审查痛苦、而且增加 bug 数量 将这些标准应用于代码库时，建议在 package（或更大）级别进行更改，子包级别的应用程序通过将多个样式引入到同一代码中，违反了上述关注点。 相似的声明放在一组Go 语言支持将相似的声明放在一个组内。 BadGood 12import \"a\"import \"b\" 1234import ( \"a\" \"b\") 这同样适用于常量、变量和类型声明： BadGood 123456789const a = 1const b = 2var a = 1var b = 2type Area float64type Volume float64 1234567891011121314const ( a = 1 b = 2)var ( a = 1 b = 2)type ( Area float64 Volume float64) 仅将相关的声明放在一组。不要将不相关的声明放在一组。 BadGood 12345678type Operation intconst ( Add Operation = iota + 1 Subtract Multiply ENV_VAR = \"MY_ENV\") 123456789type Operation intconst ( Add Operation = iota + 1 Subtract Multiply)const ENV_VAR = \"MY_ENV\" 分组使用的位置没有限制，例如：你可以在函数内部使用它们： BadGood 1234567func f() string &#123; var red = color.New(0xff0000) var green = color.New(0x00ff00) var blue = color.New(0x0000ff) ...&#125; 123456789func f() string &#123; var ( red = color.New(0xff0000) green = color.New(0x00ff00) blue = color.New(0x0000ff) ) ...&#125; import 分组导入应该分为两组： 标准库 其他库 默认情况下，这是 goimports 应用的分组。 BadGood 123456import ( \"fmt\" \"os\" \"go.uber.org/atomic\" \"golang.org/x/sync/errgroup\") 1234567import ( \"fmt\" \"os\" \"go.uber.org/atomic\" \"golang.org/x/sync/errgroup\") 包名当命名包时，请按下面规则选择一个名称： 全部小写。没有大写或下划线。 大多数使用命名导入的情况下，不需要重命名。 简短而简洁。请记住，在每个使用的地方都完整标识了该名称。 不用复数。例如net/url，而不是net/urls。 不要用“common”，“util”，“shared”或“lib”。这些是不好的，信息量不足的名称。 另请参阅 Package Names 和 Go 包样式指南. 函数名我们遵循 Go 社区关于使用 MixedCaps 作为函数名 的约定。有一个例外，为了对相关的测试用例进行分组，函数名可能包含下划线，如：TestMyFunction_WhatIsBeingTested. 导入别名如果程序包名称与导入路径的最后一个元素不匹配，则必须使用导入别名。 123456import ( \"net/http\" client \"example.com/client-go\" trace \"example.com/trace/v2\") 在所有其他情况下，除非导入之间有直接冲突，否则应避免导入别名。 BadGood 123456import ( \"fmt\" \"os\" nettrace \"golang.net/x/trace\") 1234567import ( \"fmt\" \"os\" \"runtime/trace\" nettrace \"golang.net/x/trace\") 函数分组与顺序 函数应按粗略的调用顺序排序。 同一文件中的函数应按接收者分组。 因此，导出的函数应先出现在文件中，放在struct, const, var定义的后面。 在定义类型之后，但在接收者的其余方法之前，可能会出现一个 newXYZ()/NewXYZ() 由于函数是按接收者分组的，因此普通工具函数应在文件末尾出现。 BadGood 12345678910111213func (s *something) Cost() &#123; return calcCost(s.weights)&#125;type something struct&#123; ... &#125;func calcCost(n []int) int &#123;...&#125;func (s *something) Stop() &#123;...&#125;func newSomething() *something &#123; return &amp;something&#123;&#125;&#125; 12345678910111213type something struct&#123; ... &#125;func newSomething() *something &#123; return &amp;something&#123;&#125;&#125;func (s *something) Cost() &#123; return calcCost(s.weights)&#125;func (s *something) Stop() &#123;...&#125;func calcCost(n []int) int &#123;...&#125; 减少嵌套代码应通过尽可能先处理错误情况/特殊情况并尽早返回或继续循环来减少嵌套。减少嵌套多个级别的代码的代码量。 BadGood 123456789101112for _, v := range data &#123; if v.F1 == 1 &#123; v = process(v) if err := v.Call(); err == nil &#123; v.Send() &#125; else &#123; return err &#125; &#125; else &#123; log.Printf(\"Invalid v: %v\", v) &#125;&#125; 123456789101112for _, v := range data &#123; if v.F1 != 1 &#123; log.Printf(\"Invalid v: %v\", v) continue &#125; v = process(v) if err := v.Call(); err != nil &#123; return err &#125; v.Send()&#125; 不必要的 else如果在 if 的两个分支中都设置了变量，则可以将其替换为单个 if。 BadGood 123456var a intif b &#123; a = 100&#125; else &#123; a = 10&#125; 1234a := 10if b &#123; a = 100&#125; 顶层变量声明在顶层，使用标准var关键字。请勿指定类型，除非它与表达式的类型不同。 BadGood 123var _s string = F()func F() string &#123; return \"A\" &#125; 12345var _s = F()// 由于 F 已经明确了返回一个字符串类型，因此我们没有必要显式指定_s 的类型// 还是那种类型func F() string &#123; return \"A\" &#125; 如果表达式的类型与所需的类型不完全匹配，请指定类型。 12345678type myError struct&#123;&#125;func (myError) Error() string &#123; return \"error\" &#125;func F() myError &#123; return myError&#123;&#125; &#125;var _e error = F()// F 返回一个 myError 类型的实例，但是我们要 error 类型 对于未导出的顶层常量和变量，使用_作为前缀在未导出的顶级vars和consts， 前面加上前缀_，以使它们在使用时明确表示它们是全局符号。 例外：未导出的错误值，应以err开头。 基本依据：顶级变量和常量具有包范围作用域。使用通用名称可能很容易在其他文件中意外使用错误的值。 BadGood 1234567891011121314151617// foo.goconst ( defaultPort = 8080 defaultUser = \"user\")// bar.gofunc Bar() &#123; defaultPort := 9090 ... fmt.Println(\"Default port\", defaultPort) // We will not see a compile error if the first line of // Bar() is deleted.&#125; 123456// foo.goconst ( _defaultPort = 8080 _defaultUser = \"user\") 结构体中的嵌入嵌入式类型（例如 mutex）应位于结构体内的字段列表的顶部，并且必须有一个空行将嵌入式字段与常规字段分隔开。 BadGood 1234type Client struct &#123; version int http.Client&#125; 12345type Client struct &#123; http.Client version int&#125; 使用字段名初始化结构体初始化结构体时，几乎始终应该指定字段名称。现在由 go vet 强制执行。 BadGood 1k := User&#123;\"John\", \"Doe\", true&#125; 12345k := User&#123; FirstName: \"John\", LastName: \"Doe\", Admin: true,&#125; 例外：如果有 3 个或更少的字段，则可以在测试表中省略字段名称。 1234567tests := []struct&#123; op Operation want string&#125;&#123; &#123;Add, \"add\"&#125;, &#123;Subtract, \"subtract\"&#125;,&#125; 本地变量声明如果将变量明确设置为某个值，则应使用短变量声明形式 (:=)。 BadGood 1var s = \"foo\" 1s := \"foo\" 但是，在某些情况下，var 使用关键字时默认值会更清晰。例如，声明空切片。 BadGood 12345678func f(list []int) &#123; filtered := []int&#123;&#125; for _, v := range list &#123; if v &gt; 10 &#123; filtered = append(filtered, v) &#125; &#125;&#125; 12345678func f(list []int) &#123; var filtered []int for _, v := range list &#123; if v &gt; 10 &#123; filtered = append(filtered, v) &#125; &#125;&#125; nil 是一个有效的 slicenil 是一个有效的长度为 0 的 slice，这意味着， 您不应明确返回长度为零的切片。应该返回nil 来代替。 BadGood 123if x == \"\" &#123; return []int&#123;&#125;&#125; 123if x == \"\" &#123; return nil&#125; 要检查切片是否为空，请始终使用len(s) == 0。而非 nil。 BadGood 123func isEmpty(s []string) bool &#123; return s == nil&#125; 123func isEmpty(s []string) bool &#123; return len(s) == 0&#125; 零值切片（用var声明的切片）可立即使用，无需调用make()创建。 BadGood 12345678910nums := []int&#123;&#125;// or, nums := make([]int)if add1 &#123; nums = append(nums, 1)&#125;if add2 &#123; nums = append(nums, 2)&#125; 123456789var nums []intif add1 &#123; nums = append(nums, 1)&#125;if add2 &#123; nums = append(nums, 2)&#125; 小变量作用域如果有可能，尽量缩小变量作用范围。除非它与 减少嵌套的规则冲突。 BadGood 1234err := ioutil.WriteFile(name, data, 0644)if err != nil &#123; return err&#125; 123if err := ioutil.WriteFile(name, data, 0644); err != nil &#123; return err&#125; 如果需要在 if 之外使用函数调用的结果，则不应尝试缩小范围。 BadGood 1234567891011if data, err := ioutil.ReadFile(name); err == nil &#123; err = cfg.Decode(data) if err != nil &#123; return err &#125; fmt.Println(cfg) return nil&#125; else &#123; return err&#125; 1234567891011data, err := ioutil.ReadFile(name)if err != nil &#123; return err&#125;if err := cfg.Decode(data); err != nil &#123; return err&#125;fmt.Println(cfg)return nil 避免参数语义不明确(Avoid Naked Parameters)函数调用中的意义不明确的参数可能会损害可读性。当参数名称的含义不明显时，请为参数添加 C 样式注释 (/* ... */) BadGood 123// func printInfo(name string, isLocal, done bool)printInfo(\"foo\", true, true) 123// func printInfo(name string, isLocal, done bool)printInfo(\"foo\", true /* isLocal */, true /* done */) 对于上面的示例代码，还有一种更好的处理方式是将上面的 bool 类型换成自定义类型。将来，该参数可以支持不仅仅局限于两个状态（true/false）。 12345678910111213141516type Region intconst ( UnknownRegion Region = iota Local)type Status intconst ( StatusReady = iota + 1 StatusDone // Maybe we will have a StatusInProgress in the future.)func printInfo(name string, region Region, status Status) 使用原始字符串字面值，避免转义Go 支持使用 原始字符串字面值，也就是 “ ` “ 来表示原生字符串，在需要转义的场景下，我们应该尽量使用这种方案来替换。 可以跨越多行并包含引号。使用这些字符串可以避免更难阅读的手工转义的字符串。 BadGood 1wantError := \"unknown name:\\\"test\\\"\" 1wantError := `unknown error:\"test\"` 初始化 Struct 引用在初始化结构引用时，请使用&amp;T{}代替new(T)，以使其与结构体初始化一致。 BadGood 12345sval := T&#123;Name: \"foo\"&#125;// inconsistentsptr := new(T)sptr.Name = \"bar\" 123sval := T&#123;Name: \"foo\"&#125;sptr := &amp;T&#123;Name: \"bar\"&#125; 初始化 Maps对于空 map 请使用 make(..) 初始化， 并且 map 是通过编程方式填充的。这使得 map 初始化在表现上不同于声明，并且它还可以方便地在 make 后添加大小提示。 BadGood 123456var ( // m1 读写安全; // m2 在写入时会 panic m1 = map[T1]T2&#123;&#125; m2 map[T1]T2) 123456var ( // m1 读写安全; // m2 在写入时会 panic m1 = make(map[T1]T2) m2 map[T1]T2) 声明和初始化看起来非常相似的。 声明和初始化看起来差别非常大。 在尽可能的情况下，请在初始化时提供 map 容量大小，详细请看 尽量初始化时指定 Map 容量。 另外，如果 map 包含固定的元素列表，则使用 map literals(map 初始化列表) 初始化映射。 BadGood 1234m := make(map[T1]T2, 3)m[k1] = v1m[k2] = v2m[k3] = v3 12345m := map[T1]T2&#123; k1: v1, k2: v2, k3: v3,&#125; 基本准则是：在初始化时使用 map 初始化列表 来添加一组固定的元素。否则使用 make (如果可以，请尽量指定 map 容量)。 字符串 string format如果你为Printf-style 函数声明格式字符串，请将格式化字符串放在外面，并将其设置为const常量。 这有助于go vet对格式字符串执行静态分析。 BadGood 12msg := \"unexpected values %v, %v\\n\"fmt.Printf(msg, 1, 2) 12const msg = \"unexpected values %v, %v\\n\"fmt.Printf(msg, 1, 2) 命名 Printf 样式的函数声明Printf-style 函数时，请确保go vet可以检测到它并检查格式字符串。 这意味着您应尽可能使用预定义的Printf-style 函数名称。go vet将默认检查这些。有关更多信息，请参见 Printf 系列。 如果不能使用预定义的名称，请以 f 结束选择的名称：Wrapf，而不是Wrap。go vet可以要求检查特定的 Printf 样式名称，但名称必须以f结尾。 1$ go vet -printfuncs=wrapf,statusf 另请参阅 go vet: Printf family check. 编程模式表驱动测试当测试逻辑是重复的时候，通过 subtests 使用 table 驱动的方式编写 case 代码看上去会更简洁。 BadGood 123456789101112131415161718192021// func TestSplitHostPort(t *testing.T)host, port, err := net.SplitHostPort(\"192.0.2.0:8000\")require.NoError(t, err)assert.Equal(t, \"192.0.2.0\", host)assert.Equal(t, \"8000\", port)host, port, err = net.SplitHostPort(\"192.0.2.0:http\")require.NoError(t, err)assert.Equal(t, \"192.0.2.0\", host)assert.Equal(t, \"http\", port)host, port, err = net.SplitHostPort(\":8000\")require.NoError(t, err)assert.Equal(t, \"\", host)assert.Equal(t, \"8000\", port)host, port, err = net.SplitHostPort(\"1:8\")require.NoError(t, err)assert.Equal(t, \"1\", host)assert.Equal(t, \"8\", port) 12345678910111213141516171819202122232425262728293031323334353637// func TestSplitHostPort(t *testing.T)tests := []struct&#123; give string wantHost string wantPort string&#125;&#123; &#123; give: \"192.0.2.0:8000\", wantHost: \"192.0.2.0\", wantPort: \"8000\", &#125;, &#123; give: \"192.0.2.0:http\", wantHost: \"192.0.2.0\", wantPort: \"http\", &#125;, &#123; give: \":8000\", wantHost: \"\", wantPort: \"8000\", &#125;, &#123; give: \"1:8\", wantHost: \"1\", wantPort: \"8\", &#125;,&#125;for _, tt := range tests &#123; t.Run(tt.give, func(t *testing.T) &#123; host, port, err := net.SplitHostPort(tt.give) require.NoError(t, err) assert.Equal(t, tt.wantHost, host) assert.Equal(t, tt.wantPort, port) &#125;)&#125; 很明显，使用 test table 的方式在代码逻辑扩展的时候，比如新增 test case，都会显得更加的清晰。 我们遵循这样的约定：将结构体切片称为tests。 每个测试用例称为tt。此外，我们鼓励使用give和want前缀说明每个测试用例的输入和输出值。 1234567891011tests := []struct&#123; give string wantHost string wantPort string&#125;&#123; // ...&#125;for _, tt := range tests &#123; // ...&#125; 功能选项功能选项是一种模式，您可以在其中声明一个不透明 Option 类型，该类型在某些内部结构中记录信息。您接受这些选项的可变编号，并根据内部结构上的选项记录的全部信息采取行动。 将此模式用于您需要扩展的构造函数和其他公共 API 中的可选参数，尤其是在这些功能上已经具有三个或更多参数的情况下。 BadGood 1234567891011121314151617// package dbfunc Connect( addr string, timeout time.Duration, caching bool,) (*Connection, error) &#123; // ...&#125;// Timeout and caching must always be provided,// even if the user wants to use the default.db.Connect(addr, db.DefaultTimeout, db.DefaultCaching)db.Connect(addr, newTimeout, db.DefaultCaching)db.Connect(addr, db.DefaultTimeout, false /* caching */)db.Connect(addr, newTimeout, false /* caching */) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455type options struct &#123; timeout time.Duration caching bool&#125;// Option overrides behavior of Connect.type Option interface &#123; apply(*options)&#125;type optionFunc func(*options)func (f optionFunc) apply(o *options) &#123; f(o)&#125;func WithTimeout(t time.Duration) Option &#123; return optionFunc(func(o *options) &#123; o.timeout = t &#125;)&#125;func WithCaching(cache bool) Option &#123; return optionFunc(func(o *options) &#123; o.caching = cache &#125;)&#125;// Connect creates a connection.func Connect( addr string, opts ...Option,) (*Connection, error) &#123; options := options&#123; timeout: defaultTimeout, caching: defaultCaching, &#125; for _, o := range opts &#123; o.apply(&amp;options) &#125; // ...&#125;// Options must be provided only if needed.db.Connect(addr)db.Connect(addr, db.WithTimeout(newTimeout))db.Connect(addr, db.WithCaching(false))db.Connect( addr, db.WithCaching(false), db.WithTimeout(newTimeout),) 还可以参考下面资料： Self-referential functions and the design of options Functional options for friendly APIs","categories":[],"tags":[{"name":"Golang","slug":"Golang","permalink":"https://razertory.me/tags/Golang/"}]},{"title":"最小栈","slug":"min-stack","date":"2019-11-04T04:58:26.000Z","updated":"2020-03-28T13:02:10.011Z","comments":true,"path":"2019/11/04/min-stack/","link":"","permalink":"https://razertory.me/2019/11/04/min-stack/","excerpt":"","text":"实现一个最小栈，有三种操作，min：得到栈中的最小值，push：在栈顶插入一个元素，pop：弹出栈顶元素，使这三种操作的时间复杂度都是O(1) 1234public class StackWithMin extends Stack&lt;Integer&gt; &#123; public Integer push(Integer item) public Integer pop()&#125; 额外维护一个栈 minStack 用来随时获取当前最小值。 对于 push(value) 判断当前 minStack 的 top 是否小于 value，如果小于就 minStack.push(value) 对于 pop() 判断当前 minStack 的 top 是否等于 value，如果等于就 minStack.pop() 实现一个获取最小值的函数 getMin() 用来完善上述的 top 12345678910111213141516171819202122232425262728293031public class StackWithMin extends Stack&lt;Integer&gt; &#123; Stack&lt;Integer&gt; stackMin; public StackWithMin()&#123; stackMin = new Stack&lt;Integer&gt;(); &#125; public Integer push(Integer item)&#123; if(item &lt;= min())&#123; stackMin.push(item); &#125; super.push(item); return item; &#125; public Integer pop()&#123; int value = super.pop(); if(value == min())&#123; stackMin.pop(); &#125; return value; &#125; public Integer min()&#123; if(stackMin.isEmpty())&#123; return Integer.MAX_VALUE; &#125; else &#123; return stackMin.peek(); &#125; &#125;&#125;","categories":[],"tags":[]},{"title":"判断是否是二叉搜索树","slug":"is-bst","date":"2019-11-04T02:30:22.000Z","updated":"2020-03-28T13:02:10.011Z","comments":true,"path":"2019/11/04/is-bst/","link":"","permalink":"https://razertory.me/2019/11/04/is-bst/","excerpt":"","text":"输入一个二叉树的 root 节点，判断是否是二叉搜索树。 1public boolean isBST(TreeNode root) 二叉搜索树的定义： 若任意节点的左子树不空，则左子树上所有节点的值均小于它的根节点的值； 若任意节点的右子树不空，则右子树上所有节点的值均大于它的根节点的值； 任意节点的左、右子树也分别为二叉查找树； 没有键值相等的节点。 根据这个定义，比如有个函数 f(root) 用作判断是否是 BST，那么这个函数进一步可以定义为 f(root, min, max)，min 和 max 表示遍历过程中的最大值和最小值。在当前状态下，root.val &gt; min 且 root.val &lt; max 就是合法的。同样的，对于 root.left 和 root.right 也需要满足上述条件。写成的伪代码就是 12345f(root, min, max) &#123;if root.val &lt; min return falseif root.val &gt; max return falsereturn f(root.left, min, root.val) &amp;&amp; f (root.right, root.val, max)&#125; 通过 inOrder(中序遍历) 是另一种巧妙的方法，基于二叉搜索树的定义可以明确知道中序遍历的过程一定是节点值从小到大的，因此可以认为 只要没有满足递增的序列，就不是合法的二叉搜索树 123456789101112131415161718192021222324// min, max 法boolean isValidBST(TreeNode root, TreeNode lower, TreeNode upper) &#123; if (root == null) return true; if (lower != null &amp;&amp; lower.val &gt;= root.val) return false; if (upper != null &amp;&amp; upper.val &lt;= root.val) return false; return isValidBST(root.left, lower, root) &amp;&amp; isValidBST(root.right, root, upper);&#125;// 中序遍历法public boolean isValidBST(TreeNode root) &#123; ArrayList&lt;Integer&gt; order = new ArrayList&lt;&gt;(); inOrder(root, order); for (int i = 1; i &lt; order.size(); i++) &#123; if (order.get(i) &lt;= order.get(i - 1)) return false; &#125; return true;&#125;private void inOrder(TreeNode root, ArrayList&lt;Integer&gt; order) &#123; if (root == null) return; inOrder(root.left, order); order.add(root.val); inOrder(root.right, order);&#125;","categories":[],"tags":[]},{"title":"和为某个值的二叉树路径","slug":"sum-path","date":"2019-11-04T02:19:06.000Z","updated":"2020-03-28T13:02:10.011Z","comments":true,"path":"2019/11/04/sum-path/","link":"","permalink":"https://razertory.me/2019/11/04/sum-path/","excerpt":"","text":"输入一颗二叉树的跟节点和一个整数，打印出二叉树中结点值的和为输入整数的所有路径。路径定义为从树的根结点开始往下一直到叶结点所经过的结点形成一条路径。(注意: 在返回值的list中，数组长度大的数组靠前) 1public ArrayList&lt;ArrayList&lt;Integer&gt;&gt; FindPath(TreeNode root,int target) 维护一个二维数组，从 root 节点开始进行 dfs。每遍历一个节点， target - 节点值；同时给数组加上该节点。当 target 的值和节点值相等的时候，说明这条路径是合法路径。在二维数组中加上该路径。 123456789101112131415public ArrayList&lt;ArrayList&lt;Integer&gt;&gt; FindPath(TreeNode root,int target) &#123; ArrayList&lt;ArrayList&lt;Integer&gt;&gt; paths = new ArrayList&lt;&gt;(); ArrayList&lt;Integer&gt; path = new ArrayList&lt;&gt;(); dfs(root, path, paths, target); return paths;&#125;private void dfs(TreeNode root, ArrayList&lt;Integer&gt; path, ArrayList&lt;ArrayList&lt;Integer&gt;&gt; paths, int sum) &#123; if (root == null) return; path.add(root.val); if (root.left == null &amp;&amp; root.right == null &amp;&amp; sum == root.val) paths.add(new ArrayList&lt;&gt;(path)); dfs(root.left, path, paths, sum - root.val); dfs(root.right, path, paths, sum - root.val); path.remove(path.size() - 1);&#125;","categories":[],"tags":[]},{"title":"是否是二叉搜索树的序遍历","slug":"is-post-order","date":"2019-10-27T07:58:38.000Z","updated":"2020-03-28T13:02:10.010Z","comments":true,"path":"2019/10/27/is-post-order/","link":"","permalink":"https://razertory.me/2019/10/27/is-post-order/","excerpt":"","text":"输入一个整数数组，判断该数组是不是某二叉搜索树的后序遍历的结果。如果是则输出Yes,否则输出No。假设输入的数组的任意两个数字都互不相同。 1boolean VerifySquenceOfBST(int [] sequence) BST的后序序列的合法序列是，对于一个序列S，最后一个元素是x （也就是根），如果去掉最后一个元素的序列为T，那么T满足：T可以分成两段，前一段（左子树）小于x，后一段（右子树）大于x，且这两段（子树）都是合法的后序序列。完美的递归定义 : ) 。 1234567891011121314boolean VerifySquenceOfBST(int [] sequence) &#123; if (sequence == null || sequence.length == 0) return false; return validate(sequence, 0, sequence.length - 1);&#125;boolean validate(int[] s, int left, int right)&#123; if (left &gt;= right) return true; int cur = right; while(cur &gt; left &amp;&amp; s[cur - 1] &gt; s[right]) cur--; for (int i = left; i &lt; cur - 1; i++) &#123; if (s[i] &gt;= s[right]) return false; &#125; return validate(s, left, cur - 1) &amp;&amp; validate(s, cur, right - 1);&#125;","categories":[],"tags":[]},{"title":"判断是否是子树","slug":"is-subtree","date":"2019-10-27T07:49:47.000Z","updated":"2020-03-28T13:02:10.011Z","comments":true,"path":"2019/10/27/is-subtree/","link":"","permalink":"https://razertory.me/2019/10/27/is-subtree/","excerpt":"","text":"输入两棵二叉树A，B，判断B是不是A的子结构。（ps：我们约定空树不是任意一个树的子结构） 1boolean HasSubtree(TreeNode root1,TreeNode root2) 首先在 A 中找到一个节点，满足这个节点是 B 的 root 节点，在此基础上按照 B 的结构遍历 A，B。 12345678910111213141516171819202122boolean HasSubtree(TreeNode root1,TreeNode root2) &#123; boolean result =false; if(root1!=null&amp;&amp;root2!=null)&#123; if(root1.val==root2.val) result=isSubtree(root1,root2); if(!result) result= HasSubtree(root1.left,root2); if(!result) result= HasSubtree(root1.right,root2); &#125; return result;&#125;boolean isSubtree(TreeNode root1,TreeNode root2)&#123; if(root1==null&amp;&amp;root2!=null) return false; if(root2==null) return true; if (root1.val!=root2.val) return false; return isSubtree(root1.left, root2.left) &amp;&amp; isSubtree(root1.right, root2.right);&#125;","categories":[],"tags":[]},{"title":"链表中交换节点","slug":"swap-nodes-in-pairs","date":"2019-10-26T06:47:13.000Z","updated":"2020-03-28T13:02:10.010Z","comments":true,"path":"2019/10/26/swap-nodes-in-pairs/","link":"","permalink":"https://razertory.me/2019/10/26/swap-nodes-in-pairs/","excerpt":"","text":"Given a linked list, swap every two adjacent nodes and return its head.You may not modify the values in the list’s nodes, only nodes itself may be changed. 1ListNode swapPairs(ListNode head) 由于要交换前后两个链表节点，定义一个 dummy，cur。那么核心就是交换 cur.next 和 cur.next.next。 123456789101112ListNode swapPairs(ListNode head) &#123; ListNode dummy = new ListNode(0); dummy.next = head; ListNode cur = dummy; while(cur.next != null &amp;&amp; cur.next.next != null) &#123; ListNode a = cur.next, b = cur.next.next; cur.next = b; a.next = b.next; b.next = a; cur = a; &#125; return dummy.next;","categories":[],"tags":[]},{"title":"链表相加","slug":"add-two-numbers","date":"2019-10-26T06:38:48.000Z","updated":"2020-03-28T13:02:10.009Z","comments":true,"path":"2019/10/26/add-two-numbers/","link":"","permalink":"https://razertory.me/2019/10/26/add-two-numbers/","excerpt":"","text":"You are given two non-empty linked lists representing two non-negative integers. The digits are stored in reverse order and each of their nodes contain a single digit. Add the two numbers and return it as a linked list.You may assume the two numbers do not contain any leading zero, except the number 0 itself. 1ListNode addTwoNumbers(ListNode l1, ListNode l2) 这道题本质上就是在模拟加法运算中进位的过程。 12345678910111213141516171819ListNode addTwoNumbers(ListNode l1, ListNode l2) &#123; ListNode dummy = new ListNode(0), p = dummy; int carry = 0; while (l1 != null || l2 != null || carry != 0) &#123; int cur = carry; if (l1 != null) &#123; cur += l1.val; l1 = l1.next; &#125; if (l2 != null) &#123; cur += l2.val; l2 = l2.next; &#125; p.next = new ListNode(cur % 10); p = p.next; carry = cur / 10; &#125; return dummy.next;&#125;","categories":[],"tags":[]},{"title":"有序链表去重","slug":"delete-duplication","date":"2019-10-26T06:32:58.000Z","updated":"2020-03-28T13:02:10.009Z","comments":true,"path":"2019/10/26/delete-duplication/","link":"","permalink":"https://razertory.me/2019/10/26/delete-duplication/","excerpt":"","text":"在一个排序的链表中，存在重复的结点，请删除该链表中重复的结点，重复的结点不保留，返回链表头指针。 例如，链表1-&gt;2-&gt;3-&gt;3-&gt;4-&gt;4-&gt;5 处理后为 1-&gt;2-&gt;5 传送门 定义一个 dummy 节点，pre，cur 节点。比较 pre.next 和 cur。如果相同让 pre = pre.next 做删除操作 123456789101112131415ListNode deleteDuplication(ListNode pHead) &#123; if (pHead == null) return null; ListNode dummy = new ListNode(0); dummy.next = pHead; ListNode prev= dummy, cur = pHead; while (cur != null) &#123; while(cur.next != null &amp;&amp; cur.val == cur.next.val) cur = cur.next; if (prev.next != cur) prev.next = cur.next; else prev = prev.next; cur = prev.next; &#125; return dummy.next;&#125;","categories":[],"tags":[]},{"title":"链表环的入口","slug":"entry-node-of-loop","date":"2019-10-26T06:28:43.000Z","updated":"2020-03-28T13:02:10.009Z","comments":true,"path":"2019/10/26/entry-node-of-loop/","link":"","permalink":"https://razertory.me/2019/10/26/entry-node-of-loop/","excerpt":"","text":"给一个链表，若其中包含环，请找出该链表的环的入口结点，否则，输出null。 传送门 定义一个慢和一个快指针，当两个指针相遇的时候。快指针速度和慢指针一致，等再次相遇的时候这个点就是环入口。 1234567891011121314151617ListNode EntryNodeOfLoop(ListNode pHead) &#123; if (pHead == null || pHead.next == null) return null; ListNode slow = pHead, fast = pHead; while(fast != null &amp;&amp; fast.next != null)&#123; slow = slow.next; fast = fast.next.next; if (fast == slow) &#123; slow = pHead; while(slow != fast)&#123; slow = slow.next; fast = fast.next; &#125; return slow; &#125; &#125; return null;&#125;","categories":[],"tags":[]},{"title":"链表的公共节点","slug":"first-common-node","date":"2019-10-26T06:23:52.000Z","updated":"2020-03-28T13:02:10.010Z","comments":true,"path":"2019/10/26/first-common-node/","link":"","permalink":"https://razertory.me/2019/10/26/first-common-node/","excerpt":"","text":"输入两个链表，找出它们的第一个公共结点。 传送门 首先计算出两个链表的各自长度，然后计算出一个差值 delta。然后基于 delta 的值走对应的节点，当 delta 变为 0 的时候停止。并判断是否有公共节点 12345678910111213141516171819202122232425ListNode FindFirstCommonNode(ListNode pHead1, ListNode pHead2) &#123; if (pHead1 == null || pHead2 == null ) return null; ListNode node1 = pHead1, node2 = pHead2; int len1 = 0, len2 = 0; for (; node1 !=null; node1 = node1.next, len1++); for (; node2 !=null; node2 = node2.next, len2++); int delta = len1 - len2; node1 = pHead1; node2 = pHead2; while (delta &gt; 0) &#123; node1 = node1.next; delta--; &#125; while (delta &lt; 0) &#123; node2 = node2.next; delta++; &#125; while (node1 != node2) &#123; node1 = node1.next; node2 = node2.next; &#125; return node1;&#125;","categories":[],"tags":[]},{"title":"复制复杂链表","slug":"copy-linked-list","date":"2019-10-26T06:14:46.000Z","updated":"2020-03-28T13:02:10.009Z","comments":true,"path":"2019/10/26/copy-linked-list/","link":"","permalink":"https://razertory.me/2019/10/26/copy-linked-list/","excerpt":"","text":"输入一个复杂链表（每个节点中有节点值，以及两个指针，一个指向下一个节点，另一个特殊指针指向任意一个节点），返回结果为复制后复杂链表的 head。（注意，输出结果中请不要返回参数中的节点引用，否则判题程序会直接返回空） 传送门 每个链表都有两个指针，这种情况下最简单的做法是用一个 k, v 的方式存放所有的节点以及新的只有 label，指针指向的节点为空的节点。然后用原来的节点指针指向的值复制到新节点。 123456789101112131415161718192021222324// public class RandomListNode &#123;// int label;// RandomListNode next = null;// RandomListNode random = null;// RandomListNode(int label) &#123;// this.label = label;// &#125;// &#125;RandomListNode Clone(RandomListNode pHead) &#123; HashMap&lt;RandomListNode, RandomListNode&gt; map = new HashMap&lt;&gt;(); RandomListNode cur = pHead; while(cur != null) &#123; map.put(cur, new RandomListNode(cur.label)); cur = cur.next; &#125; cur = pHead; while(cur != null) &#123; map.get(cur).next = map.get(cur.next); map.get(cur).random = map.get(cur.random); cur = cur.next; &#125; return map.get(pHead);&#125;","categories":[],"tags":[]},{"title":"反转链表","slug":"reverse-list","date":"2019-10-26T06:05:27.000Z","updated":"2020-03-28T13:02:10.010Z","comments":true,"path":"2019/10/26/reverse-list/","link":"","permalink":"https://razertory.me/2019/10/26/reverse-list/","excerpt":"","text":"输入一个链表，反转链表后，输出新链表的表头。Example: 12Input: 1-&gt;2-&gt;3-&gt;4-&gt;5-&gt;NULLOutput: 5-&gt;4-&gt;3-&gt;2-&gt;1-&gt;NULL 传送门 反转的过程需要提前准备好下个节点的，用到两个指针，cur 和 pre 指向新节点和用于存放原本的下个节点 12345678910ListNode reverseList(ListNode head) &#123; ListNode cur = head, pre = null; while (cur != null) &#123; ListNode next = cur.next; // 提前准备好指向下个节点 cur.next = pre; pre = cur; cur = next; &#125; return pre;&#125;","categories":[],"tags":[]},{"title":"归并排序","slug":"merge-sort","date":"2019-10-26T03:36:12.000Z","updated":"2020-03-28T13:02:10.010Z","comments":true,"path":"2019/10/26/merge-sort/","link":"","permalink":"https://razertory.me/2019/10/26/merge-sort/","excerpt":"","text":"归并排序最核心的思想就是 merge 两个有序的数组。假设两个只有一个元素的数组合并，比如 [34], [33]，合并之后就是 [33, 34]。假设这个数组要和 [1, 100] 合并，合并之后就是 [1, 33, 34, 100]。 可以看出当大小为 n 的有序数组和大小为 m 的有序数组合并之后大小就是 n + m。所以最重要的就是写出高效的合并算法，假设函数的签名是 123// a， b 需要合并的数组// temp 用来存放合并后的数组void merge (int[] a, int[] b, int[] temp) 其中 a，b 可以用同一个数组中的 low，mid，high 三组下标表示，这样做的原因是为了尽量减少内存开销。 123456789101112// arr 原数组// low, mid, high 分割成两个需要 merge 的子数组void merge (int[] arr, int low, int mid, int high, int[] temp) &#123; int i = low, j = mid + 1, k = 0; while (i &lt;= mid &amp;&amp; j &lt;= high) &#123; if (arr[i] &lt;= arr[j]) tmp[k++] = arr[i++]; else tmp[k++] = arr[j++]; &#125; while (i &lt;= mid) tmp[k++] = arr[i++]; while (j &lt;= high) tmp[k++] = arr[j++]; System.arraycopy(tmp, 0, arr, low, k);&#125; 在 merge 函数已经完成的情况下，接下来就需要把原来的数组进行递归地拆分然后 merge。 12345678void mergeSort(int[] arr, int low, int high, int[] tmp) &#123; if (low &lt; high) &#123; int mid = low + (high - low) / 2; mergeSort(arr, low, mid, tmp); mergeSort(arr, mid + 1, high, tmp); merge(arr, low, mid, high, tmp); &#125;&#125;","categories":[],"tags":[]},{"title":"堆排序","slug":"heap-sort","date":"2019-10-20T06:20:42.000Z","updated":"2020-03-28T13:02:10.008Z","comments":true,"path":"2019/10/20/heap-sort/","link":"","permalink":"https://razertory.me/2019/10/20/heap-sort/","excerpt":"","text":"堆是一种能方便解决 top 问题的树形数据结构，利用这个性质也可以用来解决排序问题。首先生成最大堆，然后逆序遍历该数组，过程中通过比较堆的最大值。如果小于，就放到数组的最后一个，并且调整堆。所以在堆排序中，如何构建堆和调整堆成为了问题的关键。最简单，有效的做法就是用一个一位数组来表示堆。比如一颗二叉树可以用数组来表示： 123456[9, 4, 7, 3, 1, 2] 9 / \\ 4 7 / \\ / 3 1 2 在一位数组表示法中，比如 root 节点 9 的下标是 0，它的两个子节点下标分别是 1，2 … 通过不断比较可以发现规律。下标为 n 的树的节点，两个子节点的下标分别是 2 * n + 1, 2 * n + 2; 同样的，如果一个节点下标是 m，对应的父节点的下标是 (m - 1) / 2。那么当给一个转换成二叉树的数组 123456[3, 1, 2, 5, 6] 3 / \\ 1 2 / \\ 5 6 需要通过一个方法让这个二叉树满足最大堆性质。 首先，在这个二叉树中每个节点对应的数在数组中都有对应的下标，按照上述的根节点，子节点的关系，在数组中间之后的数就不会有对应的子节点了假设数组长度是 m，那么（2 * m / 2 + 1) &gt; m 这种情况显然不需要。也就是说目前只需要从下标为 5 / 2 = 2 的节点开始。（考虑到整数相除会有取整，因此这个地方从中间开始只是为了性能更好）。这个时候， 从下标为 2 的节点 2 开始，发现没有子节点，下标减 1 从下标为 1 的节点 1 开始，发现左节点更大，于是交换变成123456[3, 5, 2, 1, 6] 3 / \\ 5 2 / \\ 1 6 此时，节点 1 没有子节点，继续从下标 1 比较，发现右节点更大，于是交换变成 123456[3, 6, 2, 1, 5] 3 / \\ 6 2 / \\ 1 5 此时，节点 5 没有子节点，下标减 1 从下标为 0 的节点 3 开始，发现左节点更大，于是交换变成123456[6, 3, 2, 1, 5] 6 / \\ 3 2 / \\ 1 5 此时，节点 3 的右边节点更大，于是交换变成 123456[6, 5, 2, 1, 3] 6 / \\ 5 2 / \\ 1 3 此时，节点 3 没有子节点，下标减 1 下标目前已经小于 0 结束。 以上就是如何构建堆的过程，可以看到本质上就是利用数组中节点之前的关系，通过比较大小的方法来交换顺序。 当一个最大堆构建成功后，如何做排序呢？ 在最大堆中，把 root 节点和数组中最后一个值交换。这个时候把数组分为两个区域，有序区和无序区。 123456[3, 5, 2, 1, 6] 3 / \\ 5 2 / \\ 1 6 此时认为 [6] 为有序区，[3, 5, 2, 1] 为无序区。此时可以认为堆是 12345 3 / \\ 5 2 / 1 这个时候继续调整成为最大堆即可。最终有序区会逐渐填满的同时，无序区会逐渐变小直到消失。这个过程可以表示为 123456789101112131415private void buildMaxHeap(int[] arr, int end) &#123; for (int i = end/2; i &gt;= 0; --i) &#123; siftDown(arr, i, end); &#125;&#125;private void siftDown(int[] arr, int i, int end) &#123; int parent = i, child = 2 * parent + 1; while (child &lt;= end) &#123; if (child+1 &lt;= end &amp;&amp; arr[child+1] &gt; arr[child]) ++child; if (arr[parent] &gt;= arr[child]) break; swap(arr, parent, child); parent = child; child = 2 * parent + 1; &#125;","categories":[],"tags":[]},{"title":"快速排序","slug":"quick-sort","date":"2019-10-19T08:52:23.000Z","updated":"2020-03-28T13:02:10.008Z","comments":true,"path":"2019/10/19/quick-sort/","link":"","permalink":"https://razertory.me/2019/10/19/quick-sort/","excerpt":"","text":"首先挑选基准值；然后分割数组，把小于基准值的元素放到基准值前面，大于基准值的元素放到基准值后面；最后递归地对小于基准值的子序列和大于基准值的子序列进行排序。如果用函数定义快速排序 f()，那么首先给这个函数签名为 f(arr, low, high)。其中，arr 表示传入的数组对象，low 表示需要排序的开始下标，high 表示需要排序的结束下标，那么这个函数可以完整地表示为。 12345f(arr, low, high) = if low &lt; high pivot = partition(arr, low, high) f(arr, low, pivot - 1) f(arr, pivot + 1, high) 总结起来就是，选基，分组，递归。分组函数 partition(arr, low, high) 的作用是把小于基准值的放在前面，大于基准值的放在后面，最后输出分组之后基准值所在的位置。做分组最经典的做法是用 lomuto 或者 hoare 分组法。lomuto 分组法选中第一为个 pivot，利用两个游标 i 和 j 把数组划分成 1234arr[pivot] | arr[(pivot+1)..(j)] | arr[(j + 1)..(i-1)] | arr[i..high]pivot | 比 pivot 小 | 比 pivot 大 | 未处理在划分结束之后，交换 pivot 和 jlomuto 分割法里面，pivot 的选取会决定数组的分组方式 1234567891011int partition(int[] array, int low, int high) &#123; int pivot = array[low], index = low; for (int i = low; i &lt;= high; i++) &#123; if (array[i] &lt; pivot) &#123; index++; swap(array, i, index); &#125; &#125; swap(array, index, low); return index;&#125; hoare 分组法选中任意一个作为 pivot，利用两个游标 i 和 j，做数组的左右往里面扫描。i 扫过的区域一定要比 pivot 小，j 扫过的区域一定比 pivot 大。当遇到不满足条件的时候，交换 i, j 指向的内容。 1234567891011int partition(int[] array, int low, int high) &#123; int p = array[low]; int i = low, j = high; while(i &lt; j) &#123; while (i &lt; j &amp;&amp; array[j] &gt; p) j--; if(i &lt; j) swap(array, i, j); while (i &lt; j &amp;&amp; array[i] &lt;= p) i++; if(i &lt; j) swap(array, i, j); &#125; return i;&#125; ps: hoare 分组法会比 lomuto 分组法有平均少 3 倍的 swap 次数。参考 hoare 和 lomuto 都是不稳定的，并且在元素全部有序的情况下复杂度都是 O(n^2) 完整代码和测试用例 发明者 Tony Hoare","categories":[],"tags":[]},{"title":"所以怎么减少 bug 呢？","slug":"how-to-reduce-bug","date":"2019-09-22T07:41:36.000Z","updated":"2020-03-28T13:02:10.008Z","comments":true,"path":"2019/09/22/how-to-reduce-bug/","link":"","permalink":"https://razertory.me/2019/09/22/how-to-reduce-bug/","excerpt":"","text":"从软件工程诞生之初，「减少 bug」成了所有开发团队的共同目标。要减少 bug 就要知道 bug 是怎么来的: 原因 产品设计。产品逻辑存在漏洞，或者悖论。表现出来的症状是用户使用成本高，以及在非常规的操作情况下，产生了非常意外的结果。例如服务器报错，客户端闪退，严重的比如支付相关的系统带来巨大的隐患。 业务理解。对业务不熟悉，写出的代码一定程度上和业务相背。 细节把控。没有处理好代码中的边界条件。 历史数据。新的 feature 不可避免的会去读写历史数据，而历史数据可能有的是脏数据。 基础不足。例如对语言、框架不熟悉，或者理解错了。出现了主观上认为意料之外的事情 准外部原因。例如某些框架可能自身有没有修复或者没有被发现的 bug 外部原因。比如各种服务提供商出现了故障。 技术协调问题。 比如前端和服务端，服务端之间的微服务调用，一方提供的服务没有按照预期的逻辑执行 解决&amp;避免 产品尽量做到可以顺利进行 「prd review」，部分具体的实现需要和 dev 一起讨论。大多数情况下，简化逻辑是可以优先考虑的点，对用户和开发都会比较友好。 程序员自身的理解能力和工作态度。理解能力原则上在招聘期间就需要被过滤，工作态度需要从管理者自上而下地去影响，这是一个需要长期控制调整的事情。 提高编程能力和保持 unit test 来最大程度避免遗漏掉代码中的边界条件。 数据库设计满足基础范式和现代化的设计规则。如果已存在脏数据，那么依照问题的严重性来确定短期内是否考虑加大时间成本。同时关键节点打印好日志，并完善日志收集系统。 原则上通过招聘来控制。但个人的学习积累也同样重要，需要避免个人发展速度慢于公司发展速度。 技术选型上保持谨慎的态度。如果某些技术没有经过可靠的验证，原则上不引入。除非这个技术实现方式非常清晰，并且团队成员一致认可。 编码上与第 3 点保持一致。架构设计上做到能够熔断、降级，确保不因为单独故障影响到整个系统。 给项目/API加上 change log 甚至 hook 来确保协议稳定，搭配上自动生成的文档。开发过程中保持高效的沟通。","categories":[],"tags":[{"name":"思考","slug":"思考","permalink":"https://razertory.me/tags/思考/"}]},{"title":"「读书」反应式设计模式","slug":"reactive-design-pattern","date":"2019-08-04T02:23:27.000Z","updated":"2020-03-28T13:02:10.007Z","comments":true,"path":"2019/08/04/reactive-design-pattern/","link":"","permalink":"https://razertory.me/2019/08/04/reactive-design-pattern/","excerpt":"","text":"一直以来我都没有刻意去学设计模式这一块，因为认为语言本身的设计会需要人附带出一些设计模式，特别是以 Java 为主导的 OOP 语言，而这些设计模式出现其实就是让人为了接受这样的语言构造而又不得不学的东西。我认为「设计模式」应该是语言无关的，是 design-pattern 而不是 coding-pattern。 Part 1反应式宣言 于 2014 年发布，是移动互联网发展最迅猛的时间段。国内移动互联网的用户量，流量在疯狂增长。在这样的时代背景下，有一波牛逼的工程师为了让自己的系统能够更符合当前的状况以及今后的发展搞出了这么一套准则： responsive 即时响应性：必须对用户作出响应。当系统有外部调用的时候，应该有且一定有快速，一致的响应时间。这可以作为系统可用性的核心指标。获得这样的特性，本质上就是降低延迟。采用的方法有利用队列、并行化等。整体的设计需要一定程度避免出现大泥球，这一点和后来火热的为微服务理念不谋而合。但有时候我们的系统会去集成外部的非反应式的系统，并且我们的系统也依旧需要保持这样的即时响应性。后文提到的资源管理模式，流量控制模式在这种场景下会有所帮助。 resilent 回弹性: 必须对失败作出反应，保持高可用性。软件，硬件，编程人员都可能犯错，出现失误。在很多情况下我们会非常关注一个系统的可靠性 reliability，但是错误总是会不期而至。人们一方面需要去避免错误的发生，另一方面，在反应式宣言中，设计系统应该更加关注发生了错误如何让即时响应性快速恢复。对于软件硬件最普遍的做法是 replication，提供一个副本，由于提供了副本也就意味着可能会有分布式一致性问题，所以又有很多的大佬活跃在分布式存储领域。除了提供副本，还有一种方法就是隔离，这就好比设计大型船舶的船舱是一个个的小舱室互相隔离的，即时船触礁破坏了几个舱室，也不至于导致沉船事故。还有就是熔断，有时候可能因为一些的错误导致某个服务的 API 非常慢，大量的 socket 被无效占用以至于整个服务不可用。这个时候就需要短期内让该 API 立即作出快速失败而不至于影响整个系统。 elastic 弹性: 必须对不同的负载情况作出反应。当系统有更高负载的时候支持自动开启更多资源，相反减少。 message-driven 消息驱动: 必须对输入作出反应。反应式宣言中提到的是反应式系统依赖异步的消息传递。 函数式编程的本质是：洞察到程序时间上可以按照纯粹的数学函数来编写；也就是说，每次给这些函数输入相同的内容时，它们将总是返回相同的值，并且不会产生副作用。这样做的好处是代码的编写，编译，分析都可以采用纯粹的逻辑推理来保证正确性。而满足函数式通常需要做到： 函数是一等公民。这一点目前有大量的语言都做到了。流行的比如 JavaScript，Golang。 无副作用，或者叫做引用透明性。如果将一个表达式替换为其求值后的结果，程序的的执行应该不受到影响。 比如对于一个 Java 代码。 12final StringBuffer a = new StringBuffer(\"foo\");final StringBuffer b = a.reverse(); 执行之后，b 为 “oof”, a 也为 “oof”。而对于别的语言比如 Ruby 12a = \"foo\"b = a.reverse 执行之后，b 为 “oof”, a 不改变。 如今并发已经有非常多的做法。最早期的做法就是 1-1 模型，即用户逻辑线程与内核调度的线程一一对应的关系比如 Java，C/C++ 。这样做通常因为需要内核做上下文切换，在并发量大的情况下性能开销会非常大。还有是 N - 1 模型，比如 lua, 这样做可以避开上下文切换带来的开销。并且由于没有实质上的并行，可以避开一切数据竞争的问题。这一点也是我认为 lua 能够在给 nginx、redis 做扩展脚本的时候流行起来的原因。还有一种就是 N - M 模型，比如 Golang 自带的 goroutine 调度器和 Clojure 中的 core.async 库实现。这样的做法当然就是对前两种的取长补短。还有一种就是通过一个线程，然后在循环中给每个要并发执行的逻辑都注册自己的方法，之后再等待回调即可。这样的做法通常叫做事件循环（event loop），在 IO 密集业务中非常适合。 在实际的编码中，方面地编写异步非阻塞的代码通常会用到 Future 和 Promise。一种理解的方式是，Future 是一个和时间解耦的值。即 “Future is a value decoupled from time”。 英文理解这个意个例子思就是，I promise you a future。在未来的某个时间点你就可以通过我 Fulfill the promise 来实现。这个 Promise 只会完成一次。用 Java 举个例子，比方说现在有个方法 retrive() 返回某个数据，这个数据存在 DB 中也可能存在缓存中，在缓存中找会比在 DB 中找更快。所以，我需要做的是先从缓存中查找这个数据，如果没有的话就从 DB 中找到。假设简化成方法 retrieveFromCache() 和 retrieveFromDB()。传统的做法就是 12345678Object retrive(Object param) &#123; Object value = retrieveFromCache(param); if (value == null) value = retrieveFromDB(param); return value;&#125; 如果改成并发调用这两个方法以高性能，那么就需要立即拿到先返回方法的结果。首先可以给 cache 和 db 给自注册一个 future。cacheFuture 和 dbFuture。这两个 future 都包含各自的方法。然后只需要采用 CompletableFuture.anyOf(dbFuture, cacheFuture)即可。如果是 Golang 可以直接用 channel 配合 select 也行。 由于经常会遇到不同的线程之间在逻辑上会有关联，比方说有时候当某个变量值发生的更改需要别的线程都知晓。在目前，通过共享变量的方式已经被部分替代为传递消息的方式，比如 CSP 编程模型，而这个模型用的最多的就是 Golang 中的 channel。一个 goroutine 只需要把值交给 channel，那么别的 goroutine 就从这个 channel 中获取值，从而达到共享数据的效果。Actor 模型和这一点相似。都是采用消息传递而非共享变量的方式。不同的是，在 Actor 模型中，每个并发的逻辑单元被称为一个 actor，每个 actor 有自己独立的邮箱。消息发送者需要知道别的 actor 的地址从而广播出去（至于怎么广播，具体广播给谁会有一定的配置和规则）。这样做相比 CSP 有两个优点： 由于每个 Actor 都可能存有重要的消息，所以部分 Actor 挂掉也不至于影响整个系统 Actor 之间不仅可以在内存中寻址还可以跨越实例通信（比如 Erlang 和 Akka），从而有更高的可扩展性。 Part 2“事件”是建立消息传递的基石。这些事件可能是当先执行程序中函数的内容以及上下文，也可能是消息队列中的具体某些消息。有两种方式进行消息传递：事件驱动（event-based）和 消息驱动（message-based），这两种都是典型的生产者 - 消费者模型。事件驱动中，生产者把信息放到一个队列中去，消费者不断得轮询从队列中消费。消息驱动中，生产者会事先知道消费者的地址，直接把消息传递给消费者。优势在于，这样使得每个消费者可能有序且并发地消费，同时由于这种引用透明性，像 Akka 的 Actor 可以做到互相之间通过网络调用来实现通信。关于异步消息如何被保证送达，通常有三个模式：1. 至多一次（at-most-once）2. 至少一次（at-least-once）3. 确切一次（exactly-once）这三个模式被现在流行的消息队列比如 Kafka，RabiitMQ 采用。 消息驱动有个前提就是系统被拆分成了可以独立部署的组件。如何拆分一直就是个非常重要的问题。 DRY，如果说每个组件都写了一套相同了业务逻辑肯定是有问题的。 TDD, 千万不要认为可以无脑 test-driven-development，这里指的是设计容易测试的结构 testability-driven-design。 设置监督层级。类似于操作系统中的进程层级，通常用更重要的监督次重要的 有界一致性。有时候分布式场景下实现数据强一致非常困难，但从用户的视角去观察有时候并需要强一致，这个时候需要做的是根据事务边界对数据和行为进行分组，这个技术又会在 DDD （领域驱动设计）的文献中又详细的讨论。 Part 3本章详细列举了各种反应式系统所需要用到的设计模式 简单组件模式 一个组件只做一件事，并且完整做完（和 unix 编程哲学一样嘛） 错误内核模式 在监督层级中，将重要的应用程序或状态功能存在根部附近，将有风险的操作放到叶子节点 放任崩溃模式 发生异常或者崩溃的时候，优先考虑备份上下文然后重启这个线程、组件、Actor 断路器模式 在失败时间长的时候，断开与用户之间的连接来保护整个服务 主动 - 被动复制模式 保持服务的多个副本运行在不同的位置，但是在任何时刻，只接受对于其中一个位置的修改 多主复制模式 在不同位置上保持服务的多个副本，每处都接受修改，并在各个副本之间传播 主动 - 主动复制模式 在不同地方保持多个副本，所有副本都接受修改操作 资源封装模式 资源以其生命周期都必须由一个组件负责 资源借贷模式 在不转让所有权的情况下，给予客户端对稀缺资源独占的临时访问权 复杂命令模式 向资源发送复合指令以避免过度使用网络 资源池模式 在资源所有者后面隐藏一个弹性的资源池 托管阻塞模式 阻塞资源需要慎重考虑并明确所有权 请求响应模式 消息中包含一个回信地址 **消息自包含模式 每个消息都包含处理请求以及理解其响应需要的全部信息 询问模式 将产生响应的过程委托给专用的临时组件 转发流模式 让信息和消息尽可能地直接流向目的地 聚合器模式 如果需要多个服务响应来计算服务的调用结果，可以专门创建一个临时组件 事务序列模式 将耗时长的分布式事务切换成快速的本地事务，并通过补偿来恢复。换言之：创建一个临时组件用来专门管理分布在多个组件中的一系列动作的执行过程。 可靠投递模式 使用 ack 来确保消息处理掉 拉取模式 消费者向生产者对数据的批量大小提出要求 托管队列模式 管理一个显式的输入队列，并对其填充级别予以反应 丢弃模式 丢弃请求，比不受控制地失败更可取 限流模式 根据与其他服务之间的约束来约定限制自己的输出速率 领域对象模式 将业务领域逻辑与通信，状态管理分离 分片模式 给予各类独一无二并且稳定的对象属性，相应地将大量领域对象进行分组分片，从而水平扩展 事件溯源模式 仅通过应用事件来执行状态变更，并通过将事件存储在日志中来持久化状态变更","categories":[],"tags":[{"name":"读书","slug":"读书","permalink":"https://razertory.me/tags/读书/"}]},{"title":"重建二叉树","slug":"rebuild-tree","date":"2019-07-20T12:29:49.000Z","updated":"2020-03-28T13:02:10.007Z","comments":true,"path":"2019/07/20/rebuild-tree/","link":"","permalink":"https://razertory.me/2019/07/20/rebuild-tree/","excerpt":"","text":"用前序和中序遍历序列构建输入 12pre: [1,2,4,7,3,5,6,8]in: [4,7,2,1,5,3,8,6] 由 pre 得知 root 节点是 1，从而在 in 中找到 1，那么此时这棵树的 root 就是 1。并得出这个二叉树左子树由 [4, 7, 2] 构成，右子树由 [5, 3, 8, 6] 构成。从而确定左子树的 pre 是 [2, 4, 7], in 是 [4, 7, 2]。右子树 pre 是 [3, 5, 6, 8]，in 是 [5, 3, 8, 6]。对左右子树递归地用这个方法进行构建，最终当左右子树都为空的时候递归结束。 找到 in 的 root 之后，根据 root 到 开始节点中间的节点个数可以对应地，确定 pre 的左子树和右子树。 12345678910111213141516171819202122 // https://leetcode.com/problems/construct-binary-tree-from-preorder-and-inorder-traversal class Solution &#123; public TreeNode buildTree(int[] preorder, int[] inorder) &#123; if (preorder == null || inorder == null) return null; Map&lt;Integer, Integer&gt; map = new HashMap&lt;&gt;(); for (int i = 0; i &lt; inorder.length; i++) &#123; map.put(inorder[i], i); &#125; return build(preorder, 0, preorder.length - 1, 0, map); &#125; private TreeNode build(int[] pre, int preStart, int preEnd, int inStart, Map&lt;Integer, Integer&gt; inPos) &#123; if (preStart &gt; preEnd) return null; int rootVal = pre[preStart]; TreeNode root = new TreeNode(rootVal); int rootIndex = inPos.get(rootVal); int leftLen = rootIndex - inStart; root.left = build(pre, preStart + 1, preStart + leftLen, inStart, inPos); root.right = build(pre, preStart + leftLen + 1, preEnd, rootIndex + 1, inPos); return root; &#125;&#125; 用中序和后序遍历序列构建","categories":[],"tags":[]},{"title":"位运算实际问题","slug":"bit-operation","date":"2019-07-18T13:47:13.000Z","updated":"2020-03-28T13:02:10.006Z","comments":true,"path":"2019/07/18/bit-operation/","link":"","permalink":"https://razertory.me/2019/07/18/bit-operation/","excerpt":"","text":"一切要从一瓶有毒的水和一群可怜的老鼠说起 有 1000 个一模一样的瓶子，其中有 999 瓶是普通的水，有一瓶是毒药。任何喝下毒药的生物都会在一星期之后死亡。现在，你只有 10 只小白鼠和一星期的时间，如何检验出哪个瓶子里有毒药？(hint: 有毒的无论怎么稀释都有毒哦) 首先给瓶子编号，默认 1 - 1000，考虑到老鼠喝了水之后就是死或者不死的状态。因为老鼠的个数是有限的，所以肯定是要想办法通过混合的方法。具体怎么混合呢？想象一下，10 只老鼠按照死和不死的状态而言，能够表示出 Pow(2, 10) = 1024 种信息。现在的情况是 1000 个瓶子，如果说用二进制表示就是 1234567891 12 103 114 1105 1116 1110...999 11111001111000 1111101000 最大的 1000 用二进制表示是 1111101000 总计 10 位。如果把 1 到 1000 的每个数都用 n = 1..10 位二进制表示就不难发现只要把第 n 位为 1 的数找出来，混在一起给编号也为 n 的小老鼠喝。那么小老鼠如果死了就说明毒药肯定在第 n 位为 1 的瓶子中。最后再确定一个 10 位二进制数转换为 10 进制就找到了。同理，如果这个题目告诉我们有两瓶毒药就把这 1000 瓶按照两两组合最后得到 C(1000,2) = 499500 个组合。再转换成上一种问题即可。 不用中间数交换两个数是的没错，千万不能写成 123int temp = a;a = b;b = temp; 这样做就大错特错了。所以怎么写呢？当然是位运算! 1234a = a ^ b;b = a ^ b;a = a ^ b;` 案例一个整数数组，里面的数字都出现两次，只有一个数字出现了一次，我们管它叫单身数字，你要写代码找到这个单身数字。我们知道相同数字异或得到 0，0 和任何数字异或都是那个数字。说明只需要把数组中所有数异或即可。 12345public int findSingleNumber(int[] nums) &#123; int result = 0; for (int num: nums) result ^= num; return result; &#125;","categories":[],"tags":[]},{"title":"伪共享","slug":"false-share","date":"2019-07-13T12:41:36.000Z","updated":"2020-03-28T13:02:10.006Z","comments":true,"path":"2019/07/13/false-share/","link":"","permalink":"https://razertory.me/2019/07/13/false-share/","excerpt":"","text":"数组和链表谁快？众所周知数组和链表有个区别在于数组是一段连续的内存空间。那么如果说存放的值一样，长度一样的数组和链表都做简单的遍历谁会更快呢？比如 123456789101112public class CPUCache1 &#123; // 遍历数组 void traverseArray(int[] arr) &#123; for (int i : arr) &#123; &#125; &#125; // 遍历链表 void traverseListNode(ListNode head) &#123; while (head != null) head = head.next; &#125;&#125; 测试代码 12345678910111213141516171819202122232425public class CPUCache1Test &#123; private int quantity = 1024 * 1024 * 10; @Test public void traverseArray() &#123; int[] arr = new int[quantity]; long now = System.currentTimeMillis(); new CPUCache1().traverseArray(arr); System.out.println(System.currentTimeMillis() - now); &#125; @Test public void traverseListNode() &#123; ListNode head = new ListNode(0); ListNode tail = head; for (int i = 1; i &lt; quantity; i++) &#123; tail.next = new ListNode(0); tail = tail.next; &#125; long now = System.currentTimeMillis(); new CPUCache1().traverseListNode(head); System.out.println(System.currentTimeMillis() - now); &#125;&#125; 在我这台的 MBP 上 traverseArray 耗费 5 毫秒，traverseListNode 耗费 26 毫秒。（多次测试稳定在这个数值） CPU 缓存上述的案例中，遍历数组计算机利用到了 CPU 缓存从而加快了速度。 计算机系统中，CPU 缓存，内存，磁盘有着庞大的存取速度差异，所以往往会在这三者的协作之间通过设计缓存来达到更快的速度。 多核心 CPU 中的缓存是分层的，每个核心独有缓存模块，不过 L3 是多核心共享。当计算机收到找寻变量的指令的时候就会按照 L1，L2，L3 ，RAM 这样的顺序寻找。Java 语言中的数组其实就是在一定程度上部分存到了缓存当中。这里产生了一个叫做缓存行的东西，它是 CPU 缓存中的最小单元，占 64 字节（早期的 CPU 是 32 字节）。Java 中的一个 int 占 4 字节，意味着一个缓存行可以存下 16 个 int 类型的变量。在遍历的时候就能利用到从而加快速度。链表就没有这个福利了。 并发场景的 CPU 缓存那么在并发的场景中，又该如何利用 CPU 缓存呢？比如说这段代码 false_sharing.c 在我的 Mac 上打印出了 12341) 8.85049 4.8528e+08 ips2) 22.4329 3.82917e+08 ips3) 30.1988 4.26669e+08 ips4) 35.8548 4.79151e+08 ips 原本认为，在每次增加线程的时候新的线程有着独立的内存空间。这个不假，但是内存中的一些数据会存入 CPU 缓存当中。在多核 CPU 中，每个核都会有自身独立的缓存区。当只有其中一个核的内容发生先变更的时候，由于内核发现内存中的变量和另一个未发生变更的内容不一致，这个时候就会花一定的时间去同步另一个核的内容。这个现象就称为伪共享（false sharing）。维基百科中伪共享的定义有这么一段 False sharing is an inherent artifact of automatically synchronized cache protocols and can also exist in environments such as distributed file systems or databases, but current prevalence is limited to RAM caches. 大意为伪共享目前是自动同步缓存协议中固有的机制。 所以，现代编程语言如何解决这个问题的呢？ 以 Ruby、Python 为代表的动态语言，天生有全局解释锁（GIL）这么个东西，任何一个时刻本质上只有一个线程在跑，也就不需要考虑这个问题。 以 Java，C++，Golang 为代表的静态语言采用补齐（Padding）的方式。比方说：Java 对象头占 8 个字节，如果定义的类所含的变量加起来也低于 64 个字节，那么就声明几个不用的变量让一个对象的实际内存大于 64 字节从而避免通过 CPU 缓存读写。在 Java 8 中，可以直接对一个对象加上 @sun.misc.Contended 并且开启 jvm 选项 -XX:-RestrictContended。","categories":[],"tags":[{"name":"操作系统","slug":"操作系统","permalink":"https://razertory.me/tags/操作系统/"}]},{"title":"分享五个你应该了解的宣言","slug":"5-manifestos","date":"2019-06-19T16:00:00.000Z","updated":"2020-03-28T13:02:10.005Z","comments":true,"path":"2019/06/20/5-manifestos/","link":"","permalink":"https://razertory.me/2019/06/20/5-manifestos/","excerpt":"","text":"宣言是指国家、政府、团体、组织或个人为表明己方的意愿、主旨、主张而向公共发布的书面声明。 GUN 宣言: 软件自由是通往富足世界的一小步。 Mozilla 宣言:互联网是全球公共资源，必须保证开放性和可用性从而建立更好的互联网。 反应式宣言:反应式系统需要具备: 响应性（Responsive），回弹性（Resilien），弹性（Elastic）和 消息驱动（Message Driven）。 敏捷宣言:可工作的软件是进度的首要度量标准并倡导可持续开发。 软件定义交付宣言:代码是指定精确操作的最佳方式。","categories":[],"tags":[{"name":"思考","slug":"思考","permalink":"https://razertory.me/tags/思考/"}]},{"title":"各种层序遍历二叉树","slug":"tree-level-order","date":"2019-06-17T04:48:27.000Z","updated":"2020-03-28T13:02:10.006Z","comments":true,"path":"2019/06/17/tree-level-order/","link":"","permalink":"https://razertory.me/2019/06/17/tree-level-order/","excerpt":"","text":"BFSBFS 我觉得可以从两个方向理解。第一种，对某个树或者图的数据结构而言，就是某个节点相连接的节点都遍历，再不断对刚遍历的节点做同样的操作直到最后遍历完所有节点。第二种，当数据结构是树形的时候这个规律可以衍生为按照层来遍历。按照当前的理解，BFS 一般可以用一个队列来实现，这是因为按照 FIFO 的特性。对于节点相连接的节点都遍历的时候，遍历结束了之后需要从最第一个遍历的节点开始。 输出： 1, 2, 3, 4, 5, 6, 7, 8 1234567891011121314151617public class Tree &#123; private ArrayList&lt;Integer&gt; order = new ArrayList&lt;&gt;(); // T:O(n) T:O(n) ArrayList bfsSearch(TreeNode root) &#123; Queue&lt;TreeNode&gt; queue = new LinkedList&lt;&gt;(); queue.offer(root); while(!queue.isEmpty()) &#123; TreeNode node = queue.poll(); if (node != null) &#123; order.add(node.value); queue.offer(node.left); queue.offer(node.right); &#125; &#125; return order; &#125;&#125; 层序遍历二叉树在 BFS 的基础上，期待上面的遍历结果能已分层的形式输出：1 2 34 5 67 8这个问题在《编程之美》和《剑指 Offer》上面都出现过。 其实只要模拟一下队列的出入过程就能发现，BFS 过程中维护的队列总是在不断的 offer 和 poll。当第 1 层的 #1 poll 的时候，第 2 层的 #2，#3 offer…, 第 2 层 #2 poll 的时候，第 3 层的 #4, #5 offer，第二层的 #3 poll 的时候。.. 也就是说，如果能知道第二层层结束了那么就有办法了。 回过头再看，第一层的 #1 两个子节点 offer 之后。队列的 size 变成了 2。如果维持一个计数器表示当计数器的值从在 [0, size] 之间的时候表示第二层正在 poll, 第三层正在 offer，那么就能表示层级的变化。 1234567891011121314151617181920212223public class Solution &#123; // T:O(n) T:O(n) ArrayList&lt;ArrayList&lt;Integer&gt;&gt; Print(TreeNode pRoot) &#123; ArrayList&lt;ArrayList&lt;Integer&gt;&gt; ret = new ArrayList&lt;&gt;(); if (pRoot == null) return ret; Queue&lt;TreeNode&gt; queue = new LinkedList&lt;&gt;(); queue.offer(pRoot); while(!queue.isEmpty()) &#123; ArrayList&lt;Integer&gt; thisLevel = new ArrayList&lt;&gt;(); int low = 0, high = queue.size(); for (int i = low; i &lt; high; i++) &#123; TreeNode node = queue.poll(); thisLevel.add(node.val); if (node.left != null) queue.offer(node.left); if (node.right != null) queue.offer(node.right); &#125; ret.add(thisLevel); &#125; return ret; &#125;&#125;","categories":[],"tags":[]},{"title":"技术人的刻意练习","slug":"deliberate-practice-of-tech","date":"2019-06-07T01:02:14.000Z","updated":"2020-03-28T13:02:10.004Z","comments":true,"path":"2019/06/07/deliberate-practice-of-tech/","link":"","permalink":"https://razertory.me/2019/06/07/deliberate-practice-of-tech/","excerpt":"","text":"刻意练习这个词是我很早之前在知乎上看到的，是我一直忽略的东西。 从上大学起我就在想程序员是一个什么样的职业。我拿去和很多职业做了对比 匠人，比如说那种铸剑师。开凿矿岩，冶炼铁矿，制作胚子，最后冷萃，打磨抛光 武士。练习体力，意志力，心理素质，战斗技巧，不断和人战斗 音乐家，比如说吉他手。学习音乐基础，指法，识谱，演奏 在我心里，做这些的工作的人们应该是对自己的事情怀有自我驱动的意愿便会十年如一日地做下去。我觉得做程序员也应该是这样的。做一行喜欢一行。 前不久我在一个 OJ 上又看到了这个词。这让我重新找到了之前看过的那篇文章并开始重新思考自己关于技术的理解。 刻意练习意味着对于你所关注的事情的态度像是我上面提到的几种职业人对于自己工作所持有的态度 强烈的动机 持续的练习 动机，比如：喜欢一个事物本身（我喜欢吉他的声音所以练习吉他），喜欢一个事物带来的反馈，比如技术上的提升让做技术人的职业生涯走得更高，获得更多的利益。不论出于什么样的动机，甚至是很对动机组合在一起的复合的动机，最终带给人的是 motivation 相对于动机的浅显易懂，持续的练习成为了阻碍大部分人的屏障。 因为天赋，精神状态，练习数量，练习是否科学这些都是决定一个人走是否能向更深层次的因素。 天赋因人而异，但因为他是相对的并且是低概率的所以在一些非极端的场景下都不会凸显出来。一些极端场景比如各类竞技，LOL 游戏中的 faker，游泳比赛中的菲尔普斯，NBA 里面的各种怪兽… 他们都为自己的工作付出了大量科学的练习。以至于和对手之间的比试很大程度上的确是在—拼天赋。 而对于绝大多数人，我认为更重要的是要拥有良好的精神状态，加上科学的练习方式以及持续的练习会是高概率走向领域深层次的必要条件。 良好的精神状态来源于对自己身体和心理健康的管理。虽然这是浅显易懂的道理，可是依旧有很多人输在了这一层。他们往往无法专注，时不时身体出现异常，总是被生理上的问题困扰。 足够的练习数量是在数字上衡量一个人技艺水平的方式，这很简单也很直觉。比如说我们生病了喜欢找看起来年龄较大富有经验的医生，高级程序员的招聘 JD 上总是习惯性的出现要求 3 - 5 年或者更多。 但我想说的是，很多无意义的付出的确是没有回报的。 科学的练习方法是技术人一定要学会掌握的技能。如果你是身边有一群更优秀的人，那么你的行为容易得到纠正式的反馈，同时观察到他人的练习方法自身也可以学到更多。如果没有这样的条件，那么也或许可以根据我总结的下面四点找到属于自己最科学的练习方法。 设定精确的目标计划 短期内的，比如一周或者两周将要做的事情。这些事情一定是经过了仔细评估的 保持专注 这是高效练习的原始动力 有效反馈 能积极快速认识到自己这样练习方式是否正确 在舒适区面前保持理性 不断练习不断产生成就感，不断割舍掉成就感往下一个目标走去 以上，便是我认为的技术人的刻意练习。","categories":[],"tags":[{"name":"思考","slug":"思考","permalink":"https://razertory.me/tags/思考/"}]},{"title":"MySQL Top Tips","slug":"100-tips-of-mysql","date":"2019-05-31T05:37:02.000Z","updated":"2020-03-28T13:02:10.004Z","comments":true,"path":"2019/05/31/100-tips-of-mysql/","link":"","permalink":"https://razertory.me/2019/05/31/100-tips-of-mysql/","excerpt":"","text":"MySQL 通过 /mysql/bin/mysqld 启动 uft8 作为 unicode 的子集，在 MySQL 的世界里就是 uft8mb3，当然 emoji 需要 uft8mb4 MySQL 会缓存查询，不过不会缓存一些系统函数以及系统表 （比如：mysql 、information_schema、 performance_schema) 的查询结果 MySQL 5.7.20 开始不再推荐使用缓存，并在 MySQL 8.0 删除 如果你发现很难权衡用什么存储引擎，或者对存储引擎没有太多了解，那么直接用默认的 InnoDB 吧 InnoDB 在 MySQL 5.6 以及以后的版本中，大多数的 DDL 语句都不会锁表了。相对的，如果用的之前的版本，都是会锁的 InnoDB 是行存储的 （默认 Compact 行格式），存下的是每一行的原始数据和额外信息，比如：VARCHAR 这种变长字段实际用的长度，null 值列表（如果没有 null 值也就没有这个列表），trx_id 事务 id InnoDB 默认会给主键创建一个 B+ 树作为聚簇索引，其中，聚簇索引的叶子节点包含了所有的用户数据 InnoDB 对非主键创建的 B+ 树索引的叶子节点包含主键和索引列，这种索引称为二级索引。其中一个二级索引只有一个 B+ 树，即使是联合索引 InnoDB 当 where 条件只为一个二级索引列的时候，会在索引叶子节点找到主键，然后通过主键的聚簇索引找到真实记录，这个过程称为回表 InnoDB 对非主键的联合索引采取按照联合索引前边的列排序，如果命中，则按照这个顺序往后。比如：针对一个表 user_posts (id, user_id, post_id) 的联合索引 index_on_user_id_post_id。这里我们称为前缀匹配规则 更多详情见 官方文档 12345678#有效select * from user_posts where user_id = 343 and post_id = 121#有效select * from user_posts where user_id = 343#无效select * from user_posts where post_id = 121#无效select * from user_posts where user_id = 343 or post_id = 121 InnoDB 创建的联合索引时，尽量让区分度高的列在左边 在业务上有唯一保证的列或者联合列时，一定用唯一索引，杜绝脏数据的产生 一个表上索引建的越多，就会占用越多的存储空间，在增删改记录的时候性能就越差 InnoDB 对于含有索引列的模糊匹配，只会在前缀确认的情况下才会生效 InnoDB 对于 order by 含有索引列的查询是能够通过索引排序的，不过这不包含：1. 混用 ASC 与 DESC 的时候 2. 含有非索引列的时候 3. 联合索引顺序不遵循则前缀匹配规则的 InnoDB 对于一个使用到索引的搜索条件和没有使用该索引的搜索条件使用 OR 连接起来后是无法使用该索引的 InnoDB 对于含有多个引列，并且每个列都有二级索引的查询，一般的做法是不断通过二级索引找到 B+ 树的叶子节点之后进行多次回表操作。某些特殊情况下会先合并索引，只进行一次回表操提升性能。不过，既然都用到多个列了，那么直接用联合索引就好了 MySQL inner join 的 on 语句，本质上和 where 是一样的。也就是说可以把一个 inner join 换成 where 的写法 MySQL 的 join 使用的是 嵌套循环连接 算法，在这个算法中，被 join 的表 （被驱动表） 如果数目大且没有索引，性能会急剧下降。这种情况如果实在无法避免，可以试着把 join buffer 参数调大，同时避免使用 select * 使不必要的数据占用空间 尽量让所有的查询都能做到 explain 下来至少 range，最好是 ref 甚至 consts InnoDB 对任何一条记录进行写的改动的时候，会同时把当前的 trx_id 也写入 MySQL 默认开启 REPEATABLE READ 事务隔离级别，这种方式原本是会有幻读问题的。快照读通过 undo log 解决，当前读通过 gap lock 解决。 MySQL MVCC 中读操作分为快照读和当前读 快照读：读到的是已经提交了的事务。比如 select .. where 当前读：读到的永远是最新的内容，比如有一个事务修改了一个记录且没有提交。比如 update .. where RC 和 RR 最本质的区别在于生成 ReadView 的时机不同","categories":[],"tags":[{"name":"基础组件","slug":"基础组件","permalink":"https://razertory.me/tags/基础组件/"}]},{"title":"「Linux 内核」」代码分析 select, poll, epoll","slug":"async_io_epoll_poll_select","date":"2019-05-09T13:28:23.000Z","updated":"2020-04-12T08:06:28.183Z","comments":true,"path":"2019/05/09/async_io_epoll_poll_select/","link":"","permalink":"https://razertory.me/2019/05/09/async_io_epoll_poll_select/","excerpt":"","text":"现在有很多讲这三个 system call 的文章，这里我从代码层面去分析和理解。 首先，他们分别来自于 &lt;sys/select.h&gt;，&lt;sys/poll.h&gt;，&lt;sys/epoll.h&gt;。如果是 Mac OS，是没有第三个的，其中前两个都在 /usr/include/sys。 当一个客户端请求服务端的时候，服务端会调用 accept() 产生一个 socket，这个 socket 相当于一个状态机，最基本的包括是否可读，是否可写，服务端和客户端在进行数据传输的过程中，这个 socket 的状态就会不断发生变化。这只是一个客户端的情况，实际上肯定是有很多的。也就是说，服务端需要同时控制许多的 socket 的读写。并且总是需要以最快的时间，最小的系统开销来向 socket 读或者写数据。庆幸的是，这些事情都由开发操作系统内核的工程师们搞定了。 fd: 一个文件描述符; fds: 一组文件描述符 select fd_set ：&lt;sys/select.h&gt; 提供的文件描述符集合，是一个能存放最多 1024 个元素的数组。 select 需要传入 fd_set 的地址，然后将它们修改成只包含就绪并用来读写。这个方法的签名为 123// 按照顺序，参数的意思分别是// fd_set 的 最大编号，准备就绪读的，准备就绪写的，异常的，以及超时时间int select(int nfds, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout); 简单点，比如说你有 fds，编号 1 到 5 并且只想让内核告诉你哪些 fd 可读。那么只需要调用 select(5, fds, NULL, NULL, NULL) 就好了。服务端会不断地循环调用这个方法来做数据的读写。 按照 select 的做法，假设最大为 600 的五个 fd 传入，那么 select 会从 0 开始遍历到 600，这样做会浪费掉大量的 CPU 资源。 pollpoll 与 select 相比较，最大的区别在于不再使用 fd_set 这样的数据结构。而是为每一个 fd 都封装了一个 pollfd 12345struct pollfd &#123; int fd; // 对应的 fd short events; short revents;&#125;; 只需要给 poll() 方法传入已经打开的 fds，方法签名为 1int poll (struct pollfd *fds, unsigned int nfds, int timeout); 在上述 select 的案例中，同样最大 600 的五个 fd 传入，poll 只需要传入这个 5 个 fds 即可。在调用了 poll() 之后， 就绪的 pollfd 中的 revents 就会被修改，也就能确认哪些 fds 就绪。 epollepoll 包含了多个方法，使用起来实际上会分成几个步骤 初始化 epoll 事件驱动所用到的数据结构 epoll_event epoll_create() 初始化当前的 context 初始化 epoll_event 中的 data.fd 关联到当前系统的 fds 依照 epoll_event 的内容，调用 epoll_ctl() 来对当前的 context 进行控制，通常是给 context 写入或者删除 fds epoll_wait() 只返回就绪的 fds 由于没有无效的遍历，epoll 的理论时间复杂度是 O(1)。 select 和 poll 的复杂度是 O(n)。 源码 参考https://devarea.com/linux-io-multiplexing-select-vs-poll-vs-epoll/#.XNOXutMzYQE https://jvns.ca/blog/2017/06/03/async-io-on-linux--select--poll--and-epoll/ 《Linux/Unix 系统编程手册》","categories":[],"tags":[{"name":"操作系统","slug":"操作系统","permalink":"https://razertory.me/tags/操作系统/"}]},{"title":"汉诺塔问题","slug":"hanoi-tower","date":"2019-03-15T12:44:06.000Z","updated":"2020-03-28T13:02:10.005Z","comments":true,"path":"2019/03/15/hanoi-tower/","link":"","permalink":"https://razertory.me/2019/03/15/hanoi-tower/","excerpt":"","text":"描述一下问题，假设有 A，B，C 三个柱子。A 柱子上有一定数目从高到低是小到大顺序的盘子叠起来，每次可以移动一个柱子上的 1 个盘子。并且任何时候小盘子永远在大盘子上。现在的问题是如何把 A 的 N 个盘子移动到 C。在线游戏 1.N = 1 ，A 移动到 C 就好。 2.N = 2，先移动第一个到 B，再把第二个移动到 C，再把 B 的移动到 C 就好。 3.N = 3 … 也许刚拿到的时候会不断尝试。仔细分析 2 的最后一步，会发现这个时候 A 没有盘子，B 有一个小的，C 有一个大的。如果把这个情况引入到 3，就是 A 没有盘子，C 有一个最大的，B 有两个较小的，把 B 的两个移动到 C 就行了。如何把 B 的两个移动到 C 参看 2 的解答即可。同样，如何把两个较小的盘子移动到 B 呢，也可以参看 2 的解答。 那么当 N 更大的时候呢？ 这个时候肯定是需要寻找规律的。从 N = 3 的情况可以看出似乎 2 的解法可以用在了 3 上，并且可以发现 N 个盘子在移动到 C 之前一定是要让 N - 1 个盘子落在 B 并且第 1 大的落在 C，并且， 让 N - 1 落在 B 就一定要让 N - 2 落在 C，并且， 让 N - 2 落在 C 就一定要让 N - 3 落在 B，并且， 让 N - 3 落在 B 就一定要让 N - 4 落在 C，并且， … 最后的结论就是探讨第 N 大的盘子落在 B 还是 C 上。如果是从数学的角度看，只要通过奇偶来判断是往 B 还是往 C 即可。那么如何用计算机来解答呢？ 假设函数签名是 hanoi(n, a, b, c)。前三个输入以及输出分别为 12345678910111213141516&gt; hanoi(1, \"A\", \"B\", \"C\")A —-&gt; C&gt; hanoi(2, \"A\", \"B\", \"C\")A —-&gt; BA —-&gt; CB —-&gt; C&gt; hanoi(3, \"A\", \"B\", \"C\")A —-&gt; CA —-&gt; BC —-&gt; BA —-&gt; CB —-&gt; AB —-&gt; CA —-&gt; C 从最简单的情况可以写出 1234567def hanoi(n, a, b, c) if n == 1 puts \"#&#123;a&#125; --&gt; #&#123;c&#125;\" else # 所以当 n &gt; 1 的时候呢？ endend 回顾上面的推导不难发现实际上如果要把 N 个盘子从 A 移动到 C。需要做的就是三步 移动 A 的 N - 1 个到 B 移动第 A 的 N 个 到 C 移动 B 的 N - 1 个到 C 这其中 A，B，C 柱子为了便于理解可以认为是起始柱，辅助柱和目标柱。这也函数 hanoi(n, a, b, c) 中后三个参数的意思就是。也就是说，再深入理解为 移动起始柱的 N - 1 个到辅助柱 移动起始柱的 N 个 到目标柱 移动辅助柱的 N - 1 个到目标柱 完善一下代码就是 123456789def hanoi(n, a, b, c) if n == 1 puts \"#&#123;a&#125; --&gt; #&#123;c&#125;\" else hanoi(n - 1, a, c, b) hanoi(1, a, b, c) hanoi(n - 1, b, a, c) endend","categories":[],"tags":[]},{"title":"N 皇后问题","slug":"n-queens-puzzle","date":"2019-03-09T01:23:09.000Z","updated":"2020-03-28T13:02:10.004Z","comments":true,"path":"2019/03/09/n-queens-puzzle/","link":"","permalink":"https://razertory.me/2019/03/09/n-queens-puzzle/","excerpt":"","text":"N 皇后问题是最早一位棋手提出了八皇后问题之后，推演而来的。八皇后问题的规则是： 在 8 * 8 的盘中放置 8 个皇后 任何一个皇后的横，竖，斜对角都不能别的皇后 我们要从中找到尽可能多的摆放方法。下图是解法之一 八皇后问题 随后，这个问题被推演成为了 N 皇后问题。也就是当棋盘大小为 N（N &gt; 0），皇后个数为 N 的时候，有多少种摆放方法。目前前 10 个分别是 1234567N =&gt; [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]U =&gt; [1, 0, 0, 1, 2, 1, 6, 12, 46, 92]D =&gt; [1, 0, 0, 2, 10, 4, 40, 92, 352, 724]# N 棋盘大小# U 不包括对称的解# D 包括了对称的解 注意到六皇后问题的解的个数比五皇后问题的解的个数要少。现在还没有已知公式可以对 n 计算 n 皇后问题的解的个数，也就是说这个问题现在还只能由程序来解答。 核心的思路是：给棋盘定义坐标，比如左上是（0，0）横轴为 x 竖轴 为 y。如果抽象成为一个二维数组，x 和 y 就是数组元素的 index。当一个皇后落子之后，那么相对应的所有 x 和 y 以及对角线就都不能下这里被称为控制区域。也就是当一个皇后（坐标为 x，y）落子之后那么棋盘的 colomns[x] 和 rows[y] 以及对角线就都被控制。 这里最先想到的是每个位置遍历，总共记录 n^2 个位置。不过这一步可以精简为记录历行，因为我们知道一行如果落子了就不在考虑这一行。也就是说如果从 0 开始往棋盘的下方遍历，那么落子之后只需要记录这一列是否下过即可。用 colomns[x] 来表示这一列。 对于对角线，一个 8 * 8 的棋盘中对角线的个数是 15 个 / + 15 个 \\。这个很容易推算，对于一个标准的 n * n 棋盘，单个方向的对角线个数 diag = 2 * n - 1 也即是说对角线是否被控制可以用对角线的数组表示。假设 / 是 diag1， \\ 是 diag2。 那么又有一个问题了，一个落子所控制的对角线能否被找到呢。比如说落在 (x, y) 处在，那么是否能找到对应的 diag1 和 diag2 呢？答案是可以的。 至于如何推算的，画一下图就知道啦。 对于 /，对应的是 diag1[x + y] 对于 \\，对应的是 diag1[x - y + n - 1] 至此，落子之后控制区域如何表示就没有问题了。 接下来就是探究怎么找到所有的落子。本质上这是一个典型的回溯法。 作为回溯算法的一种，自然是用树形空间来表示递归。例如当 n = 4 的时候，搜索空间可以用一颗 4 叉树来表示 首先从 0 开始遍历棋盘的 x 轴。并在此基础上，递归地寻找后面的位置。递归期间可以落子，那么就放入棋盘，同时更新控制的区域，继续递归。当发现递归到树的底层，或者说后面的节点全都不能遍历的时候回溯。反之，就继续遍历。 这种的典型的多插树搜索很大程度基本的递归结构是 1234# 伪代码def recur() for i in range recur() 树的节点个数通常是 range 的大小， recur 的深度是树的深度。 那么至此可以开始设计这个 leetcode 上难度为 hard 的题目 。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public class Solution &#123; private List&lt;String&gt; board; private boolean[] cols; private boolean[] diag1; private boolean[] diag2; private List&lt;List&lt;String&gt;&gt; solution; public List&lt;List&lt;String&gt;&gt; solveNQueens(int n) &#123; solution = new ArrayList&lt;&gt;(); board = new ArrayList&lt;&gt;(); char[] charArray = new char[n]; Arrays.fill(charArray, '.'); for (int i = 0; i &lt; n; i++) &#123; board.add(new String(charArray)); &#125; cols = new boolean[n]; diag1 = new boolean[2 * n - 1]; diag2 = new boolean[2 * n - 1]; nQueens(n, 0); return solution; &#125; private void nQueens(int n, int y) &#123; if (y == n) &#123; solution.add(new ArrayList&lt;&gt;(board)); return; &#125; for (int x = 0; x &lt; n; x++) &#123; if (!available(x, y, n)) continue; updateBoard(x, y, n, true); nQueens(n, y + 1); updateBoard(x, y, n, false); &#125; &#125; private boolean available(int x, int y, int n) &#123; return !cols[x] &amp;&amp; !diag1[x + y] &amp;&amp; !diag2[x - y + n - 1]; &#125; private void updateBoard(int x, int y, int n, boolean put) &#123; cols[x] = put; diag1[x + y] = put; diag2[x - y + n - 1] = put; char[] columns = board.get(y).toCharArray(); columns[x] = put ? 'Q' : '.'; board.set(y, new String(columns)); &#125;&#125; 附: 完整的代码&amp;测试用例","categories":[],"tags":[]},{"title":"无重复数组的全排列","slug":"permutation-and-combination","date":"2019-03-07T08:49:40.000Z","updated":"2020-03-28T13:02:10.004Z","comments":true,"path":"2019/03/07/permutation-and-combination/","link":"","permalink":"https://razertory.me/2019/03/07/permutation-and-combination/","excerpt":"","text":"排列组合排列 permutation（arrangement） 和组合 combination 纯粹的数学上的排列组合的定义是 排列 就是指从给定 n 个数的元素中取出指定 r 个数的元素，进行排序 组合 从给定 n 个数的元素中仅仅取出指定 r 个数的元素，不考虑排序 排列与组合唯一的区别在于操作元素的顺序是否影响了统计的结果。 数学中常用到的排列组合的公式主要用来统计目标个数。 回溯法在一些常见的算法问题中，有时候是需要输出期望的子集合。这个时候就需要采用回溯的方式来解决。 维基百科的定义是 回溯法（英语：backtracking）是暴力搜索法中的一种。对于某些计算问题而言，回溯法是一种可以找出所有（或一部分）解的一般性算法，尤其适用于约束满足问题（在解决约束满足问题时，我们逐步构造更多的候选解，并且在确定某一部分候选解不可能补全成正确解之后放弃继续搜索这个部分候选解本身及其可以拓展出的子候选解，转而测试其他的部分候选解） 给定一个没有重复数字的序列，返回其所有可能的全排列（原题）。如 123456789101112Input: nums = [1,2,3]Output:[ [3], [1], [2], [1,2,3], [1,3], [2,3], [1,2], []] 在有 3 个元素的序列中，子序列的个数可以分别为 0，1，2，3 四种。本质上回溯法是 DFS，这个问题也就演变成了一个遍历树的思路。 12345678 void backtrack(List&lt;List&lt;Integer&gt;&gt; list , List&lt;Integer &gt; tempList, int [] nums, int start)&#123; list.add(new ArrayList&lt;&gt;(tempList)); for(int i = start; i &lt; nums.length; i++)&#123; tempList.add(nums[i]); backtrack(list, tempList, nums, i + 1); tempList.remove(tempList.size() - 1); // 注意回溯的时候需要删掉最后一个，保证用下一个元素取替换并继续递归 &#125;&#125; 完整代码123456789101112131415public List&lt;List&lt;Integer&gt;&gt; subsets(int[] nums) &#123; List&lt;List&lt;Integer&gt;&gt; list = new ArrayList(); Arrays.sort(nums); backTrack(list, new ArrayList(), nums, 0); return list; &#125;private void backTrack(List&lt;List&lt;Integer&gt;&gt; list, List&lt;Integer&gt; tempList, int[] nums, int start) &#123; list.add(new ArrayList(tempList)); for (int i = start; i &lt; nums.length; i++) &#123; tempList.add(nums[i]); backTrack(list, tempList, nums, i + 1); tempList.remove(tempList.size() - 1); &#125;&#125;","categories":[],"tags":[]},{"title":"「读书」七周七语言","slug":"seven-weeks-seven-languages","date":"2019-03-01T12:57:15.000Z","updated":"2020-03-28T13:02:10.004Z","comments":true,"path":"2019/03/01/seven-weeks-seven-languages/","link":"","permalink":"https://razertory.me/2019/03/01/seven-weeks-seven-languages/","excerpt":"","text":"程序员对于编程语言的争论，是一场永远都不会停止的战争。但社会在进步，科技在进步，那么就让大家继续吵下去吧 :) 这是一本讲述七种不同类型编程语言：Ruby，Io，Prolog，Scala，Erlang Clorjure 和 Haskell 代表的现代编程语言的重要特性和编程范式的书。这里面的语言多多少少被大家听过或者用过。他们分别作为某个领域的代表而汇集到一本书里。 Ruby这门语言是我目前工作中的主语言，包括对应的生态。语法简单但又灵活，超强的 DSL 能力和丰富的社区资源给我的工作提供了巨大的便捷和效率。 Ruby 或者 Ruby on Rails 是一种在开发过程当中，程序员能够按照自然语言的思维并且很容易写出符合预期逻辑的程序。这让项目的迭代速度不会打折扣。 当数据量变大之后，它的内存管理和执行效率的劣势就慢慢展露出来。随之而来的还有” 过于灵活 “的语法特性导致了一旦 Code Review 没有做到位并且代码风格与团队成员差异巨大的情况出现时，维护项目阅读代码就变得很糟糕。我们曾经常出现服务器内存不足的报警消息，也偶尔因为难以维护部分旧有的糟糕代码而不得不进行重构。可以说 —— 动态语言一时爽，重构现场火葬场（逃 Io这一门原型语言。什么是原型语言？ 在传统的面向对象语言中，比如 Java、Ruby、C++、Python，我们通常需要定义一个称为 class 的东西，然后再基于这个 class 创建对象。而 Io 这门语言是只操作对象的，简单说就是 Io 有一个叫做根对象 Object 的东西，我们需要定义一个对象直接 Car := Object clone 即可。在 Io 这本语言的上下文中，一切都是由对象和消息组成。这一点很像 Ruby 中的元编程，甚至我个人会认为元编程学习了 Io。同样，这门基于消息的编程语言在实现 actor 作为并发模型就变得轻而易举。我们会在 Scala、Erlang 和 Haskell 以及今后众多流行编程语言中再见到 actor。Io 的语法很简单，几乎没有可以被叫做语法糖的东西，甚至关键字也少的可怜。这门语言让我想到了当今国内算是火热的 Go 语言，简单高效却给人强大的印象。 Prolog这是一门逻辑编程语言。和 SQL 一样，Prolog 基 于数据库，但是其数据由逻辑规则和关系组成; 和 SQL 一样，Prolog 包含两个部分: 一部分用于 描述数据，而另一部分则用于查询数据。在 Prolog 中，数据以逻辑规则的形式存在。这个是很有意思的一件事情，如果读者看过 伊曼努尔 · 康德 所著的《纯粹理性批判》会更加体会到这门语言的编程哲学。比如说，在 Prolog 中，数据的标准存放格式如下： 「事实 」关于真实世界的基本断言（对没错，就是你测试代码中的 assert）—— Babe 是一头猪，猪喜欢玩泥巴 「规则」真实世界的推论（if then）—— 如果你是一头猪，那么你喜欢玩泥巴 「查询」向真实世界寻求一个问题（布尔表达式）—— Babe 喜欢玩泥巴吗？ 按照大多数语言通常的 Coding Pattern，应该是这样 收集和整理逻辑 用程序表达逻辑 找出所有可能的解决方法 通过程序验证这些可能的解决方法 这是标准的做法，给大家清晰的脉络，并且也容易维护。 然而，Prolog 会给人一种神乎其技的感觉。对的，大多数语言写代码都是 how 需要我们自主阐述逻辑，然而 Prolog 是一种 what，当我们人为定义好问题之后，让计算机来解决。书中的「地图着色问题」、「数独问题」有详细解释。 曾几何时，基于规则的 AI 和 NLP 领域让这门语言大放异彩。随着后来深度学习，基于统计算法的兴起，以前这种，呵呵。。 Scala是一把名贵的瑞士军刀，因为它什么都有。运行在宇宙最强虚拟机 JVM 上，融合了面向对象和函数式编程。语法给我的感觉是吸收了 Java/Ruby/Haskell。最近刚好在写 Scala 还有学 Akka，后期会专门补充。 Erlang回到一个原始的原则，在技术选型上我始终认为工程师需要考虑所用技术的三个指标：开发效率，执行效率和可维护性。在 Erlang 中，以下的特质都给予了 Erlang 不错的上述指标 pattern matching 比如说有两个 kv 数据结构，如何最方便比较结构是否一致。很大程度上避免大量的 if-else 函数 通过 export 方式让函数给外部模块调用，加上 pattern matching 可以非常方便写测试代码 消息传递 不同的 erlang 进程可以通过消息传递的方式共享变量 热更新 修改了代码可以不重启服务器生效 parse_transform 通过发送和接受 AST 来使用模块 spawn 一个 erlang 进程只是在 erlang 虚拟机中的进程并且只占用极少的操作系统资源，高并发的情况下表现优异。 如今在天朝南部沿海某城市，很多基于 OTP 框架的游戏公司依然是 Erlang 的忠实用户。 Clorjure如果有看过 SICP 那本书，就一定知道 Lisp 的威力，当然有不少大 V 都写过赞誉它的文章比如：阮一峰博客中的译文「为什么 Lisp 语言如此先进？」 http://www.ruanyifeng.com/blog/2010/10/why_lisp_is_superior.html 还有陈皓的「Lisp 的永恒之道」https://coolshell.cn/articles/7526.html 都非常认可 Lisp 的很多理念以及这门语言带给现代编程界的影响。 所以，当这门语言跑到了 JVM 上之后，又会产生什么样的效果呢？ 工程化的环境。由于 JVM 平台的的推力，clorjure 朝着一种工业化的通用的方向发展并且简化了 Lisp 中的括号写法。在 Java 中常常会有设计接口来做工程化的抽象，clorjure 中有「协议」的概念。如果需要调用 Java 自带的方法，比如 Object.toString()，可以直接这样 Object(toString [this] (str “[“ (your object) “]”))就可以直接调用了。这相当于扩展了 Java（附带了 Lisp 的函数式和宏），也可以认为扩展了 Lisp（结合了面向对象）。所以我决定学 Scala，😂 clorjure 中还有原子和代理来保证并发安全，原子和大多数语言的 atom 类似，代理是说当某个对象被代理之后，使用者可以异步修改代理对象的值，更新发生在另外的线程，并且每次只有一个函数可以修改代理的状态。 Haskell我认为 Haskell 一定是高纯度的函数式语言。 模式匹配拒绝大量的 if-else，尾递归优化让递归更高效，元组则让函数写得更加简洁且富有表达力 🕶️。一个列表被定义为 head 和 tail 两个部分，那么列表的定义就是 head 和 tail 的模式匹配，特别是列表推导编程，严谨而且聪明。惰性求值，高阶函数，monad（一个自函子范畴上的幺半群），柯里化… Haskell 在作者的观点里是本书中最难学的语言，但也是最引人深思和启发的语言。在笔者与 Scala 前辈的交流中听到里许多关于 Haskell 的赞誉。 回到一个偏哲学意义的问题：如果编程是在给输入的问题找到一个解，那么，是否存在任何情况下只要我们输入出问题的定义，就能让计算机返回解的场景？ 希望你也喜欢这本书。😊","categories":[],"tags":[{"name":"读书","slug":"读书","permalink":"https://razertory.me/tags/读书/"}]},{"title":"「读书」数据密集型应用系统设计","slug":"ddia","date":"2019-02-01T13:26:55.000Z","updated":"2020-03-28T13:02:10.003Z","comments":true,"path":"2019/02/01/ddia/","link":"","permalink":"https://razertory.me/2019/02/01/ddia/","excerpt":"","text":"这是一本讲述现代互联网应用中，开发者们如何进行正确的存储和处理数据的书。 当今的互联网应用大多是数据密集型的。与之相对的是 CPU 密集型。 什么是数据密集型？ 当数据的数量，复杂度和数据的更新速度容易成为一个产品在用户增长，功能迭代，用户体验度提升的瓶颈的时候，我们就认为这种应用是数据密集型应用。上述的问题往往会给现在的工程师带来各种挑战，比如：数据库 CPU 报警，应用服务器内存报警，开发迭代缓慢，API 接口响应过长等等。 一个互联网应用的系统，绝大多数情况下，都需要我们满足稳定性，可扩展性和可维护性。例如：当部分硬件故障或者有一些人为 bug 的时候，系统各项指标能保持在预期之上，用户／数据量的增长的时候，工程师短时间内就能良好应对，新人的到来的时候能够快速理解系统并加入到开发。 为了让系统朝着一个正确的方向演进，这本书 《Design Data Intensive Applications》 第一章讲述了数据的模型选用，存储与检索的以及序列化／反序列化。我觉得第一章给予读者的，刚好是告诉一个项目启动的时候，知道如何做数据模型的选型，包括数据库，缓存，查询方式的选择等，避免给中后期留下过多的坑。第二章非常直白地说明了分布式存储的应用方式和场景，主要考虑的是容错，容灾难，分布式事务和分布式一致性等。这几点个人非常感兴趣。第三章主要讲大数据的处理，比如 MapReduce，消息队列以及作者对未来系统架构的构想，包括：Lambda 架构与函数式编程将会大放异彩，unix 哲学将与数据库哲学相庭抗礼，分布式存储会变得普遍。 一块硬盘的平均无故障时间是 10 到 50 年，因此从数学期望上讲，在拥有 10_000 个硬盘的存储集群上，平均每天会有 1 个硬盘发生故障。有经验的运维会选用批次号不同的硬盘来避免同一时间大范围的物理故障。对于工程师而言，出现 bug 的概率或多或少。一旦项目庞大，生产环境，出 bug 的数学期望同样很容易达到一个比较高的情况。当然，这里就不赘述。 我们需要认可的一件事是，东西早晚会坏掉的。作者认为：没有林丹妙药可以拯救上述的问题。我们应当接受，在设计系统的时候，做出良好的抽象，解耦合，充分的测试，明确详细的监控和随时准备一个模拟生产环境的沙盒子。只有不断的把每个细节都做好，从整体到局部，系统才会朝着健康的方向发展。 项目初期的时候，数据模型的选取通常因为项目的复杂度并不算高而变得很含糊。弄清楚一些要点，会让工作顺利许多。数据的查询通常依赖两种方式，索引或者搜索引擎。如今的哈希索引，B/B+ 树，LSM 树在工业场景下都有非常厉害的实现，比如 InnoDB， MongoDB。OLAP 场景和 OLTP 场景在某些情况下很难做到同时满足，需要工程师去做 trade off。考虑到可扩展和可维护，需要把应用服务器与存储服务器之间做好细致的工作划分。建立秩序，省却搜索——德国谚语 数据的传输往往离不开网络协议的数据的序列化 / 反序列化。当今在大多数 REST 服务中，JSON 占据着主导地位，这与前端工程师的付出密切相关。也是因为微服务的流行，当下的系统中，某个服务端节点很有可能是别的服务端的客户端。当然，除了 JSON，各种二进制协议比如 Protocol Buffers 有着更好的性能与不错的可维护性。但问题是这些协议在数据可读之前需要解码。 如今数据的检索和存储已经可以有多台机器同时参与，在这里姑且称之为分布式数据存储。它可以带来更大的负载能力，比如一些 OLAP 场景对数据库 CPU 有极大的消耗，这个时候可以做横向扩展。同时分布式存储可以带来更优质的容错能力。我很喜欢 replication 这个单词，正如之前提到的，当某个硬盘故障了，我们又需要对这硬盘数据进行读写。这个时候就要考虑它一定需要副本了。我们可以理解为多个存储内容相同的硬盘，互为对方的副本。而副本的存在又产生了一致性问题甚至推导出 CAP 理论。基本的 Master-Slave 架构，到 Paxos ，Raft 分布式一致性算法都是在解决副本一致性的问题。分布式场景中，还有一个经典的问题——分布式事务。我们需要理解数据库事务的 ACID 特性和经典的二阶段提交算法。真实世界中，节点的失效或者故障，网络问题比如离线，超时，重传等，都给分布式系统带来无限的挑战。在这里真佩服 etcd，Spanner，TiDB 这些为了分布式系统贡献智慧的项目。分布式系统还有一大堆说不完的有意思的，想知道的同学可以买书了～ 如今是一个各种数据系统大杂烩的时代，Redis 可以用作消息队列，Kafka 可以拿来持久化数据。传统的关系型数据库要求用户采用特定的格式存储数据，而分布式文件系统里面只是字节序列，可以是文本图像，视频，特征向量或者任何类型的数据。这些数据存到庞大的数据中心的系统中，被一些批处理系统进行数据清洗交由消费方使用。这里 unix 编程哲学重新回到了我们的眼中：输入是不可变的，输出是为了作为另一个（未知的）程序的输入，而复杂的问题是通过编写 “做好一件事” 的小工具来解决的。 相对的，在流式系统中，输入我们认为是无限的，增量的。比如采用消息队列我们认为是一个不断发布消费，数据不断增长的过程。这里又两种模式，一种是基于 FIFO 模型的，当消息 pop 的时候就消失了；还有一种是基于日志的，多个消费者可以互不影响读取内容。这两者各有优劣，需要权衡。比如对性能吞吐量，顺序要求高的情况下，采用日志的。消息处理大多数情况下需要人为实现幂等。在基于日志的流式系统中应用状态是事件流对于时间积分的结果，所以从数学的角度而言我们的的确确可以把这种系统当作持久化存储和查询的工具。 在未来，流式系统和分布式系统会被更多人接受和理解。不过为了保证现实世界的因关系，比如：两个人在对方好友列表里面互相删除之后，就不会再看到对方的动态，可是在实现分布式存储之后，如果技术上没有处理好，就会发现依然可以互刷动态。再比如：银行转账 A 扣钱，B 获得钱，结果两者没有满足转账后的数目。全局有序和分布式事务是一定要妥妥实现的。数据库的拆分将会变得低成本，或者直接上分布式数据库。函数式编程由于超强的 DSL 能力会给予数据处理场景优质的工具集。 本书在每个章节之后，都附上了大量的参考文献，其中多数是近年的经典论文。很推荐细细读一下，比如 Map Reduce，BigTable 这种。可以很明显体会到，作者将这些论文的内容，基于许多公司的业务场景，开发历程整理出了此书。对于工程师，这本书可以给予大多数人一个关于当今数据系统大而全的视角，在系统设计这个层次收获更多。对于学生可能不太适合。 我觉得服务端工程师，做的最主要的事情就是和数据打交道。通过正确的数据分析与算法优化，可以让许多的企业为此尝到甜头。这本书的最后有一句非常有意思的话：we should stop regarding users as metrics to be optimized, and remember that they are humans who deserve respect, dignity and agency。手机里的 APP 为我们提供了足够的便利，同时我们的行为喜好数据也进入了这些 APP 的后端。这些应用的创作者和运营者应当尊重用户的喜好与隐私。 希望你也喜欢这本书。😊","categories":[],"tags":[{"name":"读书","slug":"读书","permalink":"https://razertory.me/tags/读书/"}]},{"title":"最大子序列","slug":"max-sub-array","date":"2018-12-16T14:14:05.000Z","updated":"2020-03-28T13:02:10.006Z","comments":true,"path":"2018/12/16/max-sub-array/","link":"","permalink":"https://razertory.me/2018/12/16/max-sub-array/","excerpt":"","text":"题目给定一个整数数组 nums ，找到一个具有最大和的连续子数组（子数组最少包含一个元素），返回其最大和。示例： 123输入：[-2,1,-3,4,-1,2,1,-5,4],输出：6解释：连续子数组 [4,-1,2,1] 的和最大，为 6。 暴力法把所有子序列都找出来并求出最大和。利用两个循环，外循环记录开始下标，内循环记录结束下标。不断维护一个最大 sum。 123456789101112131415class Solution &#123; // T:O(n^2) S:O(1) public int maxSubArray(int[] nums) &#123; if (nums == null || nums.length == 0) return 0; int max = nums[0]; for (int i = 0; i &lt; nums.length; i++) &#123; int cur = 0; for (int j = i; j &lt; nums.length; j++) &#123; cur += nums[j]; max = Math.max(max, cur); &#125; &#125; return max; &#125;&#125; 在线处理法在线处理意味着在每次循环过程中都计算出一个当前最终值。遍历过程中维护一个 cur 用来表示当前的值左边的最大非负数的和。如果 cur 小于 0 意味着舍弃掉，反之就带上。在加上了当前值之后，由于当前值的正负，所以还需要维护一个 max 用来找到最大值。 1234567891011class Solution &#123; // T:O(n) S:O(1) public int maxSubArray(int[] nums) &#123; int cur = 0, max = Integer.MIN_VALUE; for (int i = 0; i &lt; nums.length; i++) &#123; cur = cur &lt;= 0 ? nums[i] : (cur + nums[i]); max = Math.max(max, cur); &#125; return max; &#125;&#125; 动态规划法假设函数 f(i) 表示下标从 0 到 i 的子序列最大值。 假设数组为 [-2]，f(0) = -2 假设数组为 [-2, 1]， f(1) = 1 假设数组为 [-2, 1, -3]， f(2) = 1 从上述可以发现。f(1) 可以看成 f(0) + nums[1] 与 nums[1] 比较的较大值；并且 f(2) 也满足这个条件。 那么可以容易得到状态转移方程：f(i) = max(f(i - 1) + nums[i], nums[i])。写出代码就是 1234567891011121314class Solution &#123; // T:O(n) S:O(n) public int maxSubArray(int[] nums) &#123; if (nums == null || nums.length == 0) return 0; int[] dp = new int[nums.length]; int result = nums[0]; dp[0] = result; for (int i = 1; i &lt; nums.length; i++) &#123; dp[i] = Math.max(dp[i - 1] + nums[i], nums[i]); result = Math.max(dp[i], result); &#125; return result; &#125;&#125;","categories":[],"tags":[]},{"title":"NP 问题","slug":"np-complete","date":"2018-12-05T14:11:28.000Z","updated":"2020-03-28T13:02:10.002Z","comments":true,"path":"2018/12/05/np-complete/","link":"","permalink":"https://razertory.me/2018/12/05/np-complete/","excerpt":"","text":"多项式复杂度NP 问题中的 P 意思是 Polynomial，意为多项式复杂度。其中，O(1), O(log(n)), O(n), O(nlog(n)), O(n^2), … O(n^a) 这一类的复杂度（从小到大顺序）的问题是多项式复杂度的。他们的特点是，问题的复杂度不会因为 n 的增大而增大。相反，O(a^n) 和 O(n!) 这类问题因为问题的复杂度会因为 n 增大而增大，所以认为不是多项式的。举 2 个例子， P 问题1234567891011找中位数问题在一个数组 arr = [13, 8, 1, 2, 4, 6, 11] 中，我们要找到数组的中位数。解答需要对这个数组排序，然后找到 [1, 2, 4, 6, 8, 11, 13] 中下标是 (n + 1) / 2 的数字，也就是 6。所以解答这个问题的复杂度是 O(nlog(n))。验证假设有一个解答是 3，要判断这个数是否是这个问题的答案。那么只需要判断数组中大于 3 和小于 3 的数的个数是否相等。所以验证这个解答是否正确的复杂度是 O(n)。 这种解答复杂度和验证复杂度都是 P 的就认为是 P 类问题。 NP Complete 问题123456789101112133 - SAT 问题如果有 a, b, c, d, e, f 都为 boolean 类型的变量。是否存在这样的一组 a, b, c, d, e, f 满足(a || b || c) &amp;&amp; (!a || b || f) &amp;&amp; (!e || d || f) == true解答假设 0 表示 false， 1 表示 true 那么就需要暴力采用枚举的方式去找到这组符合最终结果的值。这样的复杂度是 2^6。也就是说，当输入的变量个数为 n 的时候，复杂度就是 2^n。所以这种问题解决的复杂度是 O(a^n)，这样的问题就是 NP 问题。验证假设有一组解答是 0, 1, 1, 1, 0, 1。要判断是否是这个问题的答案，只需要带入进行计算即可。以验证这个解答是否正确的复杂度是 O(1)。（满足 P） 这种解答复杂度为非 P 而验证复杂度为 P 的就认为是 NP Complete 问题。 N == NP ?回过头看， 当验证一个解答是否是这个问题的正确答案的复杂度是 P 的时候，这个问题是 NP 问题 在 1 的基础上，我们发现如果解决这个问题的复杂度是 P 的时候，这个问题是 P 问题 在 1 的基础上，我们发现解决这个问题的复杂度不是 P 的时候，这个问题是 NP Complete 问题 当我们研究这个世界的问题的时候，有人认为所有的 NP 问题都是 P 问题；有人则认为一定不是所有的 NP 问题都是 P 问题。在计算机科学的颠峰上，人类的智慧一直在为证明 N == NP 做出努力。","categories":[],"tags":[]},{"title":"线程、进程和协程以及 IO 多路复用实现并发","slug":"tread_and_process","date":"2018-11-19T05:55:48.000Z","updated":"2020-03-28T13:02:10.002Z","comments":true,"path":"2018/11/19/tread_and_process/","link":"","permalink":"https://razertory.me/2018/11/19/tread_and_process/","excerpt":"","text":"计算机并发和并行的体系 并发（Concurrency）：如果逻辑流在时间上重叠，那么它们就是并发的 并行（Parallel）：如果逻辑流在时刻上重叠，那么它们就是并行的 erlang 之父用一张图来描述了并发和并行的区别。 Concurrent&Parallel 一些有 GIL 的语言，并不能实现并行，可以参照薄荷 CTO 的 Ruby 实例说明 Ruby 多线程的潜力和弱点。实际上，按照上述图片的例子，咖啡机就像是 CPU 的核心，部分语言即使在有多台咖啡机的情况下，任何时刻也只有一台咖啡机在工作。现代的互联网产品大多是 IO 密集型，而 IO BLOCK 这种操作严格意义上是不占用 CPU 时间的，这里可以参 Java 控制 CPU 占用 一文。所以，并行是并发的子集，通常情况下，我们研究的是并发。 进程的并发简单的说，当一个程序被客户端请求的时候，就创建一个新的进程来服务客户端，进程对 CPU 资源的获取由内核调度。详细的就是： 父进程监听着一个文件描述符 fd_a，当收到客户端请求的时候，立即打开并返回一个描述符 fd_b， 父进程派生一个子进程，子进程获得一份父进程监听的文件描述符列表的副本， 子进程关闭 fd_a，父进程关闭 fd_b（不关闭可能引起内存泄漏） 子进程为客户端服务 多进程的并发可以实现对 CPU 多核心的充分利用，同时由于多进程之间各自维护自己的虚拟空间，这也不容易产生并发安全问题，erlang 的 Actor 模型就是成功的案例。坏处是如果这个程序含有多个进程，同时这些进程之间有通信的需求，需要一些外部的进程通信机制。 线程和协程当今许多语言都有这自身独特的线程模型。在操作系统原生的设计中，线程是进程的实体，具有以下特点 同一个进程中的线程共享了同一块虚拟内存 线程由内核调度，所以 内核可以让线程尽可能享用 CPU 资源 多个线程一定是在竞争 CPU 资源 当某个线程发生阻塞的时候，内核将 CPU 资源分配给别的线程，这里涉及到了上下文切换，与模式切换不同， 实际上 “操作系统的进程切换和 CPU 的模式切换并没有什么关系，发生模式切换可以不改变正处于运行态的进程状态，这种情况下，保存上下文环境和以后恢复上下文环境只需要很少的开销。但是，如果改变正处于运行态的进程状态到另一个状态（就绪、阻塞等），则操作系统必须使其环境产生实质性的变化”——《操作系统：精髓与设计原理》 有时候，编程语言中，CPU 资源的分配用户希望把控制权从内核转交给自身，这时候，另一个概念 – 协程便产生了。通常情况下，多个协程对应着一个操作系统线程。协程的存在让用户可以自己管理程序中不同逻辑流的 CPU 分配，然而，当对应的这个操作系统线程阻塞的时候，所有的协程就可能一起阻塞；这时候就需要语言有对应的调度器来控制。现代语言中的 Golang 就做得很好。 IO 多路复用还是由于现在的互联网产品大多数 IO 密集型，所以很多场景上，单个线程 / 进程就可以实现并发。实际上，操作系统已经做到了这一点，在多个客户端都连接上同一个服务器的时候，这个服务器同时打开了对应个数 + 的 fd，服务端要做的就是当某个或者某些连接被客户端传过来的时候，能够立即处理并且传递给 fd。类似于 123for x in open_connections: if has_new_input(x): process_input(x) 这样做的坏处是会浪费大量的 CPU 时间。应用程序只关心发生变化的 fd，比如：某个 df 变为可以读或者可写。而这个任务现在只需要交给操作系统，在 linux 中，select、poll 和 epoll 都是实现 IO 多路服用的 system call。它们的主要作用就是 应用程序把一堆文件描述符传入，然后 等待返回告诉哪些文件描述符是可读 / 可写 Julia Evans 的 Async IO on Linux: select, poll, and epoll 比较详细说明了上述三种方法的工作方式 并发安全我认为，并发安全可以用一个最简单的例子解释。 有 N 个任务都会对数据 a 进行 write（write 的方式需要取决于 a 的值），如果这 N 个任务并行执行完 a 的值和串行执行完不一致，那么这个并发就是不安全的。这个概念和 数据竞态 类似。可以例举一些有并发安全问题的场景 数据库中某 column x 在业务中 unique，程序员采用 find x or insert n++ 问题 火车售票问题 总结下来，当存在 n = f(n - 1) 这样的场景时，很有可能出现并发安全问题","categories":[],"tags":[{"name":"操作系统","slug":"操作系统","permalink":"https://razertory.me/tags/操作系统/"}]},{"title":"Java 控制 CPU 占用","slug":"control-cpu-just-by-java-code","date":"2018-10-12T16:40:18.000Z","updated":"2020-03-28T13:02:10.002Z","comments":true,"path":"2018/10/13/control-cpu-just-by-java-code/","link":"","permalink":"https://razertory.me/2018/10/13/control-cpu-just-by-java-code/","excerpt":"","text":"选自《编程之美》。 在日常工作中，我们时不时会去看服务器的性能，比较关键的一个就是 CPU 占有率。那么 CPU 占有率到底是什么呢？在本书中就可以找到答案。用 Java 可以实现一个简单的版本用于更深入理解 CPU。 CPU 占有率： 在任务管理器的一个刷新周期内，CPU 忙（执行应用程序）的时间和刷新周期总时间的比率， 就是 CPU 的占用率，也就是说，任务管理器中显示的是每个刷新周期内 CPU 占用率的统计平均值。因此，我们可以写一个程序，让它在任务管理器的刷新期间内一会儿忙， 一会儿闲， 然后通过调节忙／ 闲的比例， 就可以控制任务管理器中显示的 CPU 占用率。 可以同时参考操作系统原理中，描述进程管理的时候说道：操作系统在运行的时候有一个个的进程，这些进程或忙，或闲。很多人都只关注了进程忙的时候做了什么而忽略了进程闲的手在干什么。实际上，进程闲的时候通常有以下状态： 等待用户输入 监听事件发生 处于休眠状态 也就是说：可以通过实现以上三种状态的任意一种，通过代码级别的控制在实现。 占用 cpu 最简单的方式就是一个循环 1while(true)&#123;&#125; // 考虑执行的时候程序在占用 CPU 不占用 cpu 最简单的方式就是让 CPU 休眠 1Thread.sleep(TIME) 最终需要的是一会儿占用一会儿不占用，同时这个控制是人为通过程序干预的，当前我们考虑只是单核的情况。 实际上，操作系统统计 CPU 占有率很简单的一个实现就是，在单位时间 T 中，CPU 存在繁忙时间 R，和空闲时间 I。通常情况下，对于单核 CPU 而言， T = R + I CPU 占有率 = R/T 如果有兴趣，可以把代码贴下来跑一下。 1234567891011121314151617181920public class ControlCPU&#123; public static final double TIME = 1000; //TIME= 周期 private static void control(double rate) throws InterruptedException&#123; while (true)&#123; runCPU(rate * TIME); Thread.sleep((long) (TIME - rate * TIME)); &#125; &#125; private static void runCPU(double time) &#123; long startTime = System.currentTimeMillis(); while ((System.currentTimeMillis() - startTime) &lt; time) &#123; &#125; &#125; public static void main(String[] args) throws InterruptedException &#123; control(0.5); &#125;&#125; 以上在 MacOS 10 运行中，CPU 占有率维持在了 50% 左右。（注意只会有一个 CPU 核心是被利用了）。书中还有很多高级的做法，比如实现一条 sin 三角函数曲线、多核心下的处理。如果有兴趣可以一起分享一下。","categories":[],"tags":[{"name":"操作系统","slug":"操作系统","permalink":"https://razertory.me/tags/操作系统/"}]},{"title":"开源项目之 Nginx","slug":"nginx-over-view","date":"2018-10-09T12:51:02.000Z","updated":"2020-05-04T07:14:43.177Z","comments":true,"path":"2018/10/09/nginx-over-view/","link":"","permalink":"https://razertory.me/2018/10/09/nginx-over-view/","excerpt":"","text":"正文开源项目之 Nginxnginx（读作 “engine x”）是一位名叫 Igor Sysoev 的俄罗斯软件工程师开发的。自 2004 年发布以来，nginx 就一直专注于实现高性能，高并发和低内存占用。nginx 的额外功能，比如：负载均衡、缓存和流量控制以及高效集成在 Web 服务上的能力，使得它成为了当今网站架构的必选。如今，nginx 已经成为互联网中第二受欢迎的开源 Web 服务器。 14.1 高并发为何如此重要？如今，互联网早已无处不在，我们已经很难想象十年前没有互联网的样子。现在的互联网发生了翻天覆地的变化，从基于 NSCA 的可以点击 HTML 页面和基于 Apache 的 Web 服务，到如今能够实现超过 20 亿人实时的沟通。随着 PC、手机和平板的的蔓延，互联网已经将全球经济数字化。面向信息和娱乐的在线服务变得更加优质。线上商业活动的安全方面也发生了明显变化。因此，网站也比以前更加的复杂并且需要大量的工程投入来确保鲁棒性和可扩展性。 并发性成为了网站架构设计的最大挑战之一。自从 web 服务开始的时候，并发性的等级就在持续上升。对于一个热门网站来说，支持几百甚至是几百万用户同时访问来说也不是什么稀罕事情。20 年前，产生并发的原因主要还是客户端的 ADSL 或者拨号（dial-up）连接。如今，并发的产生来源于手机端和以及新型的应用架构，这些架构主要可以支持长连接来提供新闻、信息流发布和朋友间的 feed 流等等。另一方面，导致高并发还由于现代浏览器的工作发生变化，通常是为了提高网页加载速度同时打开 4 到 6 个连接。 为了表述清楚缓慢这种问题，设想一下，一个基于 Apache 的，可以提供 100KB 大小带有文字或者图片的简单 web 服务器。生成或者重新产生这个网页只需要极少不到一秒的时间。但是在带宽只有 80kps 的情况下（下载速度 10kb/s），传输数据到客户端却会花掉 10s。本质上，服务器产生 100kb 数据的速度是相对较快的，随后在传输数据到客户端直至释放连接的过程却是相对较慢的。现在设想，你同时有 1,000 个独立的客户端连到你的服务器并且请求同样的内容。如果对于每个独立的连接，都会占用额外的 1MB 内存，那么对于 1,000 个连接来说就对导致多占用 1000 MB（1G）的内存，而这些仅仅是为了给 1000 个客户端提供 100kb 的内容。实际上，一个典型的 Apache 服务器通常会为了一个连接占用超过 1MB 的内存，遗憾的是几十 k 的带宽足够让手机之间高效通讯。尽管从某种程度而言，发送数据给客户端是慢的，提高操作系统内核的 socket 缓冲大小是可以的，这个不是一个通常的解决方法，并且会有不良影响。 在持久连接中，处理并发会做起来总比说起来有更多的问题，因为要在新建 HTTP 连接的时候避免延迟，让客户端保持连接并且确保对于每个连接服务端都能够保证有足够内存可供使用。 因此，为了能够处理因为用户量增长产生高并发由此带来的负载上升，网站的就必须基于通过一定数目的高效模块来架设。同时，在从获得客户端连接请求，到处理完请求期间，像硬件（CPU，memory，disk），网络容量以及数据存储也是同样重要的。因此，web 服务器需要能在同时请求数和每秒请求频率这两方面都拥有扩展性。 Apache 不合适吗？Apache，开始于 1990s，如今依旧统治着互联网。最初它的架构满足于当时的操作系统和硬件，同时也满足于当时的只有一个独立的物理机运行一个 Apache 服务器的互联网状态。在 2000 年始，一个独立的服务器难以满足增长起来的 Web 服务的情况越来越明显。尽管 Apache 提供了一个可靠的基金会用于未来发展，然而，它这种为了每个新连接复制自身的架构，已经不再适用于非线性的网站扩张。最终，Apache 成为了一个有着许多不同特性，第三方扩展，和一些普遍用于 web 应用开发的功能的 web 服务器。然而，没有什么东西是十全十美的，Apache 有者丰富功能的同时，对于每个连接产生的 CPU 和内存消耗使得它不能很好的扩展。 因此，当服务器的硬件、操作系统和网络条件成为了网站增长的瓶颈时，全世界的 web 工程师开始寻找一种更加高效的方法。大约十年前，一位名叫 Daniel Kegel 的杰出工程师宣称：”是时候让 web 服务能够支持 10k 并发了。” 同时他还预测了我们现在会叫互联网云服务。c10k 问题一产生，就引来了许许多多的解决方案用以优化实时的高并发。nginx 成为了其中最出色的解决方案之一。 为了解决 C10k 问题中的 10,000 个实时的连接，nginx 用了一种与众不同的架构，这种架构会更适合在同时处理大量的连接和一秒钟内完成多次请求环境中，问题规模的增长是非线性的。nginx 是事件驱动的（event-based，所以它不会用 Apache 的那种为每一个 web 请求都申请一个进程或者线程。结果便是，即使负载升高，内存和 CPU 都还是处于掌控之中。nginx 目前可以在一台普通的机器上，同时处理上万的并发。 nginx 的第一个版本主要是和 Apache 服务器一起部署，用来单独处理原本是 Apache 处理的 HTML, CSS, JavaScript 和图片这样的静态资源。在随后的迭代中，nginx 支持像 FastCGI, ，uswgi 或者 SCGI 协议集成到应用当中部署，并且可以利用像 memcached 这样的分布式缓存系统。同时像反向代理，负载均衡这样的特性也随之加上。这些额外的特点让 nginx 成为了构建可扩展性 web 服务的高效的基础组件的工具之一。 2012 年二月，Apache 2.4.x 分支发布。尽管，这个最新版本的 Apache 增加了多核处理器支持模块和用于提升可扩展性和并发的模块，然而它的性能，并发能力，以及资源利用能力与纯事件驱动的 web 服务器比，依旧难以望其项背。 很乐意看到新版的 Apache 服务器有着更好的可扩展性，尽管这样可以减少自身的瓶颈，然而像典型的 nginx-plus-Apache 配置依旧会被使用。 使用 nginx 会有更多的优势吗？能够高性能地处理高并发一直是部署了 nginx 之后获得的最主要的好处。然而，还有一些更有趣的东西。 在过去几年中，网站架构就一直在拥抱解耦并从 web 服务器中拆分出一些基础组件。然而，那些原本存在于 LAMP-based 的网站中的基础组件，在 LEMP-based（E 代表着 Nginx 的读音） 的网站中，却能让 web 服务器成为基础组件以及用一种不同的方式去集成相同的或者改进了的应用和数据库工具。 nginx 非常适合做这个，因为它可以方便提供一个并发支持，延迟超时处理，SSL 支持，静态文件支持，压缩和缓存，甚至是 http 流媒体的高效的层级，而这些功能原本处于应用层。nginx 也可以直接集成一些像 Redis/memcached 这样的 NoSQL 用以优化大用户量场景。 当近代的开发语言和工具流行起来的时候，越来越多的公司正在改变他们的开发和部署方式。nginx 成为了改变过程中最重要的部分，同时，nginx 让很多公司在有限的预算中，快速地启动开发他们的服务。 nginx 是从 2002 年开始开发。到 2004 年，它以 two-clause BSD license 发布。随后，nginx 用户量开始增高，修改建议，bug 报告，观察报告等都在社区中不断完善 ngix。 nginx 最初的源码是用 C 完成的。nginx 已经可以部署在许多架构和操作系统中，比如 Linux, FreeBSD, Solaris, Mac OS X, AIX and Microsoft Windows。nginx 拥有自己的库并且并没有大量使用 C 标准库，一些像 zlib, PCRE and OpenSSL 这一类的库因为有证书冲突而没有被采用。 在 Windows 上部署 nginx 更像是一个实现 nginx 的理论证明而不是一个功能完善的项目。由于内核限制，nginx 的一些功能特性并不能发挥出来。在 windows 上的 nginx 并发能力、性能会更低，也没有缓存和带宽策略。将来 windows 上的 nginx 版本会继续完善。 14.2. nginx 架构总览传统的解决并发的方式是每个单独的请求一个进程或者线程，并且网络和 io 操作都是阻塞式的。在传统的应用当中，这种做法会由于 CPU 和内存开销导致低效。开启一个独立的进程或者线程会需要预加载一个新的运行时环境和上下文。这些东西也会占用一些额外的 CPU 时间，线程频繁轮换导致的上下文切换带来的开销最终导致了低性能。这些问题在一些旧的 web 服务架构，比如 Apache 中得到了证实。这是在提供丰富普遍特性与优化服务器开销之前的一种权衡。 从最早开始，nginx 就被设定为在网站用户动态增长期间，用来提高网站性能和服务器资源利用率的工具，以至于它拥有一种与众不同的模型。这是受一些操作系统的事件驱动概念启发。这也产生了 nginx 的核心架构：模块化，事件驱动，异步，单线程，非阻塞。 nginx 大量采用多路复用（multiplex）和事件通知，并对每个 nginx 进程分配了特定的任务。连接被有限个数单线程的 worker 进程高效轮询（run-loop）处理。 每个 worker 都可以同时处理数千个并发连接和每秒请求。 Code Structure 代码结构worker 代码包含了核心和功能模块。nginx 核心负责维护一个紧凑的轮询，并在处理请求的每个阶段都执行模块中对应的部分。模块构成了大部分表示层和应用层功能。模块从网络和存储介质中进行数据的读写，传输内容，过滤出站内容，执行服务端的动作和当代理功能被打开的时候传递请求到被代理的（upstream）服务器。 nginx 模块化的架构可以让开发者在不修改核心代码的情况下加入一些自定也的扩展。nginx 模块稍微有点不同，比如核心模块、事件模块、阶段处理器、协议、变量处理器、filter，upstream 和负载均衡。目前，nginx 不再支持动态加载模块。模块在 nginx build 阶段就会被编译。然而，在将来 nginx 会在主版本上提供 loadable 模块和 ABI。更多关于不同模块的信息详见 Section 14.4. 在处理一些关于网络接收，处理和管理以及内容检索的时候，nginx 使用了事件通知（event notification）机制以及一些操作系统（ Linux, Solaris and BSD-based）的磁盘 IO 优化，比如：kqueue, epoll, and event ports。目的是为操作系统提供尽可能多的提示，以便为入站和出站流量、磁盘操作、socket 读写、超时等获取及时的异步反馈。针对 nginx 运行的每个 unix-like 的操作系统，对多路复用和高级 I/O 操作使用不同的方法进行了大量优化。 更多 nginx 架构高级概述详见 Figure 14.1. Figure 14.1: Diagram of nginx’s architecture Workers 的模型正如之前提到的，nginx 并不为每个连接开一个进程或者线程。相反，worker 进程为每个新连接都采用一个共用的监听 socket 并在轮询中高效处理着数千个连接。对于 nginx 的 worker，没有采用一些特别的连接机制，都是由操作系统内核来完成的。一旦启动，一些监听 socket 就会被创建。worker 就会持续地接受连接，处理 http 请求和从对应的这些 socket 中读写数据。 轮询是 nginx 代码中最复杂的部分。它包括了综合（comprehensive）的内部调用和依赖大量的异步任务处理思想。异步操作通过模块化，事件通知，函数回调和计时器实现。总体上，关键在于尽可能的非阻塞。唯一让 nginx worker 阻塞的只有磁盘不足的情况。 因为 nginx 不会为每个连接新开进程或者线程，内存占用在很多场景下都不会高。nginx 节约了 cpu 占用也是因为没有进程线程的创建和销毁。nginx 要做的就是检查网络和存储，创建新连接，把新连接加入到轮询，并且在完成之前都异步处理。nginx 谨慎采用了一些系统调用比如资源池化和内存分配，以至于在极端的情况下也不会有很高的 CPU 占用。 由于 nginx 处理连接就开了几个 worker，在多核情况下可以很好的扩展。大致就是一个核心一个 worker，这样每个 worker 充分利用 cpu 核心，避免了线程切换和锁等待。不会产生资源不足并且每个单线程的 worker 进程中都存在资源管理策略。这种模型允许在不同存储设备之间有更好的扩展性，促进磁盘利用并且避免了磁盘 IO 阻塞。总的来说，服务器资源在多个 worker 工作的情况下被更高效使利用了。 对于某些磁盘使用和 CPU 负载模式，应该调整 nginx worker 的数量。这些规则在这里有点基础，系统管理员应该基于他们的工作负载尝试一些配置。一般建议如下：如果负载模式是 CPU 密集型的—例如，处理大量 TCP/IP、执行 SSL 或压缩，nginx worker 的数量应该与 CPU 核心的数量相匹配；如果负载主要是磁盘 I/O 限制。例如，从存储中提供不同的内容，或者大量的反向代理，workers 的数量可能是内核数量的 1.5 到 2 倍。有些工程师根据单个存储单元（磁盘分区）的数量来选择 workers 的数量，这种方法的效率取决于磁盘存储的类型和配置。 nginx 开发人员在即将发布的版本中要解决的一个主要问题是如何避免磁盘 I/O 上的大部分阻塞。目前，如果没有足够的存储性能来服务于由特定的 worker 生成的磁盘操作，那么 worker 仍然可能阻塞从磁盘读取 / 写入。存在许多机制和配置文件指令来减轻此类磁盘 I/O 阻塞场景。最值得注意的是，sendfile 和 AIO 等选项的组合通常会为磁盘性能带来很大的空间。应该根据数据存储、可用的内存大小和底层存储体系结构来计划 nginx 的安装。 现有 worker 模型的另一个问题是关于内嵌脚本支持的限制。首先，使用标准的 nginx 发行版，只支持嵌入 Perl 脚本。对此有一个简单的解释：关键问题是内嵌脚本可能阻止任何操作或意外退出。这两种类型的行为都会立即导致 worker 被挂起，同时影响数千个连接。需要更多的工作来让 nginx 的嵌入式脚本更简单、更可靠、适合更多的应用程序。 nginx 进程角色nginx 在内存中运行几个进程；有一个 master 进程和几个 worker 进程。还有一些特殊用途的进程，特别是缓存加载器和缓存管理器。版本 1.x 中的所有进程都是单线程的。所有进程主要使用共享内存机制进行进程间通信。主进程作为 root 用户运行。缓存加载器、缓存管理器和 worker 作为非特权用户运行。 master 进程主要有以下任务 读取并验证配置文件 创建、绑定和关闭 socket 启动，终止和维护配置好了个数的 worker 进程 不中断情况下重新加载配置 控制热更新（从二进制文件启动和必要情况下回滚） 打开日志文件 编译内嵌的 perl 脚本 worker 进程接受和处理来自客户机的连接，提供反向代理和过滤功能，并完成 nginx 能够做的几乎所有其他事情。关于监视 nginx 实例的状况，系统管理员应该关注 worker 进程，因为他们是反映 web 服务器实际日常操作的过程。 缓存加载器进程负责检查磁盘上的缓存项，并使用缓存元数据填充 nginx 的内存数据库。实际上，缓存加载器准备 nginx 实例来处理已经存储在磁盘上的文件，这些文件位于一个特别分配的目录结构中。它遍历目录，检查缓存内容元数据，更新共享内存中的相关条目，然后在一切都干净且可以使用时退出。 缓存管理器主要负责缓存过期和失效。在正常的 nginx 操作过程中，它保持在内存中，在失败的情况下由主进程重新启动。 nginx 缓存简览nginx 中的缓存是以文件系统上分层数据存储的形式实现的。缓存 key 是可配置的，可以使用不同的特定于请求的参数来控制进入缓存的内容。缓存 key 和缓存元数据存储在共享内存段中，缓存加载器、缓存管理器和 worker 进程可以访问共享内存段。目前，除了操作系统的虚拟文件系统机制产生的优化之外，没有任何文件的是缓存在内存当中。每个缓存的读取都放在文件系统上的不同文件中。层次结构（级别和命名细节）是通过 nginx 配置指令控制的。当将响应写入缓存目录结构时，路径和文件的名称来自代理 URL 的 MD5 值。 在缓存中放置内容的过程如下：当 nginx 从 upstream 服务器读取响应时，内容首先被写入缓存目录结构之外的临时文件中。当 nginx 完成对请求的处理后，它会重命名临时文件并将其移动到缓存目录中。如果用于代理的临时文件目录位于另一个文件系统上，则会复制该文件，因此建议将临时目录和缓存目录保存在同一个文件系统上。当需要显式清除缓存目录结构中的文件时，从缓存目录结构中删除文件也是相当安全的。nginx 有第三方的扩展，可以远程控制缓存的内容，并且计划了更多的工作来让这个功能可以集成到主发行版中。 14.3. nginx 配置nginx 的配置系统受到了 Igor Sysoev 使用 Apache 的经验的启发。他的主要观点是，对于 web 服务器来说，可伸缩的配置系统是必不可少的。当使用大量虚拟服务器、目录、位置和数据集维护大型复杂配置时，会遇到扩展问题。在一个相对较大的 web 设置中，如果在应用程序和系统工程师都没有正确地完成，那么它可能是一个噩梦。 因此，nginx 配置的目的是简化日常操作，并提供进一步扩展 web 服务器配置的简单方法。 nginx 的配置保存在许多纯文本文件中，这些文件通常位于 /usr/local/etc/nginx 或 /etc/nginx。主配置文件通常称为 nginx.conf。为了保持它的整洁，部分配置可以放在单独的文件中，这些文件可以自动包含在主文件中。然而，这里应该注意到 nginx 目前不支持 apache 风格的分布式配置（即”。htaccess 文件）。所有与 nginx web 服务器行为相关的配置都应该驻留在一组集中的配置文件中。 配置文件最初由 master 进程读取和验证。当 worker 进程从 master 进程 fork 时，worker 进程可以使用编译后的只读形式 nginx 配置。配置结构由通常的虚拟内存管理机制自动共享。 nginx 配置有几个不同的内容：main, http, server, upstream, location （同时 mail 相当于邮件服务代理）。配置文件内容不重叠。例如，在 main 中不存在 location。此外，为了避免不必要的歧义，没有任何类似于 “全局 web 服务器” 配置的东西。nginx 的配置是干净和合乎逻辑的，允许用户维护包含数千个指令的复杂配置文件。在一次私人谈话中，Sysoev 说，“全局服务器配置中的 location、directory 和其他块是我在 Apache 中不喜欢的特性，所以这就是为什么它们从未在 nginx 中实现的原因。” 配置文件语法、格式和定义遵循所谓的 c 风格约定。这种生成配置文件的特殊方法已经被各种开源和商业软件应用程序所使用。从设计上讲，c 风格的配置非常适合嵌套描述，具有逻辑性，易于创建、阅读和维护，并受到许多工程师的喜爱。nginx 的 c 风格配置也很容易自动化。 虽然 nginx 的一些指令类似于 Apache 配置的某些部分，但是设置一个 nginx 实例却是完全不同的体验。例如，nginx 支持重写规则，尽管需要管理员手动修改遗留的 Apache 重写配置以匹配 nginx 风格。重写引擎的实现也不同。 一般来说，nginx 设置还支持一些原始机制，作为精简 web 服务器配置的一部分非常有用。简单地提到变量和 try_files 指令是有意义的，这些指令对于 nginx 来说是唯一的。nginx 变量被开发出来是为了提供一个更强大的机制来控制 web 服务器的运行时配置。变量经过优化以快速解析，并在内部预编译为索引。根据需要进行解析，通常，变量的值只计算一次，并在特定请求的生命周期内缓存。变量可以与不同的配置指令一起使用，为描述条件请求处理行为提供了额外的灵活性。 “try_files”指令最初旨在以更合适的方式逐步替换条件 “if” 配置语句，它的设计目的是快速有效地尝试 / 匹配不同的 uri 到内容的映射。总的来说，try_files 指令工作得很好，并且非常高效和有用。更多详情推荐读者去 try_files directive 14.4. nginx 内部如前所述，nginx 代码库由核心和许多模块组成。 nginx 的核心是负责提供 Web 服务器，Web 和邮件反向代理功能的基础；它支持使用底层网络协议，构建必要的运行时环境，并确保不同模块之间的无缝交互。但是，大多数协议和特定的应用程都是由 nginx 功能模块完成的，而不是核心模块。 在内部，nginx 通过由模块组成的的管道或模块链来处理连接。换句话说，对于每个操作，都有一个正在进行相关工作的模块；例如，压缩，修改内容，执行服务器端，通过 FastCGI 或 uwsgi 协议与 upstream 应用服务器通信，或与 memcached 通信。 有几个 nginx 模块位于核心和真正的 “功能” 模块之间。这些模块是 http 和 mail。这两个模块在核心和较低级别组件之间提供了额外的抽象级别。在这些模块中，实现了与诸如 HTTP，SMTP 或 IMAP 的相应应用层协议相关联的事件序列的处理。结合 nginx 核心，这些上层模块负责维护对各个功能模块的正确调用顺序。虽然 HTTP 协议目前是作为 http 模块的一部分实现的，但由于需要支持 SPDY 等其他协议，因此计划将来将其分离为功能模块。更多 SPDY 协议详见 SPDY: An experimental protocol for a faster web 功能模块可分为事件模块，阶段处理程序，输出 filter，变量处理程序，协议，上游和负载平衡器。大多数这些模块补充了 nginx 的 HTTP 功能，但事件模块和协议也用于 mail。事件模块提供特定的 OS 依赖事件通知机制，如 kqueue 或 epoll。 nginx 使用的事件模块取决于操作系统功能和构建配置。协议模块允许 nginx 通过 HTTPS，TLS / SSL，SMTP，POP3 和 IMAP 进行通信。 典型的 HTTP 请求处理周期如下所示。 客户端发送 http 请求。 nginx core 依据配置文件中的 location 选择合适的阶段处理器。 如果配置生效，负载均衡器就会选择一个 upstream 服务器代理。 阶段处理器执行任务，并把缓冲区的内容传递给第一个 filter。 第一个 filter 将内容传递给第二个 filter 第二个 filter 传递给第三个（迭代执行） 将最后的 response 发送给客户端。 nginx 模块调用是非常可定制的。它使用指向可执行函数的指针来执行一系列回调。然而，这样做的缺点是它可能给想要编写自己的模块的程序员带来很大的负担，因为他们必须准确定义模块应该如何以及何时运行。 nginx API 和开发人员的文档都在不断改进，并且可以更多地用来缓解这个问题。 下面这些列子是可以添加模块的位置： 在读和处理配置文件之前 在每个服务器出现以及配置文件指向的地方 当 主配置 被初始化的时候 当服务器被初始化的时候 当 server configuration 被合并到 主配置的时候 当 location configuration 初始化或者合并到 parent server configuraton 的时候 当 master 进程启动或者存在的时候 当一个新的 worker 进程启动或者存在的时候 当处理一个请求的时候 当过滤请求 header 和请求 body 的时候 当请求转发到 upstream 服务器的时候 服务器中的响应的时候 当完成与一个 upstream 服务器的交互的时候 在 worker 进程中，导致生成响应的运行循环的 action 序列如下所示： 启动 ngx_worker_process_cycle(). 使用操作系统特定的机制来处理事件（such as epoll or kqueue） 接收事件并且分发给相关的 action 处理 / 代理请求 header 和 body 产生响应内容 (header, body) 并传递给客户端 结束请求 重启 timers，events 轮询本身（步骤 5 和 6）确保增量生成响应并将其流式传输到客户端。 处理 HTTP 请求的更详细过程可能如下所示 初始化请求处理 处理 header 处理 body 调用相关的 nginx 处理器 执行每个处理阶段 这将我们带到了每个阶段。当 nginx 处理 HTTP 请求时，它会将其传递给许多处理阶段。在每个阶段都有处理程序可以调用。通常，阶段处理程序处理请求并生成相关输出。阶段处理程序被附加到配置文件中定义的位置。 阶段处理程序通常执行以下四项操作：获取位置配置，生成适当的响应，发送 header 以及发送 body。处理程序有一个参数：描述请求的特定结构。请求结构有很多关于客户端请求的有用信息，例如请求 method，URI 和 header。 读取 HTTP 请求 header 时，nginx 会查找关联的虚拟服务器配置。如果找到虚拟服务器，请求将经历六个阶段： 服务器重写阶段 location 阶段 location 重写阶段（将请求带回到上一个阶段） 连接控制阶段 try_files 阶段 日志阶段 为了响应请求生成必要的内容，nginx 将请求传递给合适的内容处理程序。根据确切的位置配置，nginx 可能首先尝试所谓的无条件处理程序，如 perl，proxy_pass，flv，mp4 等。如果请求与上述任何内容处理程序都不匹配，则由以下处理程序之一按照以下顺序选择：random index，index，autoindex，gzip_static，static。 索引模块的详细信息可以在 nginx 文档中找到，但这些是使用尾部斜杠处理请求的模块。如果像 mp4 或 autoindex 这样的专用模块则不合适，内容被认为只是磁盘上的文件或目录（即静态），并由 static 内容处理程序提供服务。对于目录，它会自动重写 URI，以便始终存在尾部斜杠（然后发出 HTTP 重定向）。 然后将内容处理程序的内容传递给 filter。filter 也附加到 location，并且可以为 location 配置多个 filter。filter 执行操作处理程序生成的输出的任务。对于预先定义的开箱即用 filter，执行的顺序在编译时就确定。对于第三方 filter，可以在构建阶段对其进行配置。在现有的 nginx 实现中，filter 只能进行出站更改，并且目前没有机制来编写和附加 filter 来进行输入内容转换。输入过滤将出现在 nginx 的未来版本中。 filter 遵循特定的设计模式。调用 filter，开始工作，并调用下一个 filter，直到调用链中的最终 filter。之后，nginx 完成响应。filter 不必等待前一个 filter 完成。调用链中的下一个 filter 可以在上一个 filter 的输入可用时立即开始工作（功能上与 Unix 管道非常相似）。反过来，生成的输出响应可以在接收到来自上游服务器的整个响应之前传递给客户端。 还有 header filter 和 body filter；nginx 会分别用相关的 filter 来给相应 header 和 body 添加数据 header filter 主要有下面三个步骤 决定是否对这个响应进行操作 操作这个响应 调用下一个 filter body filter 修改生成的数据，下面是 body filter 的一些案例 服务端 includes XSLT 过滤 图片过滤（比如修改图片尺寸） 修改编码 gzip 压缩 chunked encoding 在 filter chain 之后，响应将传递给 writer。除了 writer 之外，还有一些额外特殊用途的 filter，即 copy 和 postponefilter。 copyfilter 负责使用可能存储在代理临时目录中的相关响应内容填充内存缓冲区。 postponefilter 用于子请求。 子请求是请求 / 响应处理的非常重要的机制。子请求也是 nginx 最强大的方面之一。对于子请求，nginx 可以从与客户端最初请求的 URL 不同的 URL 返回结果。一些 Web 框架将此称为内部重定向。但是，nginx 更进一步 - 过滤器不仅可以执行多个子请求，而且可以将输出组合成单个响应，但子请求也可以嵌套和分层。子请求可以执行其自己的子子请求，并且子子请求可以发起子子子请求。子请求可以映射到硬盘，其他处理程序或上游服务器上的文件。子请求对于根据原始响应中的数据插入其他内容非常有用。例如，SSI（服务器端包含）模块使用过滤器来解析返回文档的内容，然后将 “include” 指令替换为指定 URL 的内容。或者，它可以是一个过滤器，将文档的整个内容视为要检索的 URL，然后将新文档附加到 URL 本身 upstream 和负载均衡器也值得简要描述。upstream 用于实现可以被识别为反向代理（proxy_pass 处理程序的内容。upstream 模块主要准备将请求发送到 upstream 服务器（或 “后端”）并接收响应。这里没有调用输出 filter。当 upstream 服务器准备好被写入和读取时，upstream 模块确切地做的是设置要调用的回调。存在实现以下功能的回调： 创建的请求缓冲被发送到 upstream 服务器的 重新连接到 upstream 服务器（在请求产生之前） 处理 upstream 服务器响应的内容并且存储指向从 upstream 服务器内容的指针。 放弃请求（主要是客户端过早断开连接） 从 upstream 服务器读完内容之后结束请求 整理响应 body（比如删除 http 响应 trailer） 负载均衡器模块连接到 proxy_pass 处理程序，以便在多个 upstream 服务器符合条件时提供选择上游服务器的功能。负载均衡器注册启用配置文件指令，提供额外的上游初始化函数（以解析 DNS 中的上游名称等），初始化连接结构，决定在何处路由请求以及更新统计信息。目前，nginx 支持两种标准规则，用于对 upstream 服务器进行负载均衡：循环和 ip-hash。 upstream 和负载均衡处理机制包括用于检测失败的上游服务器以及将新请求重新路由到其余服务器的算法 - 尽管计划进行大量额外工作以增强此功能。总的来说，nginx 开发团队计划对负载均衡器进行更多的工作，并且在下一版本的 nginx 中，将大大改进跨不同上游服务器分配负载以及运行状况检查的机制。 还有一些其他有趣的模块提供了一组额外的变量供配置文件使用。虽然 nginx 中的变量是在不同的模块中创建和更新的，但有两个模块完全专用于变量：geo 和 map。 geo 模块用于根据客户端的 IP 地址进行跟踪。此模块可以创建依赖于客户端 IP 地址的任意变量。另一个模块 map 允许从其他变量创建变量，实质上提供了对主机名和其他运行时变量进行灵活映射的能力。这种模块可以称为变量处理程序。 在单个 nginx worker 中实现的内存分配机制在某种程度上受到了 Apache 的启发。nginx 内存管理的高度概述如下：对于每个连接，必要的内存缓冲区被动态分配，链接，用于存储和操作请求和响应的头部和主体，然后在连接释放时释放。值得注意的是，nginx 试图尽可能避免在内存中复制数据，并且大多数数据都是通过指针值传递的，而不是通过调用 memcpy。 更深入一点，当模块生成响应时，将检索到的内容放入内存缓冲区，然后将其添加到缓冲链链接中。后续处理也适用于此缓冲链链接。缓冲链在 nginx 中非常复杂，因为有几种处理方案因模块类型而异。例如，在实现 body filter 模块时精确管理缓冲区可能非常棘手。这样的模块一次只能在一个缓冲区（链路的链路）上运行，它必须决定是否覆盖输入缓冲区，用新分配的缓冲区替换缓冲区，或者在有问题的缓冲区之前或之后插入新的缓冲区。更复杂的是，有时模块会收到几个缓冲区，因此它必须有一个不完整的缓冲区链。但是，此时 nginx 只提供了一个用于操作缓冲区链的低级 API，因此在进行任何实际实现之前，第三方模块开发人员应该能够熟练使用 nginx 这个神秘的部分。 关于上述方法的注释是在连接的整个生命周期中分配了内存缓冲区，因此对于长期连接，保留了一些额外的内存。同时，在空闲的 keepalive 连接上，nginx 只花费 550 个字节的内存。对 nginx 的未来版本进行可能的优化将是重用和共享内存缓冲区以实现长期连接。 管理内存分配的任务由 nginx 池分配器完成。共享内存区域用于接受互斥锁，缓存元数据，SSL 会话缓存以及与带宽管制和管理（限制）相关的信息。在 nginx 中实现了一个 slab 分配器来管理共享内存分配。为了同时安全地使用共享内存，可以使用许多锁定机制（互斥锁和信号量）。为了组织复杂的数据结构，nginx 还提供了一个红黑树实现。红黑树用于将缓存元数据保存在共享内存中，跟踪非正则表达式位置定义以及其他几项任务。 遗憾的是，上述所有内容从未以一致和简单的方式描述，因此开发 nginx 的第三方扩展的工作非常复杂。虽然存在关于 nginx 内部的一些好的文档 - 例如，由 Evan Mille r 生成的那些文档 - 需要大量的逆向工程工作，并且 nginx 模块的实现仍然是许多人的黑科技。 尽管与第三方模块开发相关的某些困难，nginx 用户社区最近看到了许多有用的第三方模块。例如，有一个用于 nginx 的嵌入式 Lua 解释器模块，用于负载均衡的附加模块，完整的 WebDAV 支持，高级缓存控制以及本章作者鼓励并将在未来支持的其他有趣的第三方工作。（参考 Open Resty – 译者注） 14.5. 收获当 Igor Sysoev 开始编写 nginx 时，大多数给互联网赋能的软件已经存在，并且这种软件的体系结构通常遵循传统服务器和网络硬件，操作系统和旧的互联网体系结构。然而，这并没有阻止 Igor 认为他能够继续改进 Web 服务器领域的东西。所以，虽然第一课可能看起来很简单，但事实是：总有改进的余地。 考虑到更好的 Web 软件的想法，Igor 花了很多时间开发初始代码结构并研究为各种操作系统优化代码的不同方法。十年后，参考在版本 1 上的多年积极开发，他如今正在开发 nginx 版本 2.0 的原型。很明显，一个软件产品的新架构的初始原型和初始代码结构对于未来的重要性是非常重要的。 值得一提的另一点是发展应该集中。Windows 版本的 nginx 可能是一个很好的例子，说明如何避免在既不是开发人员的核心竞争力或目标应用程序的情况下稀释开发工作。它同样适用于重写引擎，该引擎在多次尝试增强 nginx 时出现，具有更多功能以便与现有的旧设置向后兼容。 但值得一提的是，尽管 nginx 开发者社区不是很大，但 nginx 的第三方模块和扩展一直是其受欢迎程度的重要组成部分。 Evan Miller，Piotr Sikora，Valery Kholodkov，Zhang Yichun（agentzh 中文名：章亦春）以及其他才华横溢的软件工程师所做的工作得到了 nginx 用户社区及其原始开发人员的赞赏。 参考文档 春哥的 nginx 教程 淘宝 Tengine 团队","categories":[],"tags":[{"name":"基础组件","slug":"基础组件","permalink":"https://razertory.me/tags/基础组件/"}]},{"title":"GraphQL 优化分享","slug":"opt-graphql-server","date":"2018-06-13T16:04:44.000Z","updated":"2020-03-28T13:02:10.001Z","comments":true,"path":"2018/06/14/opt-graphql-server/","link":"","permalink":"https://razertory.me/2018/06/14/opt-graphql-server/","excerpt":"","text":"上周给公司的研发同事们做了一次服务端的 GraphQL 优化分享，主要是后端同事，这里打算写博文的方式记下来。 缓存其实缓存这个词，我个人会觉得比较含糊，有时候我们会把 cache 和 buffer 这两个东西弄混淆 Cache 为了弥补高速设备和低速设备的鸿沟而引入的中间层，最终起到 “加快访问速度” 的作用。比如：CPU 缓存，NoSQL 缓存 Buffer 主要目的进行流量整形，把突发的大数量较小规模的 I/O 整理成平稳的小数量较大规模的 I/O，以 “减少响应次数”（比如从网上下电影，你不能下一点点数据就写一下硬盘，而是积攒一定量的数据以后一整块一起写，不然硬盘都要被你玩坏了，比如 TCP 做数据传递的时候，单位是“帧” 而不是一个个的字节） 浏览器 / APP -&gt; 网络转发 -&gt; 应用服务器 -&gt; DB 每个地方都可以用缓存方式来做。比如这里的分享主要来自于应用服务器。 Chatty其实在我另一篇博文里面有提到这一方面的优化，这里我打算详细分析。 “GraphQL 是一种可以让程序员编写 clean code 的方法，每个 type 的 field 都有着单一的目的（single-purpose）。然而，如果我们不多加考虑，那么我们的 GraphQL 服务端就会变得非常’chatty’，或者说会执行很多重复的查询” – https://graphql.org/learn/best-practices(个人翻译) 比如我司有一个服务端查询： 1234567891011121314query clazz($id: ID) &#123; clazz(id: $id) &#123; subjects&#123; id activities &#123; id teacher &#123; id name &#125; &#125; &#125; &#125;&#125; 这样一个查询，如果不做适当的处理会变得很慢。这里的查询会遍历两个列表 subjects 和 activities 用来获取 teacher 数据，如果做一下复杂度分析，可能很容易得到，执行 SQL 的数目就是两个列表的矩阵数，也就是 O(N * M)。这样当然会很慢，所以不得不用上一些 batching 技术。 1gem 'graphql-batch' 用上这个技术之后，我之前的这种大量 SQL 的查询，就会被整型成少量的 in 查询。这里关于 N + 1 query 的东西不再赘述。做复杂度分析的时候就会发现 SQL 执行数目只和数据嵌套深度有关。 Promise在做 GraphQL Server 开发的时候，我对一个事情一直很好奇，就是每个 field 获取是不是异步的？所以我做了以下实验： 1234567891011121314151617181920212223242526272829def blocking_query(id) p Thread.current.to_s sleep 3 idendfield :a, types.String do resolve -&gt; (o, args, c) &#123; blocking_query \"a\" &#125;endfield :b, types.String do resolve -&gt; (o, args, c) &#123; blocking_query \"b\" &#125;endfield :c, types.String do resolve -&gt; (o, args, c) &#123; blocking_query \"c\" &#125;endfield :d, types.String do resolve -&gt; (o, args, c) &#123; blocking_query \"d\" &#125;end 结果非常不乐观 123456#&lt;Thread:0x007fe15e1887fb0&gt;#&lt;Thread:0x007fe15e1887fb0&gt;#&lt;Thread:0x007fe15e1887fb0&gt;#&lt;Thread:0x007fe15e1887fb0&gt;Completed 200 OK in 12641ms 可以发现我每个 field 的耗时被叠加起来了。同时发现每个 field 的获取打印线程都是一样的内容。 这时候可以加上异步优化来处理，完成一个 Promise，来自 issue 1234567891011121314151617181920212223242526272829303132require 'promise'module ConcurrentPromise def self.execute promise = Promise.new promise.source = Source.new(promise) &#123; yield &#125; promise end class Source def initialize(promise) @promise = promise # Promise isn't thread-safe, so the thread needs to store the result # on a separate object @thread_result = Promise.new @thread = Thread.new(@thread_result) do |result| begin result.fulfill(yield) rescue =&gt; err result.reject(err) end end end def wait @thread.join @promise.fulfill(@thread_result) end end private_constant :Sourceend 同时优化我的查询方法 1234567def blocking_query(id) ConcurrentPromise.execute do p Thread.current.to_s sleep 3 id endend 执行结果 123456#&lt;Thread:0x007fe15e18897d0&gt;#&lt;Thread:0x007fe15e1888f60&gt;#&lt;Thread:0x007fe15e1884g30&gt;#&lt;Thread:0x007fe15e1887g60&gt;Completed 200 OK in 3577ms 可以发现打印的线程堆栈是不同的内容，同时接口速度接近于单个 field 的执行速度。 在阅读 graphql-batch 源码中我也发现了这样子类似的一段 123456def load(key) cache[cache_key(key)] ||= begin queue &lt;&lt; key ::Promise.new.tap &#123;|promise| promise.source = self&#125; endend 基于 Promise 来和缓存 key 的方式来完成。 后记缓存，异步，这一类的词眼在现代的研发体系中总是很常见，GraphQL 的 ruby 生态还需推进。","categories":[],"tags":[{"name":"系统设计","slug":"系统设计","permalink":"https://razertory.me/tags/系统设计/"}]},{"title":"我的「java-code-lab」","slug":"start-my-coding-project","date":"2018-06-01T05:23:53.000Z","updated":"2020-03-28T13:02:10.001Z","comments":true,"path":"2018/06/01/start-my-coding-project/","link":"","permalink":"https://razertory.me/2018/06/01/start-my-coding-project/","excerpt":"","text":"在念书期间，学习数据结构课程的时候，我发现了一个质量很好的 Java 数据结构 项目，当时也是受益匪浅。后来的几年作者没有再维护，但我认为这会是一个非常好的模式。 记得在 OJ 上做 ACM 时，我总会去思考这个代码会如何去通过测试用例，达到 Accepted 状态，记得当时的同学、学长说是后台有一个跑测试用例的服务。通过这个服务可以检查你代码的语法正确性、结果正确性以及时间空间复杂度。 后来在工作中我接触到了 TDD 开发模式，或者说代码开始有单元测试，所以就会想起了这个项目，打算重做一个。一来自我学习，当程序、数据结构、算法与我有更深的理解；二是验证这种模式对于他人的益处是否有我想象的那样多。 内容基础的数据结构和算法必不可少，那么基础的程度我想先以红皮书《算法》为主。确保在这个点上，我能做到最好。随后，我会加入《算法导论》的内容和案例。几个好处：第一，《算法导论》提到的案例以及实现都是工业级的，比如切割钢条，DNA 最长公共序列，股票涨幅等。第二，《算法导论》里面给出了大量的数学推导和论证。所以在后期做性能测试的时候，如果我的测试结果和预想的有偏差，我可以找到理论依据或者的确是自己出 bug 的地方。再者，并发编程，IO，网络编程，设计模式以及一些系统设计的内容同样是这个项目会涉及的。除了基础的算法或者数据结构我应该去实现，同时我也会从 Leetcode，Lintcode，剑指 Offer 上去寻找一些相关的题目的解答和测试用例。举个例子就是我们实现 Kth Bigest 这种问题的时候，有一种就是快排的做法，可以发现只要对快排做少许的修改就可以实现；当然，用堆也是个很好的选择。 这个项目除了数据结构和算法，还有包括但不限于设计模式，网络 IO，并发编程等计算机领域较为基础的案例实现与测试。 测试在实际的项目开发中，编写测试代码是很多公司为了保证软件质量的推荐方法。但是在这个项目里面，除了证明算法的正确性。更多的是在于能够让使用者在研究某个代码的时候，能够方便、清晰发现程序是怎么跑的。这一点很重要，相比于只给出代码和代码的图文讲解，我认为：每个人的理解能力和理解方式都是不同的，但做编码工作的人，大量的认知和理论都会\b实现到代码上。所以，只有很清楚自己要写什么样的代码的时候，才能真正按照意图写出来。这一点，我认为通过测试用例的方式是最好。当我想理解一段代码的时候，我可以 debug，可以试做修改通过跑测试的方式来证明自己的思路是没有问题的。 在编写测试代码的时候，我常常遇到了一些问题，比如给定一个数组生成最低二叉搜索树，我该如何证明生成的二叉搜索树是最低的。在我写单例模式的测试代码的时候，我发现我需要模拟并发的情景来证明一些线程安全问题。这个过程充满了挑战和乐趣。 对于学习者我认为不管是实现一个经典的算法，还是 finish a problem，最重要的是要学会如何解决问题。所谓的问题是什么呢？在这个项目里，问题就是测试代码里面的内容。程序要实现的，就是以某种方式通过测试用例。这些我和一些 contributors 的实现对于每一个学习者来说只是一个参考和值得质疑的（质疑程序的性能和正确性；包括测试代码是否合乎逻辑）。我认为，当拿到一个新的算法时，最好先去自己实现一遍，然后去对应的 unit test 跑一遍，当其中有疑难的时候，记得 debug。只有在源码 + debug 的投射下，所有的疑惑都会被散去，那些看起来含糊不清的逻辑也会变得清晰。 地址 Talk is cheap. Show me the test!","categories":[],"tags":[{"name":"思考","slug":"思考","permalink":"https://razertory.me/tags/思考/"}]},{"title":"将 Rails 项目拆分成微服务架构","slug":"把rails项目拆分为微服务","date":"2018-03-07T07:21:50.000Z","updated":"2020-03-28T13:02:10.001Z","comments":true,"path":"2018/03/07/把rails项目拆分为微服务/","link":"","permalink":"https://razertory.me/2018/03/07/把rails项目拆分为微服务/","excerpt":"","text":"原文链接：* https://dev.to/iriskatastic/split-your-rails-app-using-microservices-ckl 📚正文当你的项目变得庞大的时候，项目的复杂度就会以指数级增长。笔者曾发表过另一篇文章，用以帮助别人理解 朝着微服务演进的必要性。这次，笔者会详细说明为什么微服化是一种已经被证实过的，并且可以让项目更容易让人理解和扩展的解决方案。接下来，我们来探究如何把 Rails 用微服务的方式拆分。 这篇文章的核心思想是如何将你的 Rails 项目拆分成微服务架构。 如果你有一个庞大的 Rails 项目，并且你很清楚这个项目需要变得简单容易扩展。那么本文将告诉你构建出 Ruby on Rails 微服务的主要理由和必要条件。 虽然将一个项目演变成微服务建有好有坏，但当你的项目变得庞大并且难以扩展的时候，你仍然需要想办法将它拆分。 微服务架构的第一个关键点在于，你的服务由各个部分组成，他们之间互不耦合，并且每个部分都可以独立成一个项目。各个部分之间的通信依靠各自暴露的 API，这样使得每个开发团队可以独立开发维护自己的部分，避免了不同团队之间可能会污染到对方的代码。 什么情况下你需要拆分了？ 你的测试代码会跑 20min 以上 models 的数量好几百甚至上千 项目中有完全独立的功能模块 开发人员不能够独立开发 &amp; 部署他们自己的功能模块了 或许还有更多的原因，但上述的条件是你一定不能忽视的。 当然，不用微服务架构的话，你也可以多部署一些服务器，或者实现一些执行效率更高的技术。但是可以想一下， 为什么会使用微服务方式拆分项目？microservice-based 项目由一个个独立的功能模块组成，每个功能模块有自己独立的职责。这也允许了开发团队可以各自独立开发自己的功能模块，这种本身的低耦合特性可以让每个功能模块变得更小，更简单，更容易理解，也可让新人更快地参加项目开发。最终，当项目被完全解耦拆分后，功能迭代的成本代价也将更小。新技术的引入导致的服务重写也将会变得可行。同样的，采用了微服务的跨语言特性可以让你的项目由不同语言，技术框架来完成。因此， ** 采用微服务的优势在于 ** 功能模块可以被复用 功能模块可以独立于整个项目平稳快速部署 更加容易加入新功能 减少测试时间成本，主要是可以在一个独立模块里测试而不是在大项目中 小项目一定比大项目更容易维护 独立的功能模块让开发人员可以专注于自己的那个点 更不容易改动到无关的功能模块 更好的可靠性和容错性 更容易迁移和升级 允许技术多样性（* 语言栈不统一不一定好 – 译者注 *） 任何一个服务挂掉了，都不会导致整个项目挂掉 新人更容易熟悉代码 然而，只有你的项目满足了下面的条件的时候，你才可以体会到这些优势 Ruby on Rails 项目拆分成为微服务架构的必要前置条件 你的项目代码一定是 Clean Code 因为你的项目拆分成了微服务架构之后，从业务层面看项目本身的是和之前一样。但如果你的代码写得很糟糕，项目跑在了很糟糕的代码上。你的项目将很难按照预期去迭代。 你的项目应该合理地按照正确的功能边界去拆分 当你开始构建微服务架构的时候，你的项目一定会包含了许多 “微服务” 组件，每个组件的运行应该是畅通无阻的，并且他们之间的运行不会互相干扰。只要能按照准确、良好的定义去拆分出来，你就能做到这一点。规划好组件的每个任务和与该任务相关的工作，以获得最佳结果。确保你拆分出来的每个组件都各自独立。 你的项目应该有备份 在你的微服务项目中，无论何时，其中的组件都有可能会挂掉，但你的整个项目却还是可以一直跑。整个项目中，某个（多个）功能组件挂掉了可能会给你带来一系列问题。 你需要给你的项目在这个方面做好规划以保证挂的时间可以最小。这当然就需要你能够做好对应的监控，当有异常产生的时候可以及时应对。这种策略可以让工程师们在用户感知到服务挂掉之前处理好。 当你的做好这些准备的时候，就可以开始拆了。 如何把你的 Rails 项目拆掉呢？正常情况下，一个 Rails 项目会被分层，通常是：表示层 (presentation), 业务逻辑层 (business logic), 数据连接层 (data access)。企业级项目通常也就被分为上述三层。你的微服务组件也就分成了这三类。 Presentation Layer 响应 Http 请求和实现（REST）API 或者 Web UI。在项目中通常会有复杂的用户交互接口。 Business logic layer 实现业务逻辑的组件 Data-access layer 基础连接的组件，通常是连接数据库或者消息中间件。 同样，笔者也推荐阅读 Syndicode 的 Rails 微服务。","categories":[],"tags":[{"name":"系统设计","slug":"系统设计","permalink":"https://razertory.me/tags/系统设计/"}]},{"title":"向 GraphQL 迁移","slug":"graphql-in-ruby","date":"2018-01-19T16:21:00.000Z","updated":"2020-03-28T13:02:10.001Z","comments":true,"path":"2018/01/20/graphql-in-ruby/","link":"","permalink":"https://razertory.me/2018/01/20/graphql-in-ruby/","excerpt":"","text":"时间大概是 17 年九月，我司的新 App 开工。在技术选型上，我们很激进地选择了 graphql instead of restful。当时想到的理由是很简单，但是也无可厚非的：我们的 App 和后期将要做的新 Web 在业务上有很大的相通性，并且功能模块有不少类似的地方，所以用 graphql 这样的技术来做 API 层复用性会很高。后期在做一些管理后台的时候，\b效率也会高不少。 很有幸的是，自己成了研发团队中这个技术的主要推进者。我们的服务端主语言是 Ruby，所以像 github，Shopify 这样的 GraphQL + Ruby 的领先者对我们而言如同教科书一般。我研究了几乎每个 Github 的 Query 和 Mutation，所以很大程度上也受于他们优质的思路，和客户端同学们踩了一些坑，但最后也都得出了这个在开发过程中，很多方面都会优于 rest 的结论。具体的优势，和官方说的差不多，我也不愿再赘述。这篇博客，更多的是关于技术的感悟和一些架构上的理念。吹架构可能有点悬乎。好吧，就再抽象一点，应该是做工程的一些总结；当然 GraphQL 的干货内容也不会少的。 开发模式用 graphql 很大程度上，省去了一些不必要的沟通成本。当你受够了用 rest 要去构建 API 文档之后，graphql 这种天然的 代码即文档 的便捷性让团队受益匪浅。服务端只需要在编码期间，按照标准的写法，加上对应的 desc 方法，就可以无缝生成对应的文档。 12345678GuardianType = GraphQL::ObjectType.define do name 'Guardian' description 'A guardian has his/her children' field :id, types.ID field :name, types.String field :children, types[ChildType]end 也许很多人会问，这样做，服务端的业务逻辑不久会分散一部分到客户端上了？客户端的工作量不就提升了？ 但实际上，graphql 的自动化 + 强类型 + ‘强结构’让客户端的工作量反而下降了。首先，客户端的的确是要写查询代码，就像是服务端写 SQL 一样。但是 graph 的查询要比 SQL 简单了不知道多少倍，更准确的，我可以称之为 描述 1234567891011query guardian&#123; guardain(id: 9527)&#123; id name children&#123; id name avatar &#125; &#125;&#125; 在传统的 rest 开发模式中，客户端并不知道一个请求服务端会返回什么结果。除非有准确，准时，详尽的文档。客户端会恐惧，服务端某个返回结果数据结构不对自己 crash 了。所以反而会在这一层上，思考很多。（我们评价一个技术体系的工作成本，除了实际的编码产出，还应该评估思考，权衡所带来的开销）维护未知响应结果的开销，总是会大于自己决定响应结果的开销。 技术上的恐惧感，往往来源于未知。没有测试，不知道原理，没有约定，没有规范，都是会带来未知。 \b查询性能解决 N+1 query 和查询环几乎是 graphql 服务端使用者遇到的第一个问题。 我记得在遇到这个问题的时候，我看到了 Facebook 官方出的 dataloader 来解决。所以像我们这种 Ruby 栈的团队，自然就选用了 Shopify 团队的 graphql-batch，现在回忆起来依旧记得当时的兴奋感。好吧，还是贴两张图，对比一下。 before1234567891011121314INFO -- : (0.019141s) DESCRIBE `activities`INFO -- : (0.018571s) SELECT * FROM `activities` WHERE (`subject_id` = 26)INFO -- : (0.017604s) SELECT * FROM `activities` WHERE (`subject_id` = 32)INFO -- : (0.015813s) SELECT * FROM `activities` WHERE (`subject_id` = 25)INFO -- : (0.016458s) SELECT * FROM `activities` WHERE (`subject_id` = 28)INFO -- : (0.016015s) SELECT * FROM `activities` WHERE (`subject_id` = 29)INFO -- : (0.016588s) SELECT * FROM `activities` WHERE (`subject_id` = 27)INFO -- : (0.019075s) SELECT * FROM `activities` WHERE (`subject_id` = 33)INFO -- : (0.014484s) SELECT * FROM `activities` WHERE (`subject_id` = 31)INFO -- : (0.017730s) SELECT * FROM `activities` WHERE (`subject_id` = 35)INFO -- : (0.017332s) SELECT * FROM `activities` WHERE (`subject_id` = 34)INFO -- : (0.015393s) SELECT * FROM `activities` WHERE (`subject_id` = 36)INFO -- : (0.016545s) SELECT * FROM `activities` WHERE (`subject_id` = 39)INFO -- : (0.014443s) SELECT * FROM `activities` WHERE (`subject_id` = 40) after123INFO -- : (0.022229s) SELECT * FROM `subjects` WHERE ((`clazz_id` = 1) AND (`grade_id` = 1)) ORDER BY `ordering`INFO -- : (0.018933s) DESCRIBE `activities`INFO -- : (0.105159s) SELECT * FROM `activities` WHERE (`subject_id` IN (26, 32, 25, 28, 29, 27, 33, 31, 35, 34, 36, 39, 40)) 作为一个非主流入门的 Ruby 用户（我第一个开始做的 Ruby 项目是用 Grape + ActiveRecord 搭建的），rails 其实也是最近才开始真正用上的。当遇到第一个让人兴奋的东西时，自然得去读他的源码。不过读之前，当发现了这个神奇的一幕时，我的第一直觉告诉我是不是和 JDBC 规范中的 prepared statement 一样？让相同语法的 sql 不再多执行，只要换 sql 的参数就行。这样 MySQl 编译开销就小了很多。反正缓存无处不在。不，难道是动态规划？。好吧，都是猜的。既然这个玩意儿解决了一个重要瓶颈问题，那么不管出于兴趣层面，还是业务层面，尽可能去熟悉它一定是很好的想法。 首先，这个玩意儿是学习了 Facebook 官方的 dataloader 做的。如果有去了解，自然猜到，这个项目用了 promise.rb。可以在源码里面看到这样的调用，并且作者的 sample 里面也很清晰地说明了这个玩意儿的输出开始就是个 Promise 对象。所以对于 graphql ruby 的 1resolve -&gt; (obj, arg, ctx) &#123;&#125; 这样的 do block 块就应该和普通的 http api 框架不一样。可以理解的是，一次 graph 请求的，所需要的每个 field 对于服务端来说，是并发获取的。这里并发，肯定用了他自己的一套策略。可以猜测的是，就是用的异步 I/O\u001b的方式。所以从这一点来说，这个库本质上和 graphql ruby 是完全契合的。 架构Ruby 在编程表达能力上，几乎是我见过的所有服务端语言中最强的。元编程 + Rails + 各种库，几乎算得上是互联网创业公司的神兵利器。当然，语言这种东西，好坏得看上下文的，不同的话在不同的语境中效果还是不一样的。至少对于我司的业务和人员编制，加上对于往后的预估，在大量的论证下，大家都对技术选型达成了一致。我们需要技术团队有强大的业务推进能力，并且能做出高效稳定的服务。 现在看来，这样的选择没有错。这里可以大致展示（公司没钱，服务器释放掉了 –): http://rc.kid17.com:10000 随着产品的重构和业务的迁移，我们的流量开始逐步转向 graphql 服务，各个规范也建立起来。随着业务模块的复杂度增高，用户数量的增加和数据量的增大，新一轮的挑战即将开始。首先就是要把这个巨大复杂的 Rails + GraphQL 项目拆掉，第二个就是确定新的架构，第三点就是拥抱变化，让这个项目本身变得可以维护，扩展和改进。 我司的的产品中，有两套用户体系，就像是美团的商家版和客户版，拉钩、Boss 的求职版和企业版。两种不同的身份的角色参与到整个业务。我们有 feed 流，电商，电子书，用户管理，教学管理等业务模块。我\b目前的想法是让 graphql 去代理这些服务给客户端调用。最外层用 Kong\b 去完成用户\b鉴权和 ngixn 的一些基本\b功能。 以上拙见，都来自于各种踩坑之后的感悟。 ps: 很期待可以认识同样在用\bgraphql 的团队或个人，欢迎打扰 \bwechat: chendalichun","categories":[],"tags":[{"name":"系统设计","slug":"系统设计","permalink":"https://razertory.me/tags/系统设计/"}]},{"title":"用 Netty 的一次 bug 跟踪","slug":"使用netty踩过的坑","date":"2017-11-19T16:21:00.000Z","updated":"2020-03-28T13:02:10.000Z","comments":true,"path":"2017/11/20/使用netty踩过的坑/","link":"","permalink":"https://razertory.me/2017/11/20/使用netty踩过的坑/","excerpt":"","text":"问题在给我司旧系统解决性能问题的时候，有一个让我印象深刻的事件。我们有个历史项目，是用 Netty 做的 Http 服务器，这个项目在运行一段时间之后，几乎所有的接口都会变得很慢。通过一系列的观察，可以发现的是，每次重启服务器，接口就会变得很快，然后只要过了一周，即使在并发量及低的情况下，接口依然很慢。我最终定位到了瓶颈出现在了一个点上：通过 lsof -i 这个命令，可以看到项目所绑定的 8080 端口上，有着大量的 CLOSE_WAIT 的 tcp socket fd。具体的表征就是 #本地复现 12345678966387 springchan 435u IPv6 0x978a4f8b9aa9a147 0t0 TCP localhost:http-alt-&gt;localhost:56126 (CLOSE_WAIT)java 66387 springchan 436u IPv6 0x978a4f8b9aa9b847 0t0 TCP localhost:http-alt-&gt;localhost:56127 (CLOSE_WAIT)java 66387 springchan 437u IPv6 0x978a4f8bacc17987 0t0 TCP localhost:http-alt-&gt;localhost:56319 (CLOSE_WAIT)java 66387 springchan 438u IPv6 0x978a4f8b9a8d4287 0t0 TCP localhost:http-alt-&gt;localhost:56129 (CLOSE_WAIT)java 66387 springchan 439u IPv6 0x978a4f8b9a8d53c7 0t0 TCP localhost:http-alt-&gt;localhost:56130 (CLOSE_WAIT)java 66387 springchan 440u IPv6 0x978a4f8b9a8d3707 0t0 TCP localhost:http-alt-&gt;localhost:56131 (CLOSE_WAIT)java 66387 springchan 441u IPv6 0x978a4f8b9a8d3147 0t0 TCP localhost:http-alt-&gt;localhost:56132 (CLOSE_WAIT)java 66387 springchan 442u IPv6 0x978a4f8bae270707 0t0 TCP localhost:http-alt-&gt;localhost:56350 (CLOSE_WAIT)java 66387 springchan 443u IPv6 0x978a4f8b9a8d4e07 0t0 TCP localhost:http-alt-&gt;localhost:56134 (CLOSE_WAIT) 也就是说，有着大量的 tcp socket 资源在并发量很低的情况下依旧处于 CLOSE_WAIT 的状态。可以肯定的是，这种现象一定是不健康的。CLOSE_WAIT 处于 TCP 四次分手过程中，客户端首先给服务端发送了 fin 报，服务端接受之后就会回应一个 ack 报给对方，同时状态切换为 CLOSE_WAIT。接下来呢，服务端真正需要考虑的事情是察看是否还有数据发送给对方 ，如果没有的话，那么就可以 close 这个 SOCKET，发送 FIN 报文给对方，也即关闭连接。所以你在 CLOSE_WAIT 状态下，需要完成的事情是等待你去关闭连接。这个时候，这两个过程是可以控制的，第一种，服务端不再发送报文给客户端，直接 close。第二种，服务端发送 fin 报给客户端，同时自己处于 LAST_ACK 状态。所以我比较确定的是，服务端在管理 TCP 连接的阶段出了问题。 复现第一步，我用 ab test 复现这个情况，这个还是比较容易的。在本地，我对任何一个接口并发请求，这个时候可以看到服务端有大量的 ESTABLISHED 状态的 socket。第二步，我强行让请求端发送 fin 报，其实很简单：ctr + c。这个时候如我所料，这些 socket 都变成 CLOSE_WAIT 状态。 分析 &amp; 解决这个老项目本身就是个单体应用，所以可以很确认问题是出在了项目代码这一块。可以猜测的是，是使用 Netty 的方式不对。所以我第一步，去研究了 Netty 的基本原理。 直接发现问题出现在了 channelRead() 这个方法里面。按照官方的说明，使用者的服务里面不应该有阻塞调用。这会严重耗费系统资源。如果有阻塞调用，希望放在线程池里。所以我将项目中的阻塞调用层，统一用了线程池来管理。 12345678910111213141516171819 public class HttpServerInboundHandler extends ChannelInboundHandlerAdapter&#123; @Override public void channelRead(ChannelHandlerContext ctx, Object msg)&#123; new Route(ctx, msg).init(); &#125; @Override public void channelReadComplete(ChannelHandlerContext ctx) throws Exception &#123; ctx.flush(); &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) &#123; ctx.flush(); ctx.channel().close(); ctx.close(); &#125;&#125; 修改之后 12345678910111213141516171819202122public class HttpServerInboundHandler extends ChannelInboundHandlerAdapter&#123; ExecutorService executor =Executors.newCachedThreadPool(); @Override public void channelRead(ChannelHandlerContext ctx, Object msg)&#123; executor.execute(new Route(ctx, msg)); &#125; @Override public void channelReadComplete(ChannelHandlerContext ctx) throws Exception &#123; ctx.flush(); &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) &#123; ctx.flush(); ctx.channel().close(); ctx.close(); &#125;&#125; 这里让线程池来处理阻塞调用层的业务。经过 ab 测试发现不会再有类似的问题出现。 Netty 本质上是一个纯异步的框架，用来做一些并发高，实时性强的系统，比如通信，游戏等。但对于普通的互联网服务，比如由 tomcat，jetty 构建的一些服务而言，netty 可能并不适合。Netty 设计之初意在高性能通信，用有限的资源产出最大的并发量级。所以使用者不得不很清楚 socket 编程，select，poll 模型等。这对于单纯的非实时高并发应用开发者来说，增加了不少复杂度。如果是 Java 生态的，并且需要自己实现一些服务治理功能的，基于 Netty 开发是个明智的选择。","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"https://razertory.me/tags/Java/"}]},{"title":"C 实现 http 服务器","slug":"tiny-http","date":"2017-11-10T16:00:00.000Z","updated":"2020-03-28T13:02:10.000Z","comments":true,"path":"2017/11/11/tiny-http/","link":"","permalink":"https://razertory.me/2017/11/11/tiny-http/","excerpt":"","text":"第一次接触到服务端开发，应该是学 Java 的时候，了解到了 servlet 这么一个东西。在做 Java 的时候一直以来都是和 Tomcat, Jetty 这样的 Web 容器打交道。记得那段时间找到一本书《How Tomcat works》, 我花了两周时间阅读，感触良多。后来又陆陆续续接触到了 Netty 之类的。 我开始问自己，实现一个 http 服务器，到底需要做什么? 所以，直接上 C 吧。参照某个经典的案例。 首先在读源码之前，可以再温习一下基于 TCP 的协议是如何通信的。 TCP 结合这个图，从源码的角度一步一步分析。源码地址 https://github.com/razertory/tinyhttpd 主流程 服务端初始化 等待客户端 连接，通信，结束通信，结束连接 1234567891011121314151617181920212223242526int main(void)&#123; int server_sock = -1; u_short port = 0; int client_sock = -1; struct sockaddr_in client_name; int client_name_len = sizeof(client_name); server_sock = startup(&amp;port);// 下面的 starup 函数 printf(\"httpd running on port %d\\n\", port); while (1) &#123; client_sock = accept(server_sock, (struct sockaddr *)&amp;client_name, &amp;client_name_len);// 上图的 accept 函数 if (client_sock == -1) error_die(\"accept\"); accept_request(client_sock); &#125; close(server_sock); return(0);&#125; 启动一个 http server 调用库 socket() 函数产生 httpd 调用 bind() 函数让 socket 绑定给某个端口 (绑定过程比想象复杂) 调用 listen() 函数监听端口 12345678910111213141516171819202122232425262728293031323334353637383940int startup(u_short *port)&#123; int httpd = 0; //sockaddr_in 是 IPV4 的套接字地址结构。定义在 &lt;netinet/in.h&gt;, 参读《TLPI》P1202 struct sockaddr_in name; httpd = socket(PF_INET, SOCK_STREAM, 0);// 上图的 socket 函数 if (httpd == -1) error_die(\"socket\"); memset(&amp;name, 0, sizeof(name)); name.sin_family = AF_INET; //htons()，ntohs() 和 htonl() 包含于 &lt; arpa/inet.h&gt;, 参读《TLPI》P1199 // 将 * port 转换成以网络字节序表示的 16 位整数 name.sin_port = htons(*port); //INADDR_ANY 是一个 IPV4 通配地址的常量，包含于 &lt;netinet/in.h&gt; // 大多实现都将其定义成了 0.0.0.0 参读《TLPI》P1187 name.sin_addr.s_addr = htonl(INADDR_ANY); //bind() 用于绑定地址与 socket。参读《TLPI》P1153 // 如果传进去的 sockaddr 结构中的 sin_port 指定为 0，这时系统会选择一个临时的端口号 if (bind(httpd, (struct sockaddr *)&amp;name, sizeof(name)) &lt; 0)// 上图的 bind 函数 error_die(\"bind\"); // 如果调用 bind 后端口号仍然是 0，则手动调用 getsockname() 获取端口号 if (*port == 0) /* if dynamically allocating a port */ &#123; int namelen = sizeof(name); //getsockname() 包含于 &lt; sys/socker.h&gt; 中，参读《TLPI》P1263 // 调用 getsockname() 获取系统给 httpd 这个 socket 随机分配的端口号 if (getsockname(httpd, (struct sockaddr *)&amp;name, &amp;namelen) == -1) error_die(\"getsockname\"); *port = ntohs(name.sin_port); &#125; // 最初的 BSD socket 实现中，backlog 的上限是 5. 参读《TLPI》P1156 if (listen(httpd, 5) &lt; 0) error_die(\"listen\"); return(httpd);&#125; 接受 &amp; 解析请求123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110void accept_request(int client)&#123; char buf[1024]; int numchars; char method[255]; char url[255]; char path[512]; size_t i, j; struct stat st; int cgi = 0; /* becomes true if server decides this is a CGI * program */ char *query_string = NULL; // 读 http 请求的第一行数据（request line），把请求方法存进 method 中 numchars = get_line(client, buf, sizeof(buf)); i = 0; j = 0; while (!ISspace(buf[j]) &amp;&amp; (i &lt; sizeof(method) - 1)) &#123; method[i] = buf[j]; i++; j++; &#125; method[i] = '\\0'; // 如果请求的方法不是 GET 或 POST 任意一个的话就直接发送 response 告诉客户端没实现该方法 if (strcasecmp(method, \"GET\") &amp;&amp; strcasecmp(method, \"POST\")) &#123; unimplemented(client); return; &#125; // 如果是 POST 方法就将 cgi 标志变量置一 (true) if (strcasecmp(method, \"POST\") == 0) cgi = 1; i = 0; // 跳过所有的空白字符 (空格) while (ISspace(buf[j]) &amp;&amp; (j &lt; sizeof(buf))) j++; // 然后把 URL 读出来放到 url 数组中 while (!ISspace(buf[j]) &amp;&amp; (i &lt; sizeof(url) - 1) &amp;&amp; (j &lt; sizeof(buf))) &#123; url[i] = buf[j]; i++; j++; &#125; url[i] = '\\0'; // 如果这个请求是一个 GET 方法的话 if (strcasecmp(method, \"GET\") == 0) &#123; // 用一个指针指向 url query_string = url; // 去遍历这个 url，跳过字符 ？前面的所有字符，如果遍历完毕也没找到字符 ？则退出循环 while ((*query_string != '?') &amp;&amp; (*query_string != '\\0')) query_string++; // 退出循环后检查当前的字符是 ？还是字符串 (url) 的结尾 if (*query_string == '?') &#123; // 如果是 ？ 的话，证明这个请求需要调用 cgi，将 cgi 标志变量置一 (true) cgi = 1; // 从字符 ？ 处把字符串 url 给分隔会两份 *query_string = '\\0'; // 使指针指向字符 ？后面的那个字符 query_string++; &#125; &#125; // 将前面分隔两份的前面那份字符串，拼接在字符串 htdocs 的后面之后就输出存储到数组 path 中。相当于现在 path 中存储着一个字符串 sprintf(path, \"htdocs%s\", url); // 如果 path 数组中的这个字符串的最后一个字符是以字符 / 结尾的话，就拼接上一个 \"index.html\" 的字符串。首页的意思 if (path[strlen(path) - 1] == '/') strcat(path, \"index.html\"); // 在系统上去查询该文件是否存在 if (stat(path, &amp;st) == -1) &#123; // 如果不存在，那把这次 http 的请求后续的内容 (head 和 body) 全部读完并忽略 while ((numchars&gt; 0) &amp;&amp; strcmp(\"\\n\", buf)) /* read &amp; discard headers */ numchars = get_line(client, buf, sizeof(buf)); // 然后返回一个找不到文件的 response 给客户端 not_found(client); &#125; else &#123; // 文件存在，那去跟常量 S_IFMT 相与，相与之后的值可以用来判断该文件是什么类型的 //S_IFMT 参读《TLPI》P281，与下面的三个常量一样是包含在 &lt;sys/stat.h&gt; if ((st.st_mode &amp; S_IFMT) == S_IFDIR) // 如果这个文件是个目录，那就需要再在 path 后面拼接一个 \"/index.html\" 的字符串 strcat(path, \"/index.html\"); //S_IXUSR, S_IXGRP, S_IXOTH 三者可以参读《TLPI》P295 if ((st.st_mode &amp; S_IXUSR) || (st.st_mode &amp; S_IXGRP) || (st.st_mode &amp; S_IXOTH) ) // 如果这个文件是一个可执行文件，不论是属于用户 / 组 / 其他这三者类型的，就将 cgi 标志变量置一 cgi = 1; // 构造响应 if (!cgi) // 如果不需要 cgi 机制的话， serve_file(client, path); else // 如果需要则调用 execute_cgi(client, path, method, query_string); &#125; close(client);&#125; 不需要 cgi 机制的 response1234567891011121314151617181920212223242526void serve_file(int client, const char *filename)&#123; FILE *resource = NULL; int numchars = 1; char buf[1024]; // 确保 buf 里面有东西，能进入下面的 while 循环 buf[0] = 'A'; buf[1] = '\\0'; // 循环作用是读取并忽略掉这个 http 请求后面的所有内容 while ((numchars&gt; 0) &amp;&amp; strcmp(\"\\n\", buf)) /* read &amp; discard headers */ numchars = get_line(client, buf, sizeof(buf)); // 打开这个传进来的这个路径所指的文件 resource = fopen(filename, \"r\"); if (resource == NULL) not_found(client); else &#123; // 打开成功后，将这个文件的基本信息封装成 response 的头部 (header) 并返回 headers(client, filename); // 接着把这个文件的内容读出来作为 response 的 body 发送到客户端 cat(client, resource); &#125; fclose(resource);&#125; 需要 CGI 机制的 response123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119void execute_cgi(int client, const char *path, const char *method, const char *query_string)&#123; char buf[1024]; int cgi_output[2]; int cgi_input[2]; pid_t pid; int status; int i; char c; int numchars = 1; int content_length = -1; // 往 buf 中填东西以保证能进入下面的 while buf[0] = 'A'; buf[1] = '\\0'; // 如果是 http 请求是 GET 方法的话读取并忽略请求剩下的内容 if (strcasecmp(method, \"GET\") == 0) while ((numchars&gt; 0) &amp;&amp; strcmp(\"\\n\", buf)) /* read &amp; discard headers */ numchars = get_line(client, buf, sizeof(buf)); else /* POST */ &#123; // 只有 POST 方法才继续读内容 numchars = get_line(client, buf, sizeof(buf)); // 这个循环的目的是读出指示 body 长度大小的参数，并记录 body 的长度大小。其余的 header 里面的参数一律忽略 // 注意这里只读完 header 的内容，body 的内容没有读 while ((numchars&gt; 0) &amp;&amp; strcmp(\"\\n\", buf)) &#123; buf[15] = '\\0'; if (strcasecmp(buf, \"Content-Length:\") == 0) content_length = atoi(&amp;(buf[16])); // 记录 body 的长度大小 numchars = get_line(client, buf, sizeof(buf)); &#125; // 如果 http 请求的 header 没有指示 body 长度大小的参数，则报错返回 if (content_length == -1) &#123; bad_request(client); return; &#125; &#125; sprintf(buf, \"HTTP/1.0 200 OK\\r\\n\"); send(client, buf, strlen(buf), 0); // 下面这里创建两个管道，用于两个进程间通信 if (pipe(cgi_output) &lt; 0) &#123; cannot_execute(client); return; &#125; if (pipe(cgi_input) &lt; 0) &#123; cannot_execute(client); return; &#125; // 创建一个子进程 if ((pid = fork()) &lt; 0 ) &#123; cannot_execute(client); return; &#125; // 子进程用来执行 cgi 脚本 if (pid == 0) /* child: CGI script */ &#123; char meth_env[255]; char query_env[255]; char length_env[255]; //dup2() 包含 &lt; unistd.h&gt; 中，参读《TLPI》P97 // 将子进程的输出由标准输出重定向到 cgi_ouput 的管道写端上 dup2(cgi_output[1], 1); // 将子进程的输出由标准输入重定向到 cgi_ouput 的管道读端上 dup2(cgi_input[0], 0); // 关闭 cgi_ouput 管道的读端与 cgi_input 管道的写端 close(cgi_output[0]); close(cgi_input[1]); // 构造一个环境变量 sprintf(meth_env, \"REQUEST_METHOD=%s\", method); //putenv() 包含于 &lt; stdlib.h&gt; 中，参读《TLPI》P128 // 将这个环境变量加进子进程的运行环境中 putenv(meth_env); // 根据 http 请求的不同方法，构造并存储不同的环境变量 if (strcasecmp(method, \"GET\") == 0) &#123; sprintf(query_env, \"QUERY_STRING=%s\", query_string); putenv(query_env); &#125; else &#123; /* POST */ sprintf(length_env, \"CONTENT_LENGTH=%d\", content_length); putenv(length_env); &#125; //execl() 包含于 &lt; unistd.h&gt; 中，参读《TLPI》P567 // 最后将子进程替换成另一个进程并执行 cgi 脚本 execl(path, path, NULL); exit(0); &#125; else &#123; /* parent */ // 父进程则关闭了 cgi_output 管道的写端和 cgi_input 管道的读端 close(cgi_output[1]); close(cgi_input[0]); // 如果是 POST 方法的话就继续读 body 的内容，并写到 cgi_input 管道里让子进程去读 if (strcasecmp(method, \"POST\") == 0) for (i = 0; i &lt; content_length; i++) &#123; recv(client, &amp;c, 1, 0); write(cgi_input[1], &amp;c, 1); &#125; // 然后从 cgi_output 管道中读子进程的输出，并发送到客户端去 while (read(cgi_output[0], &amp;c, 1) &gt; 0) send(client, &amp;c, 1, 0); // 关闭管道 close(cgi_output[0]); close(cgi_input[1]); // 等待子进程的退出 waitpid(pid, &amp;status, 0); &#125;&#125; 补充CGI 机制CGI 全称是 “通用网关接口”(Common Gateway Interface)，它可以让一个客户端，从网页浏览器向执行在 Web 服务器上的程序请求数据。 CGI 描述了客户端和这个程序之间传输数据的一种标准。 CGI 的一个目的是要独立于任何语言的，所以 CGI 可以用任何一种语言编写，只要这种语言具有标准输入、输出和环境变量。 如 php，perl，tcl 等。 客户端访问某个 URL 地址之后，通过 GET/POST/PUT 等方式提交数据，并通过 HTTP 协议向 Web 服务器发出请求。 服务器端的 HTTP Daemon（守护进程）启动一个子进程。然后在子进程中，将 HTTP 请求里描述的信息通过标准输入 stdin 和环境变量传递给 URL 指定的 CGI 程序，并启动此应用程序进行处理，处理结果通过标准输出 stdout 返回给 HTTP Daemon 子进程。 再由 HTTP Daemon 子进程通过 HTTP 协议返回给客户端。","categories":[],"tags":[{"name":"网络编程","slug":"网络编程","permalink":"https://razertory.me/tags/网络编程/"}]},{"title":"动态规划入门","slug":"dynamic-programming","date":"2017-07-28T16:00:00.000Z","updated":"2020-03-28T13:02:10.000Z","comments":true,"path":"2017/07/29/dynamic-programming/","link":"","permalink":"https://razertory.me/2017/07/29/dynamic-programming/","excerpt":"","text":"动态规划从计算机系统的本质来说，就是加缓存。 先从一个题目说起吧 约翰的后花园约翰想在他家后面的空地上建一个后花园，现在有两种砖，一种 3 dm 的高度，7 dm 的高度。约翰想围成 x dm 的墙。如果约翰能做到，输出 YES，否则输出 NO。（https://www.lintcode.com/problem/johns-backyard-garden/description） 分析很经典的一个问题。判断能不能被 3 build，如果不能就回溯，直到判断被 7build 或者到栈顶。实现起来也比较容易： 1234567891011121314151617181920212223public class Solution &#123; /** * @param x: the wall's height * @return: YES or NO */ public String isBuild(int x) &#123; // write you code here if (helper(x) == true) return \"YES\"; else return \"NO\"; &#125; private boolean helper(int x)&#123; if (x == 0)&#123; return true; &#125; if (x &lt; 3)&#123; return false; &#125; return helper(x - 3) || helper(x - 7); &#125;&#125; 本来以为这样的算法效率并不高，因为要反复执行 || 运算。然而，然而，some times too naive |-_- 。事实是速度并不慢。在 || 运算中，只要有一个 true 产生。那么这个表达式的值一定是 true。对于布尔运算的递归程序同样适用。所以在 debug 阶段可以看到当 helper 里面一旦有 true 的时候。递归就会停止。这里不得不佩服前人做的努力。 斐波那契先来一把传统的递归写法 123456789public int fib(int i)&#123; if (i == 0)&#123; return 0; &#125; if (i == 1) &#123; return 1; &#125; return fib(i - 1) + fib(i - 2);&#125; 这段代码简洁而又优美，但实际上性能真的很差。从实际的角度出发，在我自己的 MBP 2017，CPU 3.1 GHz Intel Core i5 的电脑上。i = 40 ， 最终结果 =102334155 的时候，跑到出结果花了 869 ms。从分析的角度出发，这种递归程序有一个特质，就是大量重复的计算。举个例子：当 i = 5， 程序递归下去实际上执行了 2 个 fib(3) 和 3 个 fib(2)。可以考虑的是，发生过的事情，真的需要让他再发生吗？是不是只需要记忆结果，那么下次再有同样的需求发生的时候，只需要去取得结果，而不再是计算相同的任务了。 所以，试着加上缓存。做法就是维护一个数据结构，将计算过的任务的结果保存在里面。 1234567891011121314151617int []cache = new int[10000]; // 暂时用这么大，先不考虑极大情况。cache[1] = 1;public int fib(int i) &#123; if (cache[i] != 0) &#123; return cache[i]; &#125; if (i &lt;= 0) &#123; return 0; &#125; int result = fib(i - 1) + fib(i - 2); cache[i] = result; return result;&#125; 好了，测试一发。嗯，结果正确。看看性能：不到 5ms。 子数组问题给一个非负整数集合 set，一个目标值元素 sum，求判断该集合中，是否包含一个子集合，使得子集合的元素和等于目标值 sum？ easy 子集合如果大小确定为 2，那么直接就可以用 k-v 映射来判断。（嵌套 for 循环解决的的就过掉～） 123456789101112public boolean isSubsetSum(int[]set, int sum) &#123; HashMap map = new HashMap(); for (int i : set) &#123; map.put(sum - i, i); &#125; for (int i: set) &#123; if (map.containsKey(i)) &#123; return true; &#125; &#125; return false;&#125; hard 状态转移方程 子集合的大小 0 &lt; size &lt;= set.length。这种情况下，需要进一步把大问题拆分成小问题，推导出状态转移方程。 12345isSubsetSum(set, n, sum) = isSubsetSum(set, n-1, sum) || isSubsetSum(set, n-1, sum-set[n-1])Base Cases:isSubsetSum(set, n, sum) = false, if sum &gt; 0 and n == 0isSubsetSum(set, n, sum) = true, if sum == 0 紧接着，coding 1234567891011121314151617boolean isSubsetSum(int set[], int n, int sum)&#123; // 基本条件 if (sum == 0) return true; if (n == 0 &amp;&amp; sum != 0) return false; // 如果最后一个元素大于 sum，忽略掉 if (set[n-1] &gt; sum) return isSubsetSum(set, n-1, sum); /* 否则，检查 sum 是否能够被组合。分为下面两种情况 (a) 包含最后一个元素 (b) 不包含最后一个元素 */ return isSubsetSum(set, n-1, sum) || isSubsetSum(set, n-1, sum-set[n-1]);&#125; 这个做法的复杂度为指数级别 动态规划矩阵其实，在《算法导论》一书中，其实有一点可以很明确 “应用于子问题重叠的情况”。分析上述的算法，可以发现有很多重叠的子问题。所以，有必要回到文章最开始的一点 加缓存。这里考虑用一个矩阵来缓存结果集。 12345678910111213141516171819202122232425boolean isSubsetSum(int set[], int n, int sum) &#123; boolean subset[][] = new boolean[sum+1][n+1]; // sum 为 0 的情况，结果始终为 true for (int i = 0; i &lt;= n; i++) subset[0][i] = true; // sum 不为 0，但集合为空，始终为 false for (int i = 1; i &lt;= sum; i++) subset[i][0] = false; // 自底向上填充矩阵 for (int i = 1; i &lt;= sum; i++) &#123; for (int j = 1; j &lt;= n; j++) &#123; subset[i][j] = subset[i][j-1]; if (i&gt;= set[j-1]) subset[i][j] = subset[i][j] || subset[i - set[j-1]][j-1]; &#125; &#125; return subset[sum][n];&#125; 可以分析出，只需要矩阵级别的复杂度就可以解决这个问题。 笔者用了大量的案例来说明解决某些问题，用动态规划的方式。后续将加上一些最短路径算法和经典背包问题的解法。 理解了动态规划的哲学，很大程度上，在做一些高性能的系统的时候一定会有更多的思路。","categories":[],"tags":[]}]}