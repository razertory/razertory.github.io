---
layout: post
title:  "关于TCP，NIO，以及Netty的进一步理解"
date:   2017-11-20 0:21:00 +0800
categories: jekyll update
---

### 问题
在给我司旧系统解决性能问题的时候，有一个让我印象深刻的事件。我们有个历史项目，是用Netty做的Http服务器，这个项目在运行一段时间之后，几乎所有的接口都会变得很慢。通过一系列的观察，可以发现的是，每次重启服务器，接口就会变得很快，然后只要过了一周，即使在并发量及低的情况下，接口依然很慢。我最终定位到了瓶颈出现在了一个点上：通过`lsof -i`这个命令，可以看到项目所绑定的8080端口上，有着大量的CLOSE_WAIT的tcp socket fd。具体的表征就是

```shell
```

也就是说，有着大量的tcp socket资源在并发量很低的情况下依旧处于CLOSE_WAIT的状态。可以肯定的是，这种现象一定是不健康的。CLOSE_WAIT处于TCP四次分手过程中，客户端首先给服务端发送了fin报，服务端接受之后就会回应一个ack报给对方，同时状态切换为CLOSE_WAIT。接下来呢，服务端真正需要考虑的事情是察看是否还有数据发送给对方 ，如果没有的话，那么就可以close这个SOCKET，发送FIN报文给对方，也即关闭连接。所以你在CLOSE_WAIT状态下，需要完成的事情是等待你去关闭连接。这个时候，这两个过程是可以控制的，第一种，服务端不再发送报文给客户端，直接close。第二种，服务端发送fin报给客户端，同时自己处于LAST_ACK状态。所以我比较确定的是，服务端在管理TCP连接的阶段出了问题。

### 复现
第一步，我用ab test复现这个情况，这个还是比较容易的。在本地，我对任何一个接口并发请求，这个时候可以看到服务端有大量的ESTABLISHED状态的socket。第二步，我强行让请求端发送fin报，其实很简单:`ctr + c`。这个时候如我所料，这些socket都变成CLOSE_WAIT状态。

### 分析&解决
这个老项目本身就是个单体应用，所以可以很确认问题是出在了项目代码这一块。可以猜测的是，是使用Netty的方式不对。所以我第一步，去研究了Netty的基本原理。
